{"paper_id": "1908.07816", "sentences": ["More recently, researchers started incorporating affect information into neural dialog models.", "While a central theme", "Recent development in neural language modeling has generated significant excitement in the open-domain dialog generation community.", "The success of sequence-to-sequence (seq2seq) learning [5, 37] in the field of neural machine translation has inspired researchers to apply the recurrent neural network (RNN) encoder-decoder structure to response generation [42].", "Following the standard seq2seq structure, various improvements have been made on the neural conversation model.", "For example, Shang et al.", "[34] applied attention mechanism [2] to the same structure on Twitter-style microblogging data.", "Li et al.", "[17] found the original version tend to favor short and dull responses.", "They fixed this problem by increasing the diversity of the response.", "Li et al.", "[18] modeled the personalities of the speakers, and Xing et al.", "[44] developed a topic aware dialog system.", "We call work in this area globally neural dialog generation.", "For a comprehensive survey, please refer to [4].", "Many application areas show significant benefits of integrating affect information in natural language dialogs.", "In earlier work on human computer interaction, Klein et al.", "[16] found users frustration caused by a computer system can be alleviated by computer-initiated emotional support, by providing feedback on emotional content along with sympathy and empathy.", "Recently, Hu et al.", "[14] developed a customer support neural chatbot, capable of generating dialogs similar to the humans in terms of empathic and passionate tones, potentially serving as proxy customer support agents on social media platforms.", "In a qualitative study [47], participants expressed an interest in chatbots capable of serving as an attentive listener and providing motivational support, thus fulfilling users emotional needs.", "Several participants even noted a chatbot is ideal for sensitive content that is too embarrassing to ask another human.", "Finally Bickmore and Picard [3] showed a relational agent with deliberate socialemotional skills was respected more, liked more, and trusted more, even after four weeks of interaction, compared to an equivalent task-oriented agent.", "ABSTRACT", "KEYWORDS", "https://doi.org/10.1145/nnnnnnn.nnnnnnn", "ACM Reference Format:", "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.", "Copyrights for thirdparty components of this work must be honored.", "For all other uses, contact the owner/author(s).", "IUI 20 Workshops, March 17, 2020, Cagliari, Italy", "2020 Copyright held by the owner/author(s).", "Human-centered computing  Human computer interaction (HCI) ; Natural language interfaces .", "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.", "Our contributions are threefold.", "(1) We describe in detail a novel emotion-tracking dialog generation model that learns the emotional interactions directly from the data.", "This approach is free of human-defined heuristic rules, and hence, is more robust and fundamental than those described in existing work.", "(2) We compare our model, MEED, with the generic seq2seq model and the hierarchical model of multiturn dialogs (HRAN).", "Offline experiments show that our model outperforms both seq2seq and HRAN by a significant amount.", "Further experiments with human evaluation show our model produces emotionally more appropriate responses than both baselines, while also improving the language fluency.", "(3) We illustrate a human-evaluation procedure for judging machine produced emotional dialogs.", "We consider factors such as the balance of positive and negative emotions in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate.", "It is the first time such an approach is designed with consideration for human judges.", "Our main goal is to increase the objectivity of the results and reduce judges mistakes due to out-of-context dialogs they have to evaluate.", "Furthermore, to the best of our knowledge, the psychology and social science literature does not provide clear rules for emotional interaction.", "It seems such social and emotional intelligence is captured in our conversations.", "This is why we decided to take the automatic and data-driven approach.", "In this paper, we describe an end-to-end Multi-turn Emotionally Engaging Dialog model (MEED), capable of recognizing emotions and generating emotionally appropriate and humanlike responses with the ultimate goal of reproducing social behaviors that are habitual in human-human conversations.", "We chose the multi-turn setting because a model suitable for single-turn dialogs cannot effectively track earlier context in multi-turn dialogs, both semantically and emotionally.", "Since being able to track several turns is really important, we made this design decision from the beginning, in contrast to most related work where models are only trained and tested on single-turn dialogs.", "While using a hierarchical mechanism to track the conversation history in multi-turn dialogs is not new (e.g., HRAN by Xing et al. [45]), to combine it with an additional emotion RNN to process the emotional information in each history utterance has never been attempted", "Vinyals and Le [42] were one of the first to model dialog generation using neural networks.", "Their seq2seq framework was trained on an IT Helpdesk Troubleshooting dataset and the OpenSubtitles dataset [21].", "Shang et al.", "[34] further trained the seq2seq model with attention mechanism on a self-crawled Weibo (a popular Twitter-like social media website in China) dataset.", "Meanwhile, Xu et al.", "[46] built a customer service chatbot by training the seq2seq model on a dataset collected with conversations between customers and customer service accounts from 62 brands on Twitter.", "seems to be making the responses emotionally richer, existing approaches mainly follow two directions.", "In one, an emotion label is explicitly required as input so that the machine can generate sentences of that particular emotion label or type [49].", "In another group of work, the main idea is to develop handcrafted rules to direct the machines to generated responses of the desired emotions [1, 48].", "Both approaches require an emotion label as input (either given or handcrafted), which might be unpractical in real dialog scenarios.", "Ghosh et al.", "[11] made the first attempt to augment the original LSTM language model with affect treatment in what they called Affect-LM.", "At training time, Affect-LM can be considered as an energy based model where the added energy term captures the degree of correlation between the next word and the affect information of the preceeding text.", "At text generation time, affect information is also used to increase the appropriate selection of the next word.", "A key component in Affect-LM is the use of a well established text analysis program, LIWC (Linguistic Inquiry and Word Count) [28].", "For every sentence, for example, I unfortunately did not pass my exam, the model generates five emotion features denoting ( sad : 1, angry : 1, anxiety : 1, negative emotion : 1, positive emotion : 0).", "This makes Affect-LM both capable of distinguishing affect information conveyed by each word in the language modeling part and aware of the preceeding texts emotion in each generation step.", "In a similar vein, Asghar et al.", "[1] appended the original word embeddings with a VAD affect model [43].", "VAD is a vector model, as opposed to a categorical model (LIWC), representing a given emotion in each of the valence, arousal, and dominance axes.", "In", "The standard seq2seq framework is applied to single-turn response generation.", "In multi-turn settings, where a context with multiple history utterances is given, the same structure often ignores the hierarchical characteristic of the context.", "Some recent work addresses this problem by adopting a hierarchical recurrent encoder-decoder (HRED) structure [32, 33, 35].", "To give attention to different parts of the context while generating responses, Xing et al.", "[45] proposed the hierarchical recurrent attention network (HRAN), using a hierarchical attention mechanism.", "However, these multiturn dialog models do not take into account the turn-taking emotional changes of the dialog.", "Recent work on incorporating affect information into natural language processing tasks has inspired our current work.", "They can be mainly described as affect language models and emotional dialog systems.", "contrast to Affect-LM, Asghars neural affect dialog model aims at generating explicit responses given a particular utterance.", "To do so, the authors designed three affect-related loss functions, namely minimizing affect dissonance, maximizing a affective dissonance, and maximizing affective content.", "The paper also proposed the affectively diverse beam search during decoding, so that the generated candidate responses are as affectively diverse as possible.", "However, literature in affective science does not necessarily validate such rules.", "In fact, the best strategy to speak to an angry customer is the de-escalation strategy (using neutral words to validate anger) rather than employing equally emotional words (minimizing affect dissonance) or words that convey happiness (maximizing affect dissonance).", "As much as these work in the above section inspired our work, our approach in generating affect dialogs is significantly different.", "Most of related work focused on integrating affect information into the transduction vector space using either VAD or LIWC, we aim at modeling and generating the affect exchanges in human dialogs using a dedicated embedding layer.", "The approach is also completely data-driven, thus absent of hand-crafted rules.", "To avoid learning obscene and callous exchanges often found in social media data like tweets and Reddit threads [29], we opted to train our model on movie subtitles, whose dialogs were carefully created by professional writers.", "We believe the quality of this dataset can be better than those curated by crowdsource platforms.", "For modeling the affect information, we chose to use LIWC", "The Emotional Chatting Machine (ECM) [49] takes a post and generates a response in a predefined emotion category.", "The main idea is to use an internal memory module to capture the emotion dynamics during decoding, and an external memory module to model emotional expressions explicitly by assigning different probability values to emotional words as opposed to regular words.", "Zhou and Wang [50] extended the standard seq2seq model to a conditional variational autoencoder combined with policy gradient techniques.", "The model takes a post and an emoji as input, and generates the response with target emotion specified by the emoji.", "Hu et al.", "[14] built a tone-aware chatbot for customer care on social media, by deploying extra meta information of the conversations in the seq2seq model.", "Specifically, a tone indicator is added to each step of the decoder during the training phase.", "Usually the probability distribution p ( y | X ) can be modeled by an RNN language model conditioned on X .", "When generating the word y t at time step t , the context X is encoded into a fixed-sized dialog context vector c t by following the hierarchical attention structure in HRAN [45].", "Additionally, we extract the emotion information from the utterances in X by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector e , which is combined with c t to produce the distribution.", "The overall architecture of the model is depicted in Figure 1.", "We are going to elaborate on how to obtain c t and e , and how they are combined in the decoding part.", "because it is a well-established emotion lexical resource, covering the whole English dictionary whereas VAD only contains 13K lemmatized terms.", "In parallel to these developments, Zhong et al.", "[48] proposed an affect-rich dialog model using biased attention mechanism on emotional words in the input message, by taking advantage of the VAD embeddings.", "The model is trained with a weighted cross-entropy loss function, which encourages the generation of emotional words.", "The hierarchical attention structure involves two encoders to produce the dialog context vector c t , namely the word-level encoder and the utterance-level encoder.", "The word-level encoder is essentially a bidirectional RNN with gated recurrent units (GRU) [5].", "For utterance x j in X ( j = 1 , 2 , . . . , m ), the bidirectional encoder produces two hidden states at each word position k , the forward hidden state h f jk and the backward hidden state h b jk .", "The final hidden state h jk is then obtained by concatenating the two,", "We describe our model one element at a time, from the basic structure, to the hierarchical component, and finally the emotion embedding layer.", "The utterance-level encoder is a unidirectional RNN with GRU that goes from the last utterance in the context to the first, with its input at each step as the summary of the corresponding utterance, which is obtained by applying a Bahdanau-style attention mechanism [2] on the word-level", "We first consider the problem of generating response y given a context X consisting of multiple previous utterances by estimating the probability distribution p ( y | X ) from a data set D = {( X ( i ) , y ( i ) )} Ni = 1 containing N context-response pairs.", "Here", "is the response with T i words.", "is a sequence of n ij words.", "Similarly,", "is a sequence of m i utterances, and", "We are able to achieve this goal, i.e., capturing the emotion information carried in the context X , in the encoder, thanks to LIWC.", "We make use of the five emotion-related categories, namely positive emotion , negative emotion , anxious , angry , and sad .", "This set can be expanded to include more categories if we desire a richer distinction.", "See the discussion section for more details on how to do this.", "Using the newest version of", "The main objective of the emotion embedding layer is to recognize the affect information in the given utterances so that the model can respond with emotionally appropriate replies.", "To achieve this, we need an encoder to distinguish the affect information in the context, in addition to its semantic meaning.", "Equally we need a decoder capable of selecting the best and most human-like answers.", "where s t  1 is the previous hidden state of the decoder,  tj + 1 is the previous hidden state of the utterance-level encoder, and v a , U a , V a and W a are word-level attention parameters.", "The final dialog context vector c t is then obtained as another linear combination of the outputs of the utterance-level encoder  tj , for j = 1 , 2 , . . . , m ,", "where s t  1 is the previous hidden state of the decoder, and v b , U b and W b are utterance-level attention parameters.", "Here  tj is the utterance-level attention score placed on  tj , and can be calculated as", "Here  tjk is the word-level attention score placed on h jk , and can be calculated as", "encoder output.", "More specifically, at decoding step t , the summary of utterance x j is a linear combination of h jk , for k = 1 , 2 , . . . , n j ,", "We trained our model using two different datasets and compared its performance with HRAN as well as the basic seq2seq model by performing both offline and online testings.", "We used two different dialog corpora to train our model the Cornell Movie Dialogs Corpus [6] and the DailyDialog dataset [20].", "the program LIWC2015, 1 we are able to map each utterance x j in the context to a six-dimensional indicator vector 1 ( x j ) , with the first five entries corresponding to the five emotion categories, and the last one corresponding to neutral .", "If any word in x j belongs to one of the five categories, then the corresponding entry in 1 ( x j ) is set to 1; otherwise, x j is treated as neutral, with the last entry of 1 ( x j ) set to 1.", "For example, assuming x j = he is worried about me, then", "In our experiments, the models were first trained on the Cornell Movie Dialogs Corpus, and then fine-tuned on the DailyDialog dataset.", "We adopted this training pattern because the Cornell dataset is bigger but noisier, while DailyDialog is smaller but more daily-based.", "To create a training set and a validation set for each of the two datasets, we took segments of each dialog with number of turns no more than six, 2 to serve as the training/validation examples.", "Specifically, for each dialog D = ( x 1 , x 2 , . . . , x M ) , we created M  1 contextresponse pairs, namely U i = ( x s i , . . . , x i ) and y i = x i + 1 , for i = 1 , 2 , . . . , M  1, where s i = max ( 1 , i  4 ) .", "We filtered out those pairs that have at least one utterance with length greater than 30.", "We also reduced the frequency of those pairs", "where W e and b e are trainable parameters.", "The emotion flow of the context X is then modeled by an unidirectional RNN with GRU going from the first utterance in the context to the last, with its input being a j at each step.", "The final emotion context vector e is obtained as the last hidden state of this emotion encoding RNN.", "since the word worried is assigned to both negative emotion and anxious .", "We apply a dense layer with sigmoid activation function on top of 1 ( x j ) to embed the emotion indicator vector into a continuous space,", "where w y t  1 is the word embedding of y t  1 .", "Similar to AffectLM [11], we then define a new feature vector o t by concatenating s t (which we refer to as the language context vector) with the emotion context vector e ,", "The probability distribution p ( y | X ) can be written as", "We use the cross-entropy loss as our objective function", "We model the probability distribution using an RNN language model along with the emotion context vector e .", "Specifically, at time step t , the hidden state of the decoder s t is obtained by applying the GRU function,", "on which we apply a softmax layer to obtain a probability distribution over the vocabulary,", "where W and b are trainable parameters.", "Each term in Equation (13) is then given by", "We two datasets in Table 1.", "o t = concat ( s t , e ) ,", "p t = softmax ( Wo t + b ) ,", "1 https://liwc.wpengine.com/", "whose responses appear too many times (the threshold is set to 10 for Cornell, and 5 for DailyDialog), to prevent them from dominating the learning procedure.", "See Table 1 for the sizes of the training and validation sets.", "The test set consists of 100 dialogs with four turns.", "We give more detailed description of how we created the test set in the section of human evaluation.", "Thus a lower perplexity score indicates that the model has better capability of predicting the target sentence, i.e., the humans response.", "Some researchers [19, 34, 48] argue that perplexity score is not the ideal measurement because for a given context history, one should allow many responses.", "This is especially true if we want our conversational agents to speak more diversely.", "However, for our purpose, which is to speak emotionally appropriately and as human-like as possible, we believe this is a good measure.", "We do recognize that it is not the only way to measure chatbots performance.", "This is why we also conducted human evaluation experiment.", "Human Evaluation.", "Human evaluation has been widely used to evaluate open-domain dialog generation tasks.", "This approach can include any criterion as we judge appropriate.", "Most commonly, researchers have included the models ability to generate grammatically correct, contextually coherent, and emotionally appropriate responses, of which the latter two properties cannot be reliably evaluated using automatic metrics.", "Recent work [1, 48, 49] on affect-rich conversational chatbots turned to human opinion to evaluate both fluency and emotionality of their models.", "But such human experiments are sensitive to risk factors if the experiment is not carefully designed.", "They include whether the intructions are clear, whether they have been tested with users before hand, and whether there is a good balance of the human judgement tasks.", "Further, if a test set for human evaluation is prepared", "Our choice of including S2S is rather obvious.", "Including HRAN instead of other neural dialog models with affect information was not an easy decision.", "As mentioned in the related work, Asghars affective dialog model, the affect-rich conversation model, and the Emotional Chatting Machine do not learn the emotional exchanges in the dialogs.", "This leaves us wondering whether using a multi-turn neural model can be as effective in learning emotional exchanges as MEED.", "In addition, comparing S2S and HRAN also gives us an idea of how much the hierarchical mechansim is improving upon the basic model.", "This is why our final comparision is based on three multi-turn dialog generation models: the standard seq2seq model (denoted as S2S), HRAN, and our proposed model, MEED.", "In order to adapt S2S to the multi-turn setting, we concatenate all the history utterances in the context into one.For", "all the models, the vocabulary consists of 20,000 most frequent words in the Cornell and DailyDialog datasets, plus three extra tokens: <unk> for words that do not exist in the vocabulary, <go> indicating the begin of an utterance, and <eos> indicating the end of an utterance.", "Here we summarize the configurations and parameters of our experiments:", "BLEU score is often used to measure the quality of machinetranslated text.", "Some earlier work of dialog response generation [17, 18] adopted this metric to measure the performance of chatbots.", "However, recent study [22] suggests that it does not align well with human evaluation.", "Nevertheless, we still include BLEU scores in this paper, to get a sense of comparison with perplexity and human evaluation results.", "The evaluation of chatbots remains an open problem in the field.", "Recent work [22] has shown that the automatic evaluation metrics borrowed from machine translation such as", "BLEU score [27] tend to align poorly with human judgement.", "Therefore, in this paper, we mainly adopt human evaluation, along with perplexity and BLEU score, following the existing work.", "Automatic Evaluation.", "Perplexity is a measurement of how a probability model predicts a sample.", "It is a popular method used in language modeling.", "In neural dialog generation community, many researchers have adopted this method, especially in the beginning of this field [32, 42, 45, 4850].", "It measures how well a dialog model predicts the target response.", "Given a target response y = { y 1 , y 2 , . . . , y T } , the perplexity is calculated as", "We have made the source code publicly available.", "3", "3 https://github.com/yuboxie/meed", "sample.Totake into account the above issues, we took several iterations to prepare the instructions and the test set before conducting the human evaluation experiment.", "Part of our test set comes from the DailyDialog dataset, which consists of meaningful complete dialogs.", "To compensate for the inbalance, we further curated more negative emotion dialogs so that the final set has equal emotion distributions.", "We provide the details about the test data preparation process and the evaluation experiment below.", "Human Evaluation Experiment Design.", "In the final human evaluation of the model, we recruited four more PhD students from our university (1 female and 3 males, aged 2225).", "Three of them are fluent English speakers and one is a native speaker.", "The recruitment proceeded in the same manner as described above; the raters were offered 80 CHF (or roughly US $80) per participant gift coupons for fulfilling the task, and extra 20 CHF (or roughly US $20) coupon was promised", "Preparation of Natural Dialog Test Set.", "We first selected the emotionally colored dialogs with exactly four turns from the DailyDialog dataset.", "In the dataset each dialog turn is annotated with a corresponding emotional category, including the neutral one.", "For our purposes we filtered out only those dialogs where more than a half of utterances have non-neutral emotional labels, resulting in 78 emotionally positive dialogs and 14 emotionally negative dialogs.", "We recruited two human workers to augment the data to produce more emotionally negative dialogs.", "Both of them were PhD students from our university (males, aged 24 and 25), fluent in English, and not related to the authors lab.", "We found them via email and messaging platforms, and offered 80 CHF (or roughly US $80) gift coupons as incentive for each participant.", "The workers fulfilled the tasks in Google form 4 following the instructions and created five negative dialogs with four turns, as if they were interacting with another human, in each of the following topics: relationships , entertainment , service , work and study , and everyday situations .", "The Google form was released on 31 January 2019, and the workers finished their tasks by 4 February 2019.", "Subsequently, to form the final test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above.", "by randomly sampling the dialogs from the dataset, it may include out-of-context dialogs, causing confusion and ambiguity for human evaluators.", "Unbalanced emotional distribution of the test dialogs may also lead to biased conclusions since the chatbots abilities are evaluated on the unrepresentative", "as a bonus to the rater judged to be the most serious.", "For the evaluation survey, we also leveraged Google form.", "Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared (S2S, HRAN, and MEED), and obtain the respective responses.", "Dialog contexts and three models responses were included into Google form.", "According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria:", "Automatic Evaluation Results.", "Table 2 gives the perplexity and BLEU scores obtained by the three models on the two validation sets and the test set.", "As shown in the table, MEED achieves the lowest perplexity and the highest BLEU score on all three sets.", "We conducted t -test on the perplexity obtained, and results show significant improvements of MEED over S2S and HRAN on the two validation sets (with p -value < 0 . 05).", "Human Evaluation Results.", "Table 3, 4 and 5 summarize the human evaluation results on the responses grammatical correctness, contextual coherence, and emotional appropriateness, respectively.", "In the tables, we give the percentage of votes each model received for the three scores, the average score obtained, and the agreement score among the raters.", "Note that we report Fleiss  score [10] for contextual coherence and emotional appropriateness, and Finns r score [9] for grammatical correctness.", "We did not use Fleiss  score for grammatical correctness.", "As agreement is extremely high, this can make Fleiss  very sensitive to prevalence [13].", "On the contrary, we did not use Finns r score for contextual coherence and emotional appropriateness because it is only reasonable when the observed variance is significantly less than the chance variance [40], which did not apply to these two criteria.", "As shown in the tables, we got high agreement among the raters for grammatical correctness, and fair", "In this subsection, we present the experimental results of the automatic evaluation metric as well as human judgement, followed by some analysis.", "For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral.", "For this survey, the Google form was launched on 12 February 2019, and all the submissions from our raters were collected by 14 February 2019.", "With t-SNE [25], we are able to reduce the dimensionality of the weights to two, and visualize them in a straightforward way.", "For better illustration, we selected 100 most frequent (emotionally) positive words and 100 most frequent negative words from the vocabulary, and used t-SNE to project the corresponding language model weights and emotion weights to two dimensions.", "Figure 2 gives the results in three subplots.", "Since HRAN does not have the emotion context vector, we just visualized the whole output layer weight vector, which does a similar job as the language model weights in", "agreement among the raters for contextual coherence and emotional appropriateness.", "5 For grammatical correctness, all three models achieved high scores, which means all models are capable of generating fluent utterances that make sense.", "For contextual coherence and emotional appropriateness, MEED achieved higher average scores than S2S and HRAN, which means MEED keeps better track of the context and can generate responses that are emotionally more appropriate and natural.", "We first conducted Friedman test [12] and then t -test on the human evaluation results (contextual coherence and emotional appropriateness), showing the improvements of MEED over S2S are significant (with p -value < 0 . 01).", "decoding phase, Equation (16) takes o t , the concatenation of the language context vector s t and the emotion context vector e , and generates a probability distribution over the vocabulary words by applying a softmax layer.", "The weight matrix of this softmax layer is denoted as W , whose shape is | V | 2 d , where | V | is the vocabulary size and d = 256 is the hidden state size of the RNNs.", "Thus the i th row of the weight matrix W i can be regarded as a vector representation of the i th word in the vocabulary.", "Since we concatenate the language context vector and the emotion context vector as the input to the softmax layer, the first half of the weight vector W i corresponds to the language context vector, and the second half corresponds to the emotion context vector.", "We refer to them as language model weights and emotion weights, respectively.", "If the emotion embedding layer is learning and distinguishing affect states correctly, we will see clear differences in the visualization.", "The comparison between perplexity scores and human evaluation results further confirms the fact that in the context", "Visualization of Output Layer Weights.", "We may wonder how HRAN and MEED differ in terms of the distributional representations of their respective vocabularies (words in the language model, and affect words).", "We decided to visualize the output layer weights as word embedding representations using dimensionality reduction technique for the various models.Inthe", "of dialog response generation, perplexity does not align with human judgement.", "In Table 2, for all the three sets, HRAN performs worse than S2S in terms of perplexity.", "However, for all of the three criteria in human evaluation, HRAN actually outperforms S2S.", "Based on this, we conclude that perplexity alone is not enough for evaluating a dialog system.", "5 https://en.wikipedia.org/wiki/Fleiss%27_kappa#Interpretation", "Case Study.", "We present four sample dialogs in Table 6, along with the responses generated by the three models.", "Dialog 1 and 2 are emotionally positive and dialog 3 and 4 are negative.", "For the first two examples, we can see that MEED is able to generate more emotional content (like fun and congratulations) that is appropriate according to the context.", "For dialog 4, MEED responds in sympathy to the other speaker, which is consistent with the second utterance in the context.", "On the contrary, HRAN poses a question in reply, contradicting the dialog history.", "MEED.", "We can observe from the first two plots that positive words (green dots) and negative words (red dots) are scattered around and mixed with each other in the language model weights for HRAN and MEED respectively, which means no emotion information is captured in these weights.", "On the contrary, the emotion weights in MEED, in the last plot, have a clearer clustering effect, i.e., positive words are mainly grouped on the top-left, while negative words are mainly grouped at the bottom-right.", "This gives the hint that the emotion encoder in MEED is capable of tracking the emotion states in the conversation history.", "We pre-trained our model on the Cornell movie subtitles and then fine-tuned it with the DailyDialog dataset.", "We adopted this particular training order because we would like our chatbot to talk more like human chit-chats, and the DailyDialog dataset, compared with the bigger Cornell dataset, is more daily-based.", "Since our model learns how to respond properly in a data-driven way, we believe having a training dataset with good quality while being large enough plays an important role in developing an engaging and user-friendly chatbot.", "Thus, in the future, we plan to train our model on the multi-turn conversations that we have already extracted from the much bigger OpenSubtitles corpus and the EmpatheticDialogues dataset.", "6", "To extract the affect information contained in the utterances, we used the LIWC text analysis program.", "We believe this emotion recognition step is vital for a dialog model to produce emotionally appropriate responses.", "However, the choice of emotion classifier is not strictly limited to LIWC.", "It could be replaced by other well-established affect recognizer or one that is more appropriate to the target domain.", "For example, we can consider using more fine-grained emotion categories from GALC [31], or using DeepMoji [8], which was trained", "Evaluation of dialog models remains an open problem in the response generation field.", "Early work [18, 30, 36] on response generation used automatic evaluation metrics borrowed from the machine translation field, such as the BLEU score, to evaluate dialog systems.", "Later on, Liu et al.", "[22] showed that these metrics correlate poorly with human judgement.", "Recently, a number of researchers begain developing automatic and data-driven evaluation methods [24, 38], with the ultimate goal of replacing human evaluation.", "However they are still in an early stage.", "In this paper, we used both perplexity measures and human judgement in our experiments to finalize our model.", "In other words, using the perplexity measures, we were able to determine when to stop training our model.", "But this condition does not gurantee the optimal results until", "on millions of tweets with emoji labels and is more suitable for tweet-like conversations.", "However, for DeepMoji, the 64 categories of emojis do not have a clear and exact correspondence with standardized emotion categories, nor to the VAD vectors.", "In this section, we briefly discuss how our framework can incorporate other components, as well as several directions to extend it.", "6 https://github.com/facebookresearch/EmpatheticDialogues", "capable of recognizing and generating emotionally appropriate responses, which is the first step toward such a goal.", "We have demonstrated how to do so by (1) modeling utterances with extra affect vectors, (2) creating an emotional encoding mechanism that learns emotion exchanges in the dataset, (3) curating a multi-turn and balanced dialog dataset, and (4) evaluating the model with offline and online experiments.", "For future directions, we would like to investigate the diversity issue of the responses generated, possibly by extending the mutual information objective function [17] to multi-turn settings.", "We would also like to adopt the Transformer architecture with pre-trained language model weights, and train our model on a much larger dataset, by extracting multi-turn dialogs from the OpenSubtitles corpus.", "human judgement test can validate them.", "We thus highly recommend this combination, which is also a common practice in the research community [45, 4850].", "Our model uses RNNs to encode the input sequences, and GRU cells to capture long-term dependency among different positions in the sequences.", "Recent advances in natural language understanding have proposed new network architectures to process text input.", "Specifically, the Transformer [41] uses pure attention mechanisms without any recurrence structures.", "Compared with RNNs, the Transformer can capture better long-term dependency due to the self-attention mechanism, which is free of locality biases, and is more efficient to train because of better parallelization capability.", "Following the Transformer architecture, researchers found that pre-training language models on huge amounts of data could largely boost the performance of downstream tasks, and published many pre-trained language models such as BERT [7] and RoBERTa [23].", "As future work, we would like to adopt the Transformer architecture to replace the RNNs in our model, and initialize our encoder with pre-trained language models.", "We hope to increase the performance of response generation.", "We believe reproducing conversational and emotional intelligence will make social chatbots more believable and engaging. In this paper, we proposed a multi-turn dialog system", "learning-with-neural-networks"]}
