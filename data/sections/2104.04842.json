{"sections":[{"body":"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445569\nIncorporating the framework, we have developed iChatProfle, an assistive chatbot design tool that can automatically generate a profle of an interview chatbot with quantifed performance metrics and ofer design suggestions for improving the chatbot based on such metrics. To validate the efectiveness of iChatProfle, we designed and conducted a between-subject study that compared the performance of 10 interview chatbots designed with or without using iChatProfle. Based on the live chats between the 10 chatbots and 1349 users, our results show that iChatProfle helped the designers build signifcantly more efective interview chatbots, improving both interview quality and user experience.\nCCS CONCEPTS • Human-centered computing → Human computer interaction; • Computing Methodologies → Intelligent agents.\nKEYWORDS Conversational AI Agents; Interview Chatbot; Chatbot Debugging; Chatbot Evaluation Framework; Chatbot Design Suggestion; Automatic Chatbot Profling; Automatic Chatbot Evaluation\nACM Reference Format: Xu Han, Michelle Zhou, Matthew J. Turner, and Tom Yeh. 2021. Designing Efective Interview Chatbots: Automatic Chatbot Profling and Design Suggestion Generation for Chatbot Debugging. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3411764.3445569"},{"header":"1 INTRODUCTION","body":"During the past few years, chatbots have been used to conduct interviews by engaging users in one-on-one text-based conversations [67]. Recent studies show that interview chatbots are more efective at engaging users and eliciting quality information from the users, compared to traditional online surveys [31, 66, 67].\nDespite their promises, it is challenging and time consuming to build efective interview chatbots due to the limitations in today’s technologies and the complexity involved in interview conversations [18, 65]. Like building any complex interactive systems [42], one potential approach is to design and improve an interview chatbot iteratively. Specifcally, the iterative design of an interview chatbot is to fulfll two main goals. First, like designing any user interviews or surveys [7, 42], designers of an interview chatbot need to ensure the efective design of an interview task (e.g., proper and clear wording of questions). Second, like building any conversational agents [29], designers of an interview chatbot need to make sure that the chatbot can successfully carry out such an interview task [65].\nTo achieve above goals, designers often conduct pilot studies prior to a formal study [49]. However, interview chatbot designers face two challenges in detecting let alone fxing the potential issues revealed by the pilot studies. First, designers must examine chat transcripts to discover potential issues and it is laborious and time consuming to do so manually. For example, Fig 2(a) shows that an interview question poses a challenge for a user due to a lack of clarity, while Fig 2(b) shows a chatbot-unrecognized user input during an interview, which could result in poor user experience or even abandoned interviews. To detect such issues in practice, chatbot designers must examine chat transcripts (Fig 2(c)) to discover them. It is laborious and time consuming to do so manually especially if the designers need to detect such issues from a large number of chat transcripts. Second, even if the designers have discovered such issues from reading chat transcripts, they might not know how to fx the issues and improve the chatbot due to a lack of relevant experience (e.g., designing efective interview interactions).\nTo address the above two challenges, we have been developing a tool, called iChatProfle, which can aid chatbot designers in building, evaluating, and improving interview chatbots iteratively. In particular, iChatProfle automatically analyzes chat transcripts and computes a set of chatbot performance metrics to present designers with a chatbot profle (Fig 1(a)). Based on the chatbot profle, iChatProfle also automatically generates a set of design suggestions to guide designers to improve the chatbot (Fig 1(b)).\nIn this paper, we present the key steps taken to build iChatProfle. First, we present a formative study that was conducted to understand the difculties that designers face when building an interview chatbot and identify their desired design assistance. Second, we describe a computational framework that quantitatively measures the efectiveness of an interview chatbot from multiple dimensions, including elicitation ability, user experience, and ethics. Third, we present iChatProfle that was built based on our formative study and the computational framework. To validate the efectiveness of iChatProfle, we designed and conducted a between-subject user study that compared the performance of chatbots designed with or without using iChatProfle. A total of 10 chatbots were created and evaluated live by 1349 participants from Amazon Mechanical Turk. We compared the performance of these chatbots. The results show that the chatbots designed with the help of iChatProfle performed signifcantly better along many dimensions, including improved user response quality and user experience.\nTo the best of our knowledge, our work is the frst on building an assistive design tool for creating interview chatbots. As a result, our work reported here provides three unique contributions:\n(1) A computational framework for quantifying the efectiveness of interview chatbots. This framework comprehensively evaluates the efectiveness of an interview chatbot by computing an extensive set of performance metrics covering multiple dimensions: elicitation ability, user experience, and ethics. Other chatbot researchers and practitioners can easily adopt or extend this framework to build their own chatbot evaluation tools. (2) Practical approaches to assisting iterative design of interview chatbots. iChatProfle presents practical implementations of an assistive chatbot design tool. Because we have demonstrated the efectiveness of our implementations, others could replicate or extend our approaches to create more tools aiding chatbot design. (3) Design implications for building assistive chatbot design tools beyond building interview chatbots. Although our current work focuses on aiding the design of interview chatbots, it presents design considerations for assisting the design of other types of chatbots, such as counseling or training chatbots, which share similar design requirements (e.g., ethical considerations)."},{"header":"2 RELATED WORK","body":""},{"header":"2.1 Chatbots for Information Elicitation","body":"AI-powered conversational user interfaces, also known as AI chatbots or chatbots for short, allow users to communicate with computers in natural language, providing more fexible [5] and personalized user experience [69]. Such benefts have encouraged the creation of a wide array of chatbot applications, such as virtual assistants [38], social companions [56], and interview chatbots [37]. Our work is most relevant to the use of chatbots for information elicitation [31, 66, 67].\nResearchers have developed various chatbots to elicit information from users through text-based conversations. For example, Bohus and Rudnicky introduce dialog systems that gather required\ninformation for performing specifc tasks (e.g., making travel reservations) [4]. More recently, a number of interview chatbots have been developed to elicit information from a target audience. For example, a chatbot is built to interview students for efective teaming [66] and another chatbot to interview gamers for eliciting their game opinions [67]. Williams et al. have developed a chatbot to interview employees for workplace productivity [64]. Compared to traditional, static online surveys, these interview chatbots enhance information elicitation [31, 66] by providing interactive feedback [8] and asking follow-up questions [46].\nOur work is directly related to the eforts of creating interview chatbots. However, existing work focuses on developing interview chatbots for specifc information elicitation tasks (e.g., [64, 66, 67]) or powering interview chatbots with specifc skills (e.g., giving them a personality [71] and active listening skills [65]). While we learn from these eforts, our work reported here has a very diferent focus: we want to build a tool that can automatically evaluate the performance of an interview chatbot and provide design suggestions for improving the chatbot."},{"header":"2.2 Chatbot Platforms","body":"There are a number of chatbot platforms and these platforms can be broadly divided into three categories. First, chatbot platforms like Chatfuel [60] and ManyChat [2] allow non-IT professionals to build a chatbot without coding. Since these platforms provide limited AI/NLP capabilities, it would be difcult to create interview chatbots that can understand users especially when open-ended interview questions are involved. The second type includes platforms like Google Dialogfow [13] and IBM Watson [25]. These platforms provide designers with more fexibility to customize a chatbot’s AI/NLP capabilities but designers must have basic AI/NLP knowledge to use the tools. The third category includes platforms like Juji, which provides a rich set of pre-built AI capabilities to enable non-IT designers to build chatbots without any expertise of AI/NLP [59]. While chatbot designers can choose to use any of the chatbot platforms, none of the platforms provides a tool like ours reported here: a tool that helps designers evaluate a chatbot’s performance and provides design suggestions to improve the chatbot."},{"header":"2.3 Evaluating Conversational AI Systems","body":"Researchers have developed a number of approaches to evaluating conversational AI systems. These approaches can be roughly organized into two categories: measuring objective system performance (e.g., task completion rate for task-oriented chatbots [29]) and assessing subjective human experience (e.g., measuring users’ trust in a chatbot [71]). Incorporating multiple metrics, evaluation frameworks have also been proposed to systematically measure the performance of conversational AI systems. For example, PARADISE has been used to evaluate the performance of task-oriented, spoken dialog systems [62], typically developed by AI/NLP experts. Unlike these works that evaluate conversational AI systems in general, our work presented here focuses on evaluating the performance of interview chatbots, typically designed by non-IT professionals. While we borrow some of the existing objective and subjective metrics, we have developed a computational framework specifcally for quantifying the performance of interview chatbots with actionable insights—design suggestions that can help designers improve an interview chatbot."},{"header":"2.4 Design Suggestion Generation for Efective Interaction","body":"Our work on generating design suggestions is also related to various eforts on guiding the design of human-computer interfaces, such as chatbot systems [21] and graphical user interfaces (GUI) [36, 68]. For example, Han et al. combine domain-specifc knowledge together with observational studies to generate rule-based design suggestions for task-oriented chatbots [21]. One of the drawbacks of this approach lies in its infexibility of adapting design suggestions to changing design goals or dynamic design issues occurring in real time. On the other hand, Lee et al. use autoencoder and k-nearest neighbor algorithms to recommend GUI design examples that help designers in real time [36]. Moreover, Xu et al. have developed a system that incorporates crowdsourcing to generate design suggestions for GUI designers [68]. While we learn from these approaches, we are unaware of any approach to automatic generation of design suggestions based on computed chatbot performance as our approach does."},{"header":"3 STUDY PLATFORM - JUJI","body":"As mentioned in Section 2.2, there are three types of chatbot platforms. Although we could build iChatProfle on top of any chatbot platform, we decided to build it on Juji for three main reasons."},{"header":"3.1 Supporting Interview Chatbots","body":"First, recent studies show that other researchers have used Juji to build various interview chatbots, which matches our focus on aiding the design of efective interview chatbots [37, 61, 65, 66]. Building and deploying an interview chatbot on Juji is very similar to creating a survey on a popular survey platform like SurveyMonkey or Qualtrics. Designers use Juji’s GUI to enter a set of interview questions and Juji will automatically generate a publicly accessible interview chatbot with a set of default conversational skills [67]. Juji also automatically handles side talking and keeps a conversation on track to ensure the completion of an interview [65]."},{"header":"3.2 Supporting Non-IT Designers","body":"For non-IT chatbot designers, Juji relieves them from implementing many needed AI skills of interview chatbots while providing them with much freedom to customize a conversational experience. Specifcally, Juji ofers a graphical user interface (GUI) for chatbot designers to create, deploy, and manage their custom chatbots without coding [59]. Below are some of the common chatbot customizations supported by Juji GUI.\nCustomize chatbot questions/messages. Not only can designers easily add/edit/delete text-based chatbot questions, but can also customize questions or messages by adding paraphrasing, inserting URLs or images, and using functions in such text (Fig 3(a)). For example, one can insert a function to retrieve an interviewee’s name to personalize a conversation.\nCustomize chatbot responses and persona. Designers can customize chatbot responses to user input by either directly reusing Juji pre-built conversations [28] or defning their own if-then statements. For example, Fig 3(b) shows such a customization. It states that if interviewees’ responses contain positive sentiment, the chatbot would then acknowledge such input accordingly. Additionally, one can customize chatbot persona or the pace of a conversation.\nAccess interviewee responses. To help designers monitor interview progress and make design adjustments, Juji provides designers with an interactive report dashboard that displays interviewee responses visually Fig 3(c). These responses are automatically extracted from the interview conversations. Designers can also download all interviewee responses into a CSV fle that contains all the question-response pairs collected. Because non-IT designers can use Juji to create working interview chatbots, Juji platform is suitable for testing whether our approach can assist any chatbot designers to improve their chatbot iteratively."},{"header":"3.3 Supporting Easy Integration and Public Access","body":"Lastly, the extensibility of Juji makes it easy for us to build and integrate iChatProfle. Specifcally, Juji provides APIs for developers to access chatbot services and extend Juji chatbots with third-party functions [27]. Moreover, Juji is publicly available and provides an easy access for our study participants and also for others who wish to replicate or extend our work."},{"header":"4 FORMATIVE STUDY AND DERIVED","body":"ICHATPROFILE DESIGN GUIDANCE\nTo guide the development of iChatProfle, we conducted a study to frst identify the types of design assistance desired by chatbot designers.\nFrom a public university, we recruited fve students (3 males and 2 females, age ranges from 18 to 31) who were interested in building chatbots. None of them reported any prior chatbot design experience. Three of them interacted with conversational agents like Siri or Amazon Alexa. Our study was a semi-structured, face-to-face online interview. Each interview lasted about an hour and each participant received $20 for their time. At the beginning of each interview, the participants were asked about their past chatbot design experience. The participants were then given a 15-minute tutorial of Juji. They were also encouraged to try diferent Juji features and get themselves familiar with the Juji GUI. After the tutorial, the participants were asked to use Juji to design an interview chatbot that elicits user input about the COVID-19 pandemic. They were given a list of questions on this topic (Table 1).\nWe selected this set of interview questions for three reasons. First, we wanted to ensure the practical value of our tool development\nefort, which is to help designers build interview chatbots for realworld uses (e.g., practical user research). Second, we wanted the interview questions to appeal to a wide audience who would be interacting and evaluating the designed chatbots. Third, COVID-19 is a pressing topic that satisfes both criteria.\nIn this study, we intentionally did not set any specifc design requirements because we wished to observe what the participants would do and the challenges they would face. Each participant was allotted 30 minutes to design their interview chatbot. The allotted time was determined based on the results of a pilot study where 3 participants could accomplish such a design task well within 30 minutes. After completing the task, participants were interviewed to discuss the types of design help they had hoped to receive during their design process. We transcribed the audio conversations from these discussions.\nWe followed qualitative analysis methods and the grounded theory [40, 68] to code the participant interview data. During the frst pass, two coders individually reviewed and coded participants’ responses. They then met and discussed their respective codes to identify common themes and reconcile diferences. Below we report the main fndings, which infuenced the design of our computational framework for evaluating the performance of interview chatbots as well as the development of iChatProfle."},{"header":"4.1 Two Types of Design Assistance Wanted","body":"During the participant interviews, all participants expressed the importance of receiving design assistance. Our analysis also revealed two main types of design assistance that the participants wanted. The frst type (T1) is objective, quantitative feedback on their existing chatbot design that could help designers understand the chatbot defciencies and point them to the right directions to improve their chatbot. Almost all participants expressed the need for receiving such feedback on their chatbot. For example, one participant mentioned that \"I hope to receive some feedback telling me the exact score my chatbot will get ... Just like those user ratings on the website of Alexa skills.\".\nThe second type (T2) is design suggestions for improving a chatbot. Almost all participants expressed that they still would not know what to do even if a quantitative evaluation was available. For example, one participant stated \"I am really new to this (interview chatbot design). I am afraid even I was told this part should be improved, I still don’t know how. More specifc design suggestions would be of great help.\" This indicates that designers also wish to receive concrete and actionable design suggestions that could guide them to improve a chatbot.\nIn addition to obtaining design guidance, the participants also expressed the need of viewing relevant conversation examples in\nthe chatbot \"debugging\" process. For example, one participant stated \"When doing chatbot response customization, I thought a lot about the wording choice since we all know that everyone’s having a hard time during this pandemic. I hoped my chatbot can always be empathetic but I have to admit it might not be the case due to so many diferent real-world cases.\" In such a case, providing designers with the actual conversation fragments (evidence) might help them better grasp the conversation situations and improve their chatbot. In other words, augmenting design suggestions with real conversational examples would also be helpful.\n4.2 iChatProfle Design Guidance Based on the desired design assistance, we derived three design goals of iChatProfle so it can fulfll designers’ needs:\n• Evaluate the performance of an interview chatbot quantitatively and present the evaluation results in a structured way. (G1) • Provide specifc, actionable design suggestions based on the evaluation results to help a chatbot designer improve the chatbot. (G2) • Augment design suggestions with evidential conversation examples to guide a chatbot designer to make design choices. (G3)\nIn addition to the three goals directly determined from the fndings of our formative study (G1 from T1, G2+G3 from T2), we derived another two criteria to guide the implementation of iChatProfle for practical purposes:\n• Adoption. Ensure that non-IT experts can easily utilize iChatProfle. (C1) • Compatibility. Ensure that iChatProfle can be utilized regardless which chatbot platforms are used for designing chatbots. (C2)\nWe derived C1 as iChatProfle is intended to help chatbot designers especially those with no AI/NLP expertise to design, evaluate, and improve interview chatbots. As a result, our efort will help democratize the applications and adoption of conversational AI. The purpose of C2 is to enable iChatProfle to be used with a wide range of chatbot platforms beyond Juji and beneft more designers. Following the goals (G1-G3) and the criteria (C1-C2), we designed iChatProfle as discussed in section 6."},{"header":"5 COMPUTATIONAL FRAMEWORK FOR","body":"QUANTIFYING INTERVIEW CHATBOT EFFECTIVENESS\nSince our formative study indicated that chatbot designers wish to obtain certain quantitative feedback on the performance of their existing chatbot (T1), we frst formulated a computational framework that quantitatively measures the efectiveness of such an interview chatbot from multiple aspects. The framework aims at achieving two goals: 1) providing quantifed insights into the performance of an interview chatbot; 2) using such insights to provide specifc and practical design suggestions for improving the chatbot.\nBased on the previous work on assessing human interviews [3, 7, 15, 20, 24, 50], communication theories for conducting efective interviews [47, 67], and evaluating chatbot efectiveness [12, 19, 55,\n65, 70], we formulated a set of performance metrics to quantitatively assess the efectiveness of an interview chatbot from three main dimensions: elicitation ability, user experience, and ethics.\nTo ensure both the coverage and practicality of chatbot evaluation, we used four criteria to choose our metrics. First, we selected only metrics that can be used to generate design suggestions and help designers improve an interview chatbot. Second, we chose metrics to measure both a chatbot’s abilities to complete an interview task efectively (elicitation abilities) and a user’s experience with the chatbot (user experience) because an ideal interview chatbot should be able to complete interview tasks while delivering a satisfactory user experience. Moreover, we included metrics to evaluate the ethics of an interview chatbot because such a chatbot might engage with a user in a conversation on private and sensitive topics [3, 67]. Third, we chose metrics to measure the performance of an interview chatbot both “locally” (interview question level) and “globally” (interview level). For example, the metric informativeness measures the amount of information conveyed by user responses to each interview question, while the metric user sentiment measures a user’s overall interview experience with a chatbot. This is to ensure specifc design suggestions can be generated to help designers improve a chatbot question by question (locally), while providing designers with an assessment of the overall interview experience (globally). Fourth, we chose only metrics that can be easily obtained/computed from available data (e.g., chat transcripts). This is to facilitate real-time, automatic assessment of chatbot performance and design suggestion generation. Table 2 summarizes all the metrics."},{"header":"5.1 Elicitation Ability","body":"The primary task of interview chatbots is to elicit high-quality responses from participants. Existing literature shows that the success of an interview is often determined by two aspects: the elicited response quality and level of user engagement [7, 15, 19, 43, 55, 65]. We thus model an interview chatbot’s elicitation abilities from two sub-dimensions: response quality and user engagement. While response quality directly assesses the quality of user responses to an interview question, the level of user engagement quantifes how much a participant is engaged with a chatbot from multiple aspects (e.g., how long an engagement is).\n5.1.1 Response Qality. We developed a metric to evaluate the quality of user interview responses.\nInformativeness. This metric indicates how much information a user’s text response contains. Similar to the metric used in [67], we measure a word’s surprisal—a word’s rareness appearing in modern English [63]. To enable easy reuse of our metric regardless which English dictionary is used, we compute the informativeness of a user input (U ) as a sum of the normalized surprisal of each word in U :\nNÕ surpr isal (wordn )−min_surpr isal In f ormativeness(U ) = (1)max _surpr isal −min_surpr isal\nn=1\nHere min_surprisal and max_surprisal are the minimum and maximum of surprisal, computed among all words in the vocabulary. N\nrepresents the word count within the response. Currently, we use the Wikipedia Corpus [1] to estimate word frequency.\nThis metric (e.g., a low informativeness score) can signal designers that there are potential issues with an interview question. For example, a question might be too broad and follow-up questions are needed to elicit more informative responses.\n5.1.2 Level of User Engagement. In the context of interviews, the level of user engagement measures a user’s behavior during an interview [15]. Specifcally, we have defned a set of metrics to assess a respondent’s behavior when engaging with an interview chatbot. Response Length. This metric computes the word count of a respondent’s free-text response to an interview question. We chose this metric because previous work indicates that engaged respondents are more willing to give long responses [67]. Designers can use this metric to gauge their chatbot performance and to make corresponding design improvements (e.g., adding follow-up questions or changing a yes/no question to an open-ended question to elicit longer responses). Engagement Duration. This metric indicates how long a participant is willing to engage with an interview question. Although engagement duration alone does not signal the quality of user responses [67], we hope to use it as an indicator of potential issues with an interview question. For example, if the engagement duration of a particular open-ended interview question is exceedingly short, it might signal that the question is too narrow and needs to be rephrased to encourage more open and longer engagement. Completion Rate. This metric computes the percentage of participants completing an interview question or an entire interview. It is a commonly used metric to measure the efectiveness of an interviewer [7, 20]. To better help designers improve their chatbots question by question (see Section 6), we compute the completion rate for each interview question (Q) by counting the number of users who completed the question (Cq ) and the number of users who responded to the question (Tq ):\nCompletionRate(Q) = Cq /Tq (2) For the frst interview question (when n = 1), we directly use the number of participants as the numerator. This metric (e.g., a low completion rate) can be used to signal potential issues related to an interview question (e.g., too vague) or the chatbot’s inability to handle user responses to the question (e.g., user’s expressed unwillingness to answer this question). A low interview-level completion rate could also refect potential issues with an interview (e.g., too many questions). Corresponding design suggestions can then be made to help the designers improve the chatbot."},{"header":"5.2 User Experience","body":"Informed by literature in interaction design [54] and interview design [7, 20], we proposed fve metrics to measure a user’s experience with an interview chatbot. User Satisfaction Rating. This metric is directly computed from participants’ ratings of their chatbot interview experience. This rating can be easily obtained: when piloting an interview chatbot, a question like \"How satisfed are you with the interview experience?\"\ncan be added at the end of an interview session for a participant to report their level of satisfaction. User Trust Rating. This metric measures participants’ perceived trust in an interview chatbot. Trust is important because it afects participants’ willingness to share information [37]. Similar to obtaining the user satisfaction rating, a question like \"How much do you trust this chatbot? Please rate it on a scale of 1 to 5\" can be added at the end of an interview when piloting an interview chatbot. User Sentiment. This metric evaluates participants’ sentiment toward an interview chatbot since such a metric is widely used to measure user satisfaction with interviews/surveys [7]. To obtain user sentiment, one can elicit participants’ rationale (why) when eliciting their satisfaction rating and trust rating during pilot interviews. Currently, we use the Vader model [16] to perform sentimental analysis on the collected users responses, and compute the percentages of positive, neutral and negative responses. Level of Empathy. This measures the level of empathy an interview chatbot has since research shows that an empathetic chatbot is able to elicit higher quality responses [65]. Currently, we compute the level of empathy by the frequency of empathetic words used by a chatbot. Specifcally, given a conversation segment associated with interview question Q, we normalize the number of empathetic words (Ec ) over the total number of words within chatbot utterances (Tc ) in this segment:\nLevelO f Empathy(Q) = Ec /Tc (3)\nWe extracted the empathetic words from EmpatheticDialogues by identifying top 15 content words from each of its 32 emotion categories [53]. This metric can help designers identify chatbot responses that lack of empathy and make corresponding improvements. Although more sophisticated algorithms can be used to measure empathy [70], we opted for the current approach that requires little training so that others can easily adopt it even without AI/NLP expertise or training data. Repetition Rate. This metric computes the frequency an interview chatbot has to repeat itself during an interview. We include this metric for two reasons. First, repetition afects the quality of a dialogue system, which in turn infuences user experience [55]. Second, repetition may signal a chatbot’s inability to handle certain user input. For example, a chatbot might not\nbe able to handle unexpected user input and have to re-ask an interview question [65]. Currently, given a conversation segment associated with an interview question (Q), we normalize the number of the repeated bi-grams (Rc ) over the total number of bi-grams (Tc ) of chatbot’s utterances:\nRepetitionRate(Q) = Rc /Tc (4)"},{"header":"5.3 Ethics","body":"An interview chatbot may engage participants in a conversation on private or sensitive topics or the participants may voluntarily ofer private and sensitive information [14, 23, 37, 70]. It thus is important to build ethical chatbots that respect participants as well as protect their privacy. We thus have developed two metrics to evaluate the ethics of an interview chatbot. Hate Speech Rate. This metric assesses how much an interview chatbot includes hate speech in its utterances. Such assessment becomes even more important if a chatbot uses auto-synthesized responses as what Tay was using [58]. Currently, we use an automated hate speech detection algorithm to compute the hate speech rate [10]. This metric can help chatbot designers be better aware of a chatbot’s built-in AI capabilities and correct a chatbot’s behavior if needed. Privacy Intrusion Rate. This metric evaluates how much an interview chatbot elicits private or sensitive information from a participant (e.g., password or social security number). Currently, for each interview question, we frst identify “sensitive” words/phrases appearing in chatbot utterances or user responses using Google’s Data Loss Prevention (DLP) API [9]. These words, such as a social security number, might risk a user’s privacy. Given a conversation segment associated with an interview question (Q), we compute the rate as follows:\nPrivacyIntrusion(Q) = Sc /Tc (5)\nHere Sc is the count of sensitive words appearing in the user responses and Tc is the total word count in user responses. To better protect a user’s privacy, chatbot designers can use this metric to curb an interview chatbot from eliciting such information or reminding a user of not giving up such information unnecessarily during an interview."},{"header":"6 ICHATPROFILE","body":"To help chatbot designers evaluate and improve an interview chatbot iteratively, we have developed a tool called iChatProfle, following the design goals and design criteria summarized in Sec 4.2. It automatically computes the metrics (Table 2) to assess the performance of an interview chatbot and generates a chatbot profle. Based on the profle, iChatProfle also automatically generates a set of design suggestions for improving the chatbot."},{"header":"6.1 System Overview","body":"As shown in Fig 4(a), iChatProfle consists of three key components: chatbot profle generator, design suggestion generator, and profle presenter. The chatbot profle generator takes a set of chat transcripts as input and automatically computes all the chatbot performance metrics (Table 2) to generate a chatbot profle. In general, chat transcripts are the results of the live chats between an interview chatbot and its pilot/testing users. Once a chatbot profle is created, the design suggestion generator automatically generates a set of specifc suggestions with conversation evidence for improving the chatbot. The chatbot profle, the design suggestions, and the conversation evidence are then assembled together by the profle presenter and displayed in a visual dashboard for easy comprehension (C1). To\nmake iChatProfle easily work with any chatbot platforms, it is implemented as an independent tool and takes only chat transcripts as its input (C2)."},{"header":"6.2 Profling Interview Chatbots","body":"Given a set of chat transcripts, iChatProfle automatically computes all the metrics mentioned in Section 5 to assess the performance of an interview chatbot. Each chat transcript is frst segmented by interview question and each segment consists of one or more conversation turns (Fig 4(b)). Each metric (e.g., response length), except completion rate and user sentiment, is frst computed/extracted per transcript (user) and all the scores are then averaged across all transcripts (users). The completion rate and user sentiment are directly calculated from all the transcripts (e.g., Formula 2)."},{"header":"6.3 Generating Design Suggestions","body":"Given the computed performance metrics, iChatProfle automatically generates a set of design suggestions using a rule-based approach. To make each design suggestion actionable, we formulate rules based only on the question-level evaluation metrics (e.g., informativeness). Such a design suggestion can be used by a chatbot designer to further customize and tweak chatbot behavior around a specifc interview question. In contrast, it is difcult for designers to act upon an interview-level metric, such as user satisfaction rating, although it informs the designers the overall performance of a chatbot.\nBelow is an example rule. It states that if the computed repetition rate for an interview question (Q) is above a certain threshold, it then uses a template to generate a set of design suggestions that could reduce repetitions and improve user experience.\n1: if repetition rate(Q) > threshold then 2: generate-design-suggestions (reduce-repetition-template) 3: end if\nIn our current implementation, the default thresholds are determined by the corresponding metric scores of the opening question (Q1). This is because a recent study shows that the conversation around the very frst question could be used as a good indicator [67]. The only exception is for hate speech rate, where the threshold is set to 0. It means that if any hate speech is detected, design suggestions will be generated. Additionally, the thresholds can also be defned by designers themselves based on their needs.\nOnce a rule is triggered, iChatProfle automatically generates actionable design suggestions in two steps. First, it uses a templatebased approach to generate design suggestions in natural language [41]. Second, it automatically extracts relevant conversation fragments as evidence to substantiate the generated design suggestions.\n6.3.1 Template-based Natural Language Generation. For each metric, we have defned a template that contains one or more design guidelines for improving a chatbot (Table 3). These design guidelines are formulated based on previous research fndings and commercial product design guidelines (Alexa, Google Home and Juji) for improving interview quality and user experience [8, 11, 14, 22, 28, 44, 45, 55, 65, 70]. For example, there are two\nguidelines on improving the metric informativeness: one is to better articulate or explain an interview question to minimize ambiguity, while the other is to improve a chatbot with active listening skills to make users feel heard [65].\nGiven a template, it takes two steps to generate design suggestions in natural language: document planning and surface realization [41]. In document planning, we defne the content to be conveyed in four parts: (a) the design guideline (D), (b) the corresponding interview question (Q), (c) the corresponding metric (M), and (d) an explanation on why the design guidelines are given. In surface realization, we generate natural language statements by a template: \"For question Q, do D because metric M is Z\". Here Z is either \"too low\" or \"too high\", depending on which metric value triggers the generation. We have used a python library SimpleNLG[26] to automatically generate grammatically correct natural language sentences. The library helps organize basic syntactic structure (e.g., tense) and sentence elements (e.g., punctuation).\nUsing the example rule mentioned above, assume that the computed repetition rate for the interview question \"Where are you located?\" exceeds the threshold, signaling potential issues around this interview question. This triggers iChatProfle to generate a set of design suggestions as shown in Fig 1(b).\n6.3.2 Conversation Evidence Extraction. To act on design suggestions, chatbot designers may need more information to understand the conversation situations. As we learned from the formative study, it is difcult for designers to anticipate conversation situations. Continuing the above example, although a chatbot designer now knows that the interview question \"Where are you located\" has caused high repetitions, s/he might not know what caused the repetitions. In such a case, providing designers with the actual conversation fragments (evidence) might help them better grasp the situations and make chatbot improvements.\nThus, iChatProfle automatically extracts relevant conversation fragments from chat transcripts to give designers more concrete ideas on how to improve a chatbot. These conversation fragments are essentially concrete evidence to show chatbot designers what went wrong. However, such fragments might be too many, which would not help the designers but overwhelm them. We have thus used GloVe embeddings [48] and a k-means algorithm [33] to select the most representative conversation fragments in three steps.\nGiven an interview question (e.g., “where are you located”) and a performance metric (e.g., repetition rate), iChatProfle frst selects all conversation segments (Fig 4(b)) that produced a metric score worse than the threshold. These selected segments are then encoded by GloVe embbedings. Second, these segments are grouped into k clusters based on their cosine similarity. Elbow method is used [32] to fnd the optimal number of clusters (k). Third, iChatProfle then ranks the clusters by coverage (i.e., the number of segments in each cluster). Within the top-K clusters, one conversation segment is randomly selected per cluster as the representative evidence to substantiate the design suggestions. Currently, K is determined by the available space in the visual dashboard after displaying the design suggestions. The rest of the conversation segments can also be accessed through a hyperlink.\nUsing the above example on the interview question of “Where are you located”, two clusters are formed, one with the coverage of 75.0% and the other 25.0%. Assuming K=2, iChatProfle selects one conversation segment from each cluster (Table 4 and Table 5)."},{"header":"6.4 Presenting Chatbot Profle and Design Suggestions","body":"To present a generated chatbot profle, design suggestions, and relevant conversation evidence, we used Tableau [57] to implement a web-based, interactive visual dashboard (Fig 1). A displayed chatbot profle consists of all computed metrics visualized in various forms\ndepending on the type of information. For example, response length is visualized in a bar chart while user sentiment is displayed in both a pie chart (showing the percentages of each type of sentiment) and word clouds (Fig 1(a)). The profle also visually indicates the thresholds that would trigger design suggestions, which helps designers better understand the meanings of metric scores and make design decisions. Users can interact with each metric to view corresponding design suggestions if there is any. If a performance metric (e.g., informativeness) deems to be improved, iChatProfle presents the generated design suggestions and conversation evidence (Fig 1(b))."},{"header":"7 EVALUATION","body":"To evaluate the efectiveness of iChatProfle, we designed and conducted a between-subject user study that compared the performance of 10 chatbots designed with or without using iChatProfle."},{"header":"7.1 Study Method","body":"Using the same set of interview questions about COVID-19 shown in Table 1, we frst built an interview chatbot on Juji using only Juji’s built-in features without making any customization. After asking all the interview questions, the chatbot also included questions to elicit user satisfaction rating and trust rating, as well as their rationale behind each rating. This chatbot served as our baseline. The baseline chatbot was deployed on the web to engage with respondents in a live chat. The pilot study collected a total of 128 chat transcripts. Using these transcripts, iChatProfle automatically generated a chatbot profle and corresponding design suggestions to improve the baseline chatbot.\nSince we wished to compare chatbot performance with and without using iChatProfle, we recruited 10 chatbot designers, who were randomly divided into two groups, 5 in each group. Each designer started with a 15-minute tutorial of the Juji platform by watching a tutorial video and learning several key Juji features (e.g., how to customize a chatbot’s actions). They were given additional time to play with Juji and get familiar with various design features. Each designer was then given the baseline chatbot for them to import into their own account so they could preview and improve the baseline. They also had access to the report dashboard and all the interviewee responses extracted from the 128 conducted chatbot interviews as described in section 3.2. They were asked to describe the good and bad aspects of the baseline chatbot. Next, they were asked to improve the baseline chatbot along three dimensions: user response quality, user experience, and ethics. They were allowed to use any chatbot customizations (e.g., rewording a question or customizing a chatbot’s reactions to user input) as long as all the original interview questions and the question order were kept. All the designers in one group (Group B, w/ iChatProfle) were also given iChatProfle to view the generated profle of the baseline chatbot and corresponding design suggestions, while the other group (Group A, w/o iChatProfle) was not given the tool but only the interviewee responses . We also collected the participants’ demographics, including their gender and age, and their chatbot experience (chatbot interaction or design experience).\nEach designer was allotted about 30 minutes to improve their chatbot. A post-task interview was also conducted. The designers in Group w/o iChatProfle were asked about their design and the\nchallenges/difculties they faced during their chatbot design process. The designers in Group w/ iChatProfle were asked to describe their design and their experience of using iChatProfle. Because of the COVID-19 pandemic, the whole study was conducted online via an 1:1 Zoom meeting. On average, each study session lasted about an hour.\nTen (10) designers from the two groups built a total of 10 chatbots based on the baseline chatbot provided to them. Each of these chatbots was deployed on the web to engage with respondents in a live chat."},{"header":"7.2 Participants","body":"All chatbot respondents, including the ones in the pilot study, were recruited on Amazon Mechanical Turk (MTurk) with an approval rating equal to or greater than 99% and located in the U.S. or Canada. Each participant was paid $12.5/hr.\nThe 10 chatbot designers (6 males, 4 females, ages 20 to 35) were students recruited from a public university majoring in diverse disciplines, including Computer Science, Information Science, Psychology, and Environmental Studies. Two (2) participants reported prior experience of building chatbots, which fve (5) of them reported prior experience of interacting with conversational agents, like Siri, Amazon Alexa or Google Assistant. None of them had built interview chatbots or used Juji. Each participant was paid $20 for their time."},{"header":"7.3 Study Results","body":"From the 10 deployed interview chatbots, we collected a total of 1349 interview transcripts including the transcripts of incomplete interviews. We kept the incomplete ones because they could indicate the performance of a chatbot. On average, each chatbot interviewed 135 users (135 chat transcripts). Given their respective interview transcripts, iChatProfle computed ten (10) chatbot profles to characterize the performance of each of the chatbots. We then compared the computed chatbot performance between the 5 chatbots (702 transcripts) designed by the participants in Group w/o iChatProfle and another 5 chatbots by Group w/ iChatProfle (647 transcripts) using the tool.\n7.3.1 ANCOVA Analyses. Specifcally, we performed a series of ANCOVA analyses, which blend analysis of variance (ANOVA) and regression [30], to examine the efect of with or without using iChatProfle (independent variable) on various chatbot performance metrics (dependent variables). We ran ANCOVA analyses on every metric in Table 2 except three due to a lack of data: completion rate (10 samples), user sentiment (10 samples), and hate speech rate (no data). Both completion rate and user sentiment were computed per chatbot (a total of 10 chatbots) and our algorithm did not detect any hate speech in any of the chatbots.\nWe compared the chatbot performance between Group w/o iChatProfle (702 transcripts) and Group w/ iChatProfle (647 transcripts). The assumption check was conducted to make sure the unequal sample size would not afect the reliability of the results. In each analysis, the independent variable was the group and the dependent variable was one of the chatbot performance metric scores computed. All analyses were controlled for designers’ differences, including their gender and chatbot experience. We did\nnot control their age because they all are of the similar age. Whenever applicable (e.g., for informativeness but not user satisfaction rating), each analysis was additionally controlled for respondents’ diferences—the corresponding metric score of the frst question (Q1). This is because prior study shows that a respondent’s behavior in the opening question is a signifcant predictor of his/her behavior in the entire interview [67].\nSimilarly, we ran ANCOVA analyses to compare each chatbot’s performance metrics between Group w/o iChatProfle and the baseline (128 transcripts), and between Group w/ iChatProfle and the baseline, respectively. Bonferroni correction was applied to adjust p values.\nSince all the designers were given the same goal to improve a chatbot along three dimensions, our analyses were to answer two questions:\n• RQ1: Did iChatProfle help designers build better interview chatbots? • RQ2: How did iChatProfle help make chatbot design decisions?\nBefore running ANCOVA analyses, we also examined the correlations among all dependent variables. Consistent with prior fndings [67], informativeness was not correlated with engagement duration. Moreover, engagement duration did not signifcantly correlate with\nany other metrics except repetition rate. We also noted that a chatbot’s empathy level signifcantly correlated with informativeness and response length. Intuitively, this result is sensible since respondents would be more cooperative with an empathetic chatbot [65].\n7.3.2 iChatProfile helped designers build beter chatbots (RQ1). Table 6 and Table 7 summarize the analysis results. The results show that iChatProfle helped create chatbots that performed signifcantly better at both interview and individual question level. At the interview level, for example, the chatbots in Group B achieved a higher completion rate (78% vs. 74% vs. 74%) and more positive user sentiment (66% vs. 45% vs 46%), than those in Group w/o iChatProfle and the baseline.\nAt the question level, the chatbots in Group w/ iChatProfle also performed better than those in Group w/o iChatProfle and the baseline on almost all dimensions, including response quality (informativeness), user engagement (response length and engagement duration), and user experience (level of empathy and repetition rate). Only the diference in privacy intrusion rate is insignifcant. This is because the interview questions (Table 1) did not elicit much private or sensitive user information. Our results also indicated that the performance diferences between the chatbots in Group w/o iChatProfle and the baseline are mostly insignifcant. In fact, Group w/o iChatProfle performed worse than the baseline on certain metrics, such as completion rate and user satisfaction rating. This implies that designers had difculty improving a chatbot without any specifc design guidance. Moreover, making improvements without knowing chatbot defciencies could even hurt the chatbot performance. For example, a designer in Group w/o iChatProfle added a follow-up question \"What made you feel that way” to interview question Q4 \"What challenges are you facing”. But he ignored user responses that already talked about their feelings when answering Q4, which made users feel unheard. No wonder one user commented ”I have already stated that, you were unable to understand answers.”\nIn all the analyses, the use of iChatProfle was a signifcant factor impacting the chatbot performance diferences. Three control variables, Q1, gender, and chatbot experience, were shown signifcant for a few analyses, although none of these control variables had interaction efect with the use of iChatProfle. Control variable Q1 was signifcant for informativeness, response length, and engagement duration. Since Q1 was used to account for respondents’ diferences [67], the efect of Q1 implies the efect of respondents’ behavior (e.g., uncooperation) on interview quality, consistent with previous fndings [67]. In addition, control variable gender signifcantly impacted a chatbot’s empathy level, and one’s chatbot experience infuenced informativeness and response length. It is interesting that the chatbots made by male designers were more empathetic than those made by female designers ( Male 0.026 vs. Female 0.003, p<0.05 ). Although one’s chatbot experience helped make chatbots better at eliciting information (e.g., informativeness), it had no efect on user experience, such as user satisfaction rating or user trust rating.\nGuiding Designers to Make Practical Chatbot Improvements Recall that at the beginning of their task, all designers were asked to comment on the baseline chatbot and their plan to improve it. All of them gave vague descriptions or improvement plans. However,\nadditionally controlled for respondents’ diferences (the corresponding metric in Q1). c. informativeness and length were additionally controlled for engagement duration; and engagement duration was controlled for length [65].\nafter the designers in Group w/ iChatProfle had access to iChatProfle, they seemed knowing what they needed to do. On average, the designers in Group w/ iChatProfle did 495% (99 vs. 20) of chatbot customizations compared to those in Group w/o iChatProfle (Table 8).\nSpecifcally, the designers in Group w/ iChatProfle appreciated the design suggestions and evidential conversation examples. We examined the chatbots made by the designers in Group w/ iChatProfle and observed that all of them followed one or more design suggestions given by iChatProfle. For example, the two design suggestions, \"add customizations to show the chatbot is actively listening\" and \"reword the question to make it more acceptable to users\", were followed by all 5 designers in Group w/ iChatProfle to customize and improve the baseline chatbot behavior around interview questions Q2 and Q4 (Table 1). In addition to following the design suggestions, the designers in Group w/ iChatProfle also found the conversation examples very helpful. While the design guidelines informed designers what to do (e.g., rewording a question), the conversation examples helped them fgure out how to do it. For example, Q2 “where are you located” asked respondents about their location. When chatting with the baseline chatbot, some respondents were unclear about the question, which caused a higher repetition rate. iChatProfle generated a design suggestion \"reword the question to make it more acceptable to users\" with conversation examples (Table 4-5). One designer who followed the suggestion reworded the original question to “May I ask where are you located? No need to be very specifc, just city name would do :)”. He stated in the post-task interview that \"I read the bad example and realized that some people might not like this kind of questions directly asking for their personal information, so I changed it.\"\nIn comparison, the chatbot customizations made by designers in Group w/o iChatProfle were fewer and with a higher percentage of unmatched chatbot customizations (43% vs. 33% in Group w/ iChatProfle ) — customizations that were not suggested by iChatProfle. Table 8 shows the design suggestions given by iChatProfle and the chatbot customizations made by designers two groups, respectively. From our observations, none of the designers from Group w/o\niChatProfle went through all the 128 interviewee responses. They mainly reviewed the report dashboard and randomly selected a few responses to examine, without specifcally knowing what to learn from the interview results let alone how to improve the chat based on the results. Without any guidance from a tool like iChatProfle, the designers in Group w/o iChatProfle made their customizations based on their intuition or ad hoc reasons. For example, when asked \"why did you decide to add these customizations\", one designer in Group w/o iChatProfle said \"Because the functionality of adding customization provided by Juji is a large block (a big area of the interface). It is very noticeable and I decided to try it out.\". Another designer also stated that \"Juji provides so many features to choose from and I’m not sure which one to use ... I decide to add them all in the end.\". Since the designers in Group w/o iChatProfle didn’t use iChatProfle, a lack of guidance for identifying chatbot defciencies or improvements defnitely contributed to their inferior chatbot performance.\nInspiring Designers to Make Creative Chatbot Improvements In addition to guiding designers to make practical chatbot improvements, iChatProfle also inspired designers to make creative chatbot improvements beyond what was suggested by the tool. For example, one designer in Group w/ iChatProfle decided to add transitions between interview questions, \"Come on, get up and do 10 bobby jumps before we continue. Cheer up!\". During the posttask interview, when asked why he made such a design decision, he mentioned that \"I noticed the profle shows the original design (baseline) did not engage people well. So I thought why not engage them physically?\" We also checked the feedback left by the respondents who chatted with this chatbot. We noticed the positive comments such as \"You (Juji) asked me to do bobby jumps. I didn’t actually do it but it’s interesting and I like it.\""},{"header":"8 DISCUSSIONS","body":"While our study results are encouraging, the study also revealed several limitations. Here we discuss these limitations and future\nwork. We also briefy discuss design implications of our work on building chatbots beyond interview chatbots."},{"header":"8.1 Limitations","body":"8.1.1 Study Scope and Participants. While our results should be widely applicable for building a class of interview chatbots, the scope and the participants of our study present its limitations. Our study reported here focused on a low-stakes interview task (e.g., user or market research interviews) and recruited all the interviewees on Amazon Mechanic Turk. It is unclear whether our results would hold for diferent types of interview tasks, such as high-stakes tasks like job interviews with much more motivated interviewees and additional chatbot requirements (e.g., detecting faking [71]). Moreover, in our study all the chatbot designers were university students. It would be interesting to investigate how our results would hold or change with diferent chatbot designer groups (e.g., experienced chatbot designers).‘\n8.1.2 Ofering Finer-Grained Design Suggestions. Currently, iChatProfle often ofers multiple design suggestions per performance metric. For example, it ofers two suggestions if the response length is below a threshold (Table 3). However, under certain circumstances, one suggestion might be more useful than others. Using the example for improving response quality due to a vague question, adding explanations to the question would be more useful than making the question more acceptable to users. This requires that iChatProfle further discerns the causes to the chatbot performance so it can narrow down the design suggestions and recommend the most suitable one. One potential method to address this is to analyze user responses and identify diferent semantic themes, similar to the data-driven methods used by others to recognize the semantic themes in user input [65]. Based on the recognized themes and the computed performance metric, iChatProfle can recommend the most suitable design suggestion(s).\nAdditionally, iChatProfle currently produces evidential conversation examples along with design suggestions, which proved to be helpful for designers in our study. However, these examples are the “negative examples” extracted from existing chats and no positive examples are given. For example, if the design suggestion is to “give empathetic feedback”, it would be helpful for a designer to see a “positive example”—what an empathetic feedback is like. Again, this would require more usage data, which will then allow iChatProfle to extract “good examples”.\n8.1.3 Evaluating iChatProfile Usability. Although our ultimate goal is to help designers build efective interview chatbots, we have not yet evaluated the usability of iChatProfle for two reasons. First, we want to verify its usefulness and efect before evaluating its usability. Second, our current implementation is standalone and not integrated with any chatbot platforms. Thus certain operations are cumbersome involving much manual work (e.g., manually downloading all the chat transcripts from a chatbot platform and then uploading them into iChatProfle). A more integrated version should be created and then usability evaluation makes better sense."},{"header":"8.2 Future Work and Design Implications","body":"There are several directions that we can extend iChatProfle to refne its functions and expand its uses.\n8.2.1 System Explainability. As mentioned in Section 6.3.1, iChatProfle provides explanations on why certain design suggestions are given. Our study participants expressed their appreciation of such explanations. However, when system suggestions were inconsistent with designers’ belief, current explanations need to be expanded. For example, one designer stated \"The score of trust level doesn’t actually refect my experience, I am wondering why.\". In such cases, deeper explanations would be helpful than just stating “the score is lower than a threshold\" .\nOne possible direction of future work is to construct a multi-layer framework for evaluating the performance of interview chatbots. Following [52], this framework could have three layers: design quality (e.g., the question-level performance metrics used in our current work), user belief (user perceived chatbot performance, such as perceived usefulness[51] and ease of use [6]), and user attitude (users’ overall feelings towards the whole chatbot, such as perceived trust and satisfaction in our work). A path model can then be generated to reveal causal relationships between diferent layers to make chatbot profling and design suggestions more explainable. In particular, such relationships could explicate how design qualities may infuence users’ attitude more clearly, or why a certain metric could contribute to the success/failure of the overall design through an infuence path across layers [52].\n8.2.2 Benchmarking Interview Chatbot Evaluation. As our studies show, it is difcult for designers to evaluate chatbot performance without any guidance. While iChatProfle helps designers make specifc chatbot improvements, it could not inform the designers how much test an interview chatbot actually needs before achieving an\nacceptable performance. For example, it would be very valuable to inform a chatbot designer that a minimal of N pilot users or at least M rounds of evaluations are needed to test a particular interview chatbot and achieve an acceptable performance. As iChatProfle collects more usage data, ofering designers with the above suggestions will become more feasible. Specifcally, an evaluation benchmark can be established for each type of interview question as well as for a particular type of interview chatbot. This will also enable us to establish diferent thresholds for diferent chatbot tasks.\n8.2.3 Real-Time Chatbot Evaluation and Feedback. Although we showed how iChatProfle was able to help improve chatbot performance signifcantly within just one iteration of design, building an efective interview chatbot often takes multiple iterations. Currently, iChatProfle generates a chatbot profle and design suggestions after a designer deploys the chatbot and collects a set of transcripts from live chats. However, designers may wish to receive prompt feedback while designing a chatbot, since early and timely feedback could improve creative work[34, 35]. To enable continuous chatbot evaluation and improvement, one approach is to integrate iChatProfle with crowdscourcing tools that can recruit testers, administer live chats, and provide a chatbot profle and design suggestions to improve the chatbot, all near real time [39]. Another approach is to employ deep learning algorithms to simulate real-world user behaviors so that chat transcripts can be obtained in real time [12] to aid the iterative evaluation and improvement of chatbots in real time.\n8.2.4 Assistive Design of Chatbots beyond Interview Chatbots. Our work demonstrates the efectiveness of iChatProfle for helping designers to evaluate and improve an interview chatbot iteratively. Since interview chatbots share many characteristics as other types of chatbots, such as counseling or training chatbots, iChatProfle could be extended to help designers build such chatbots as well. Especially iChatProfle aims at helping non-AI experts in chatbot design, it could help professional coaches or trainers design, evaluate, and improve their own chatbots. We hope that our work can serve as a stepping-stone on the path to democratize chatbot design for a wide variety of applications beyond interview tasks."},{"header":"9 CONCLUSIONS","body":"We described a computational framework for evaluating interview chatbots and presented iChatProfle, a tool that helps designers to evaluate and improve interview chatbots iteratively. Given a set of chat transcripts, it automatically quantifes the performance of a chatbot and generates a chatbot profle. Based on the generated chatbot profle, it also ofers design suggestions in natural language with evidential conversation examples, which help guide designers to improve the chatbot. To validate the efectiveness of iChatProfle, we designed and conducted a between-subject study that compared the performance of chatbots designed with and without using iChatProfle. Based on the transcripts collected from the live chats between 10 chatbots and 1394 users, our results show that iChatProfle helped produce interview chatbots with signifcantly better performance across almost all dimensions, including response quality, user engagement, and user experience."}],"type":"Sections"}