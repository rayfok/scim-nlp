{"sections":[{"body":"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI â€™21, May 8â€“13, 2021, Yokohama, Japan Â© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445765\nvisual understanding of elicited gesture spaces. It further opens new directions, such as comparing elicitations across studies. We discuss implications for elicitation studies and research, and opportunities to extend our approach to additional tasks in gesture elicitation.\nCCS CONCEPTS â€¢ Human-centered computing â†’ Visualization systems and tools; HCI design and evaluation methods.\nKEYWORDS Gesture elicitation, dimensionality reduction, deep learning, visual analytics\nACM Reference Format: Duong Hai Dang and Daniel Buschek. 2021. GestureMap: Supporting Visual Analytics and Quantitative Analysis of Motion Elicitation Data by Learning 2D Embeddings. In CHI Conference on Human Factors in Computing Systems (CHI â€™21), May 8â€“13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3411764.3445765"},{"header":"1 INTRODUCTION","body":"Designing effective interactions and user interfaces often involves exploring two potentially high-dimensional spaces [65]: 1) The\nar X\niv :2\n10 3.\nspace of human behaviour (e.g. comfortable motion ranges of arm and hand), and 2) the space of senseable input in a system or context (e.g. tracking of up to X body joints in 3D).\nAlthough central to HCI, the field has developed few dedicated methods and tools for supporting the (joint) exploration of such user-sensor spaces (cf. [65]). One successful method that has seen widespread use is the elicitation study paradigm [66], which helps HCI researchers and practitioners to explore the space of possible and â€œintuitiveâ€ or â€œguessableâ€ (gesture) commands: Participants are shown a â€œreferentâ€ (often a system action, e.g. volume up) and are asked to propose and perform a gesture they would use for it (e.g. turn wrist right). This is repeated for several referents.\nResearchers then analyse these gesture proposals, compute measures to identify common proposals (e.g. [59, 66]), and decide on a set of gestures to be used in an interactive system, typically composed of the gestures with high agreement among participants (e.g. [56, 67]).\nIn this way, elicitation studies inform gestural interaction with user-driven exploration: Most studies focus on the human behaviour space and thus do not rely on a specific sensor; they typically video-record participants for manual gesture analysis (e.g. [14, 28]). Some additionally employ a sensor in elicitation (e.g. Leap [62], Kinect [56]), thus also potentially considering the senseable space.\nWhile elicitation studies have become a widely used staple in the HCI toolbox, they still present challenges (cf. [51, 63]), including the need for manual data analysis. This limits elicitation studies, as well as the general endeavor of systematically exploring behavioursensor spaces in HCI, as characterised in the following paragraphs:\nâ€¢ Workload: Watching videos to manually classify gestures (e.g. [14, 28, 56, 62, 67]) is tedious work [51]. It may thus also hinder the use of elicitation in user-centred processes that require repeating such work (e.g. iteration). â€¢ Subjectivity: As critically pointed out [51], manual interpretation at best requires further efforts (e.g. multiple coders); at worst, it leads to subjectively biased results. â€¢ Limited scale: Without quantitative analysis tools, largescale elicitation remains scarce (survey: mean=25 participants [63]). This stands in contrast to motivations for diverse samples and for training recognisers on elicited gestures [14, 25, 49, 63]. â€¢ Isolated results: The lack of quantitative data analysis methods and tools hinders replication, reuse, extensions, and comparisons across elicitations, even if the same sensor was used (e.g. Kinect). Most work collects and analyses new data [63], isolated from previously published datasets.\nThese challenges motivate our work on new quantitative methods and tools for analyzing elicitation data. Fittingly, recent related work highlighted the need and feasibility of more objective, computational measures [59], and called for further computational models and measures, based on a survey of 216 elicitation studies [63].\nAddressing this, we extend the computational toolbox for analyzing gesture elicitation data with these contributions:\nâ€¢ GestureMap, a method and tool for visualizing and exploring motion data from elicitation studies on an interactive, learned 2D map, inspired by concepts from visual analytics.\nâ€¢ New computational capabilities for gesture representation, consensus and clustering, based on average gestures computed with DTW Barycenter Averaging (DBA) [43], to connect exploration to quantitative measures, extending the computational approach motivated in recent work [59, 63]. â€¢ Insights from using GestureMap in a detailed case study on datasets from the literature, plus a qualitative expert evaluation with eight researchers."},{"header":"2 RELATEDWORK","body":"The analysis concepts introduced in this work are built on previous work spanning HCI, machine learning and visual analytics. This section briefly describes gesture elicitation studies, followed by an overview of tools that support researchers across different tasks involved in analyzing elicitation data. In particular, we outline existing analysis concepts for high-dimensional data."},{"header":"2.1 Gesture Elicitation Studies","body":"The gesture elicitation paradigm was first introduced by Wobbrock et al. [66] to elicit usersâ€™ interaction preferences for new systems. This method was then specifically adapted to include gesture proposals to control surface tabletop computers [67]. In a subsequent study, Morris et al. [68] confirmed that new users do prefer the user-defined gesture set over the one created by experts. Since then, this method has become a standard tool for the design of gesture input mappings for new interactive systems, for example to control a swarm of robots [28], smart-home appliances [26, 56], or AR/VR applications [44].\nCentral to gesture elicitation studies is an in-depth analysis of the proposed data to find common behavior. Researchers have therefore developed various measures to formalize the consensus among participants [56, 57, 61, 62, 67].\nHowever, these measures rely on subjectively assessing the similarity of the observed gestures: They require researchers to group proposals into subgroups that they consider identical, which is usually done by manual annotation based on watching videos of the participants in the study [28, 39]. Thus, while these measures set standards on how to compute consensus from gesture proposal, they cannot avoid subjectivity per se.\nTo address this, Vatavu [59] has recently proposed a new, datadriven approach: It employs a distance measure as an objective basis for assessing consensus in elicitation studies. Our work builds on this idea, extends its data-driven perspective with a visual analytics tool, and introduces a new measure fitting this visualization."},{"header":"2.2 Gesture Analysis Tools","body":"Several tools have been created for more effective and objective analyses. Video analysis has been the preferred evaluation method, but the annotation of individual video sequences can be timeconsuming [51]. An efficient analysis becomes evenmore important as large-scale gesture data sets can be collected online, for example, through cloud elicitation tools [2]. Thus, researchers devised different ways to distribute the work among people [1, 36].\nWhile the concepts introduced in this paper also enable researchers to better annotate sequences, our focus lies in particular on the exploration of elicited gesture data.\nNebeling et al. [41] created a tool to analyze recordings created by a Kinect camera sensor. They included three visualizations. First, they used a 3D animation of a Kinect skeleton. Second, they provided a visualization where only the moving joint is drawn on the canvas. The third visualization is similar to the second, but additionally employs a heat-map to emphasize the time domain.\nThe most similar work to ours is GestureAnalyzer by Jang et al. [24] which also focuses on the analysis of gesture elicitation studies. To find behavioral patterns, it employs a variation of the smallmultiples plot [52] and an interactive hierarchical clustering interface visualized in a tree layout. Their calculations and analyses are based on hand-engineered features. The gesture map which we propose in this work facilitates a richer exploration of the behavior space using machine learned features for the gesture poses. It provides an overview of the gesture data and introduces a new continuous traceable 2D paths which represent gesture sequences. For further discussions on the comparison of these two systems we refer to section 8.2.2."},{"header":"2.3 Visualization of High Dimensional Data","body":"A key challenge in visual analytics is the effective visualization of high-dimensional data. This typically involves two steps: 1) Projecting the data to 2D for display on a screen. 2) Suitably visualising the projected data, considering the analystsâ€™ tasks and goals. While there exist many dimension reduction techniques [20, 32, 38, 54, 55], we use a Variational Autoencoder [30] to reduce the dimensions of the raw sensor data. To visualize temporal data, a common representation is a line plot, horizon plot [16], or a small-multiples plot [52]. However, these highly abstract visualizations may occlude the nature of the underlying data. For example, these plots may hide the structure of a 3D skeleton recording. We therefore combine an abstract 2D mapping with a grid of representative 3D skeletons to give analysts a visual overview of the proposed gestures.\nAlso related to our work are tools to analyze and visualize machine learned representations of complex data: Deep learning models are capable of learning human-understandable features of high-dimensional data: For example, Kingma and Welling [30] and Lawrence [33] sample multiple points from the learned space and visualize them to demonstrate that the learned space is continuous and smooth, but without providing interaction functionalities. Smilkov et al. [48] filled this gap by providing a generic tool to visualize these embeddings.\nSome researchers created specific visualizations to facilitate interpretation of the axes of a (2D) projection, to judge the variation of the data [27, 60] or the relative importance of the data attributes along an axis [31]. Liu et al. [35] used a cartographic approach to compare and analyze learned embedding spaces. In this work, we adapt similar visualization concepts with the goal to create an interpretable gesture space.\nTo the best of our knowledge, GestureMap is the first tool to use a latent variable model to analyze sensor-based motion data in the context of gesture elicitation studies. We combine interactive k-means clustering, automatic metric computation, a new visualization, and analysis concepts to provide an integrated platform."},{"header":"3 GESTUREMAP CONCEPT","body":"We introduce a structured analysis approach based on a learned 2D gesture map, as realised in GestureMap. We motivate the conceptual features via related work as summarized in Table 1 and elaborate on them in the following sections."},{"header":"3.1 Feature Requirements and Overview","body":"The features in GestureMap were informed by close examination of the literature on gesture elicitation and related concepts and tools: We collected features 1) proposed in related work, 2) motivated in calls for further improvements, and 3) explicitly requested from future work. In addition, we included further ideas. Table 1 shows an overview of the relation to related work. The following paragraphs further introduce and motivate the features.\n3.1.1 3D Skeleton View (Figure 1b 2â—‹). Related tools [24, 41] show a 3D skeleton view with animation. GestureMap also offers this, to afford easy examination of a recorded gesture.\n3.1.2 2D Map View (Figure 1b 1â—‹). GestureMap is fundamentally motivated by providing researchers with a visual overview of the elicited gesture space.\nFurthermore, some researchers indicated that participants may struggle to propose gestures, if they are unfamiliar with the gesture design space [9, 12, 46]. They therefore modified elicitation such that people could choose from a predefined list of gesture proposals.\nGestureMap addresses these needs as its 2D map shows observed gesture proposals and gives an idea of past behavior. While we focus on researchers as users of this map in this paper, it could also be shown to participants as we described in Section 8.3.\n3.1.3 2D Map Overlays (Figure 1b 1â—‹, Figure 1c). Prior work has extensively used scatter plots to analyze machine learned representations [35, 48]. Our map view affords different plots on top of it, such as:\nâ€¢ Scatter plots (point = body posture; Figure 1b 1â—‹) â€¢ Drawing paths (path = gesture; Figure 4) â€¢ Densities (e.g. where in the space are postures and gestures located? Figure 1c)\n3.1.4 Linked Views of Postures. Villarreal-Narvaez et al. [63] called for future work to include multiple representations of gestures. GestureMap realises this by linking the 2D map and the 3D skeleton. Concretely, the 3D skeleton view updates while the user moves the cursor over the 2D map to present the posture at that point in the gesture space.\n3.1.5 Linked Animations of Gestures. Complementary to the feature for postures, GestureMap accounts for the temporal nature of gesture data [24, 41] by offering linked animations of gesture paths (point moving on the path) and 3D skeletons (skeleton moving).\n3.1.6 Gesture Clustering. As larger data sets are expected in the future [1, 24, 41], we also provide an interactive clustering method to reduce manual workload for identifying similar gesture (sub)groups.\n3.1.7 Sharing Results. Motivated by such interests in relatedwork [24, 36, 41], we include an export functionality to easily share analyses with other researchers."},{"header":"3.2 The Learned 2D Gesture Map","body":"Here, we describe the map concept in more detail.\n3.2.1 Core Visualization Concept. Following a cartographic approach [47], and in line with 2D projections in visual analytics (e.g. [27, 64]), we use a map metaphor to visually guide analysts through the elicited gesture space. This gesture map is a 2D plot with a grid of representative body poses shown as small human skeletons. These â€œpose landmarksâ€ give an overview of the poses in the corresponding rectangular map region (Figure 1b 1â—‹). The map itself is continuous, that is, each 2D point represents a pose. Thus, since gestures are sequences of poses, they are paths connecting multiple points on the map. In this way, the gesture map combines a line plotâ€™s simplicity with the structural expressiveness of a small-multiples visualization [24].\n3.2.2 Learning a Gesture Map. The two dimensions of the map do not have a direct predefined meaning yet emerge from elicited data. Formally, let the set of all ğ‘ individual gesture poses in the dataset be denoted by G = {ğ‘”ğ‘– | ğ‘– = [1, . . . , ğ‘ ]}, ğ‘”ğ‘– âˆˆ Rğ· where ğ· is the dimensionality of the raw sensor data (in our case D=20). A gesture sequence which consists of ğ‘‡ gesture poses can be viewed as an ordered tuple of size ğ‘‡ i.e., g = (ğ‘”1, . . . , ğ‘”ğ‘‡ ).\n(1) To reduce the dimensions of the raw sensor data, we use an encoder ğ‘“ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ : Rğ· â†’ R2 to embed every gesture pose ğ‘”ğ‘– âˆˆ G into a latent space code ğ‘§ğ‘– âˆˆ R2. These latent codes represent a pose using only two learned features. (2) The raw and high-dimensional gesture sequence g is then embedded as a two-dimensional path z = (ğ‘§1, . . . , ğ‘§ğ‘‡ ) in the latent space.\n(3) To create the grid of gesture poses in the background, we compute an evenly spaced grid M âˆˆ Rğ‘šÃ—ğ‘šÃ—2 of ğ‘š rows and columns over a visible region in the latent space. For example, if the embedded gesture poses (latent codes) range from -4 to 4 in both x and y dimension, we would linearly sample a number of points within this square region. (4) Using the decoder model we can decode arbitrary 2D map points into a full pose, i.e. fğ·ğ‘’ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ : R2 â†’ Rğ·\nIn this paper we use a Variational Autoencoder (VAE). In general, layout and quality of the space (e.g. smoothness), and of pose decoding, depend on the model, and we reflect on this in our discussion."},{"header":"3.3 Map Interaction Concepts","body":"Here we describe how users can interact with the map.\n3.3.1 Pan and Zoom. The map supports pan and zoom and accordingly recomputes the grid of landmarks (small skeletons). This feature helps to adjust the viewport to support exploration of datadense areas, and deal with the fact that landmark representations are discrete indicators for the continuous space.\n3.3.2 Examining Poses. Scatter or density plots can be projected onto the map (e.g. Figure 1b 1â—‹ and Figure 1c). Using â€œdetails on demandâ€, users can hover over points to see the corresponding pose skeleton (Figure1b 2â—‹), and referent, participant and trial number in the detail view (Figure1b 6â—‹). The scatterplot may help researchers to detect outlier body poses, while the density plot reveals regions with recorded data.\n3.3.3 Examining Gestures. For further inspection, one or more gestures can be selected (e.g. Figure 4) from a referentâ€™s list of gesture proposals (Figure 1 3â—‹). This allows researchers to view details on-demand e.g. to reduce the risk of information overload.\n3.3.4 Examining Unseen Poses. A fundamentally new capability of GestureMap is that unseen poses or gestures (i.e. not proposed by participants) can be simulated by decoding arbitrary 2D points in the learned space. In our prototype users can thus hover over the map to visualize 3D skeletons for any cursor location. Analysts can examine if empty regions are anatomically not feasible (cf. 8.3.3) or if people did not show such behaviour. This might be useful to adjust elicitation setup/instructions, for example to prompt people to also cover a previously empty part of the map."},{"header":"3.4 Analysis Concepts Using the Gesture Map","body":"Exploratory analysis seeks to uncover structural patterns in the dataset, identify anomalies, and single-out outliers [53]. We thus conceptualized the gesture map to enable researchers to seamlessly cycle between the detection of new observations and the assessment of supporting evidence. The analysis concept is structured further by differentiating between global observations and local observations. The former targets questions that may span multiple referents or the entire dataset, while the latter focuses on a few gestures to identify specific behavioral idiosyncrasies.\n3.4.1 Global Observations. The first of many analysis steps often involves developing an overview of the data to understand its underlying properties: Researchers here often use statistical plots to summarize the data and to identify broad patterns.\nDeveloping an Overview of the Gesture Space. GestureMap supports this as well: For instance, Figure 3 depicts a scatter plot projected on the gesture map. Scatter points on top of the pose grid enable researchers to quickly identify which general poses were observed in the data. Each scatter point corresponds to a pose from the dataset, whereas empty patches in the gesture map may indicate behavior that has not been observed (e.g. poses/gestures not proposed by participants during elicitation).\nSpotting Clusters and Outliers. Scatter points may visually cluster near gesture poses that are characteristic for a particular referent. These clusters can help researchers to form a mental model of the main poses that are characteristic for a group of gesture sequences. It might also be interesting to analyze outlier behavior which can be detected by examining scatter points that lie far from these clusters.\nComparing Referents and Regions. Additionally, color codes facilitate the comparison of behavior across different referents. For example, it might be interesting to identify which referents share behavior and which are distinctive. Regions in the gesture map that contain multiple embedded data points from different referents may indicate that this region encodes shared generic behavior.\nJudging Densities and Overlap of Referents. Scatter plots may contain too much detail and clutter the visualization. Density plots then offer a visualization of the most frequent gesture poses. Researchers can use it to detect overlapping or distinctive behavior across different referents. For example, these observations can inform researchers interested in building gesture recognizers in judging the difficulty of separating gestures for the various referents.\n3.4.2 Local Observations. The key local observation in elicitation data is to examine individual gesture proposals. GestureMap also supports such analysis, as outlined here:\nAfter the initial data exploration it is often necessary to find concrete example for detected patterns. For example, in the elicitation context, we might be interested in comparing the behavior across different participants and experimental trials.\nThe gesture map can serve as a common visual basis for such investigations: By projecting multiple gestures onto the map, researchers can evaluate each participantâ€™s behavior individually. The trajectory of the embedded gesture paths can inform them on specific behavioral characteristics. For example, a participantâ€™s movement can be subtle, in which case the embedded gesture path is simple in shape and typically spans a small region in the gesture map. In contrast, a complex gesture may be represented as an intricate path that may meander across the map.\nThus, by comparingmultiple embedded gesture paths researchers can visually assess gestures as similar or not. Considering research interests in the elicitation context from the literature, for example, this might support researchers to examine if a participant can remember and repeat the same gesture proposal across multiple trials [40], or if behavior was influenced by a priming effect [6]."},{"header":"4 CONSENSUS AND CLUSTERINGWITH DBA","body":"We introduce the concept of an average gesture sequence as a new computational capability in the context of gesture elicitation. This has three practical values, which complement our tool:\n(1) Descriptive: The average gesture can serve as a single, visual proxy for a group of gestures, which opens up new visualization opportunities (e.g. showing and comparing referents as average paths on our gesture map). (2) Evaluative: It enables a newmeasure of consensus/variability among gesture proposals for a referent. This measure aligns well with other statistical notions of variability: Consensus is assessed via the variance of actual gestures around the average gesture. (3) Explorative: The average gesture enables clustering methods that require averaging (e.g. here: k-means), supporting the automated detection of groups of gestures in a dataset (e.g. in â€œopen elicitationâ€ without referents, cf. [63]).\nWe next describe the technical approach in more detail."},{"header":"4.1 Computing an Average Gesture with DBA","body":"We employ the DTW Barycenter Averaging (DBA) algorithm by Petitjean et al. [43] to compute an average gesture: Intuitively, this algorithm first aligns an initial sequence with every sequence in the set of gesture proposals, before computing a centroid (barycenter) for each aligned coordinate. For further technical details we refer the reader to the related work [43]."},{"header":"4.2 Consensus as Variation Around Barycenter","body":"Vatavu [59] were the first to propose a data-driven consensus measure that does not rely on human judgement of gesture similarity. To achieve this, they employed Dynamic Time Warping (DTW) distance computations to define a consensus measure: They considered two gesture sequences gğ´ and gğµ as similar if the DTW distance was below a threshold Î”ğ·ğ‘‡ğ‘Š (gğ´, gğµ) â‰¤ ğœ . To determine consensus for a referent they calculated the pairwise distances across all gesture proposals for this referent. Finally, to report a measure independent of the threshold value ğœ , they used a logistic regression model to determine the consensus for a range of normalized threshold values and reported the growth rate as an indication of the overall consensus.\nThis work motivates us to further explore data-driven measures of consensus: We follow a similar approach, but instead of regressing on the DTW distance values, and relying on pairwise comparisons, we directly compute an average sequence from all gesture proposals in a referent group, using DBA.\nWe then measure the DTW distance of every gesture proposal ğ‘”âˆ— for a referent ğ‘… to the computed average gesture (i.e. barycenter) gğ·ğµğ´ for ğ‘…. Finally, we report the variance of these DTW distances as a measure of consensus. Formally, this is noted as:\nVARğ‘… =\nâˆ‘Gğ‘… gâˆ— (Î”ğ·ğ‘‡ğ‘Š (g âˆ—, gğ·ğµğ´))2\n|Gğ‘… | (1)\nGğ‘… denotes the set of all gestures elicited for referent ğ‘…. Intuitively, for example, a high value VARğ‘… may inform an analyst that referent ğ‘… contains quite varied gesture proposals (i.e. low consensus).\nThe gesture variance integrates well with GestureMapâ€™s visualization concept because this already displays the involved average gestures as visual elements. Moreover, this approach yields a one-number summary without a logistic regression model on top. Overall, we see this approach as an additional measure, not\na replacement of others: As a flexible tool, GestureMap can be extended to additionally display further such measures (e.g. the one by Vatavu [59]) to support researchers with the analysis."},{"header":"4.3 Clustering Gestures with DBA & K-Means","body":"Being able to compute an average gesture enables the use of clustering methods that require average computations. Here, we use k-means in particular. The idea of clustering gesture elicitation data is motivated by two aspects:\n(1) Exploration: For example, in â€œopen elicitationâ€ [63] or settings where referents are not predefined, such as in the work by Williamson and Murray-Smith [65], a clustering may proved a valuable reference point to identify novel behavior. (2) Annotation: Clustering may also be used to help kickstart (manual) annotation in cases where explicit groupings of proposals are desired (e.g. for agreement measures [67]).\nConsidering the literature, Jang et al. [24] used an interactive hierarchical clustering approach with complete-linkage. In contrast, we experimented with the k-means algorithm, using DBA to calculate the centroids. We motivate this choice by interpretability of the resulting centroids, versus the abstract representations in the hierarchical approach: In particular, the centroids (i.e. average/barycenter gestures) are more compatible with our 2D gesture map, on which they could be displayed as paths. In contrast, a hierarchical treemap does not directly fit the map metaphor well."},{"header":"5 IMPLEMENTATION OF GESTUREMAP","body":"We implemented GestureMap as an analysis tool that integrates the described concepts of both the interactive gesture map (Section 3) and the DBA-based computations (Section 4). Here we describe the key implementation aspects."},{"header":"5.1 User Interface and Functionality","body":"Figure 1 shows the UI; the following sections refer to the numbers in the figure. Overall, we implemented all UI views and interactions conceptually described in Section 3.\n5.1.1 Gesture Map Figure 1b 1â—‹. Researchers can zoom, pan, and hover over the gesture map, and overlay a scatter plot or a density plot (e.g. Figure 1c) to explore individual or multiple gesture poses.\n5.1.2 Experiment View Figure 1b 3â—‹. This view lists all referents and gesture proposals in a compact way as numbers for quick reference and selection. When hovering over an element, the corresponding gesture path is shown on the map for a moment.\n5.1.3 3D Skeleton View Figure 1b 2â—‹, Figure 1b 4â—‹. This view either shows the raw skeleton recording or a reconstructed skeleton. If researchers animate a gesture, it is simultaneously animated in this view and on the map. The progress of the animation can be controlled via a play/pause button and slider.\n5.1.4 Statistics View Figure 1b 5â—‹. This view shows different metrics, namely variances around the average gesture sequence per selected referent (Section 4.2), the distributions of DTW distances of proposals to their average gesture sequence, and nearest neighbor distances for a selected gesture.\n5.1.5 Cluster View Figure 1b 6â—‹. This dialog is unfolded with a button in Figure 1b 3â—‹ and lets users interactively cluster gesture proposals for a referent. Centroids can be animated and once the clusters have been computed, users can toggle all gesture proposals that were assigned to a centroid."},{"header":"5.2 Architecture","body":"We used a server-client architecture. The frontend and backend modules communicate through a REST API through which the data is transmitted as a JSON formatted string. The frontend was implemented with NodeJS [18] and React [15]. For plotting, we use the PlotlyJs library [22]. For the backend we used the Flask framework [10] and Pandas [11] to handle the data transformations and queries. We cached expensive computations such as the computed average sequences and distances matrices on MongoDB [21] to . PyTorch [42] was used to develop the embedding model."},{"header":"6 EXPERIMENTS","body":"Ledo et al. [34] identified four evaluation strategies for toolkit contributions. We follow their perspective to evaluate GestureMap, combining two such strategies: First, here we follow the Demonstration strategy and provide a detailed analysis of examples on elicitation data from related work. Second, Section 7 follows the Usage strategy and reports on a user study with HCI researchers."},{"header":"6.1 Datasets","body":"We consider four existing datasets: One explicit gesture elicitiation study by Vatavu [59], plus three datasets collected for gesture recognition systems [3, 7, 17]. We first focus on the dataset by Vatavu [59] that consists of 1312 full body gestures elicited from children aged 3-6, recorded with a Kinect sensor. For preprocessing, we followed the original authors [59] but left out the resampling step."},{"header":"6.2 Model Training","body":"We used a Variational Autoencoder (VAE) [13] to embed the data as a 2D gesture map. The VAE here serves as an exemplar of a model with both powerful (non-linear) encoding and decoding capabilities. We reflect on other possible choices in our discussion.\nWe trained the VAE on the poses (frames) of the mentioned dataset [59] which has 60 dimensions (20 body joints Ã— ğ‘¥,ğ‘¦, ğ‘§). We adapted the architecture from Spurr et al. [50] (i.e. 4 hidden layers for both encoder and decoder) and used a 2D bottleneck layer. In line with Fu et al. [19], we used a weight term to modulate the mix of KL-loss and reconstruction loss in early training. We trained for 2000 epochs with Adam [29] (lr=3ğ‘’âˆ’5).\nWe experimented with different numbers of hidden neurons â„: Overall, reconstruction loss decreases for larger models, regularized by the KL-loss, leading to diminishing returns and a decision for â„ = 512 here. For full details, we provide the training scripts and model comparisons on the project website."},{"header":"6.3 Global Observations","body":"Here demonstrate the use of GestureMap in a walkthrough of an explorative analysis: Examining the gesture map, the center (Figure 2C) reveals start/end poses (standing upright, arms at rest). We further see, for example, sitting (Figure 2B), clapping (Figure 2D),\nand raising an arm (Figure 2A). Thus, the map reveals the space of poses elicited by Vatavu [59] at a glance: For example, their referents included crouch, draw a flower, draw a circle, draw a square, applaud or raise your hands, which all match the poses in our map.\nUsing overlays in GestureMap, we can identify similarity and differences between gestures across referents: For example, Figure 3 (left) shows that crouch, draw circle, draw flower, draw square share common behavior; their scatter points largely overlap in the region that encodes â€œraised armâ€ behavior. In contrast, for instance, gestures proposed for crouch cover a different region (pink).\nThe variance plot in GestureMap (Figure 3 right) indicates that proposals for crouch and draw flower vary more than for draw circle and draw square. Potentially, for the children the basic shapes afforded less flexible interpretation than a flower or crouching.\nWe defined a consensus measure on this variability (Section 4.2): Comparing this variability between all referents, our results largely agree with Vatavu [59]: In particular, applaud, fly like a bird and hands up show high consensus while climb ladder, crouch, turn around have low consensus."},{"header":"6.4 Local Observations","body":"Proposals for crouch form two main clusters (pink points in Figure 3 left), one in the region of starting poses, another in sitting/crouching regions. Thus, GestureMap visually reveals that people interpreted crouch in different ways, matching the high variance (Figure 3\nright). Examining the map locally, in combination with gesture animations, reveals that some children sat on the floor, some on their heels, some crawled on hands/knees, and others stood with a stooped body posture. Some additionally jumped at the end of their gesture proposals to get back onto their feet.\nAs another such example, for throw ball, behavior can be categorized into four clusters: Most children used their right hand, others used two hands, and some kicked the ball. Only a few used the left hand. As Figure 4 shows, the children mostly stuck to their interpretation across multiple repeated trials for that referent, revealing consistency (cf. [5]). This is an example for using GestureMapâ€™s spatial visualisation of gestures as paths for visual comparison via shape."},{"header":"6.5 Interactive Clustering","body":"For a typical elicitation study, such as this one by Vatavu [59], it is reasonable to expect clusters induced by the referents. Therefore, to demonstrate our proposed clustering analysis we removed the referent labels and then evaluated if k-means finds clusters that match the original referents.\nConcretely, we ran the clustering with 15 sequences chosen randomly. We then inspected the mix of original referents present in the gestures assigned to each found cluster. We repeated this ten times and made these observations:\nâ€¢ Our k-means clustering identified those referents with high agreement (e.g. hands up, crouch, applaud, fly like a bird). â€¢ Gestures for referents with much common behavior appeared as one cluster (e.g. draw circle, square, flower). Note that this is not necessarily â€œwrongâ€, since a behaviour â€œdraw somethingâ€ would also have been a plausible referent. â€¢ The resting pose was detected as a separate cluster. â€¢ Other referents were (clearly) present only in some of the clustering repetitions.\nOverall, this indicates the potential of automated clustering, for example, when examining data from open elicitation with no given referents. We return to ideas for improvements in our discussion.\nIn another experiment, we applied clustering to look for patterns within a referent: As mentioned, referents such as throw ball and crouch contained distinct patterns, revealed on the map. Indeed, running k-means revealed some of them: For example, for throw ball k-means also detected throwing with the right hand vs using both hands. In contrast, it did not separately find left hand and kicking, presumably since those were proposed only a few times."},{"header":"6.6 Comparison Between Datasets","body":"Other researchers noted that elicitation findings are spread across multiple venues and need to be consolidated [63]. GestureMap supports this as it offers a platform to visualize and analyze multiple studies. We demonstrate this by creating a gesture map using four datasets [3, 8, 17, 59].\nTo motivate a concrete example, citetJain2016 showed that observers can distinguish behavior of children and adults. Figure 5 shows all 20 proposals for jump from the data by Aloba et al. [3], next to the childrenâ€™s proposals from Vatavu [59]. The gesture paths visit roughly similar main parts of the gesture space, yet the children do not find consensus. Our variance measure also reflects this (Aloba - adults 23.70, children 44.89; Vatavu - children 54.94).\nAs a second example, we compared behavior diversity across datasets. Without knowing anything about the referents, Figure 6 already reveals that one dataset [3] (blue) covers a larger region than the other [59] (orange). Thus, it seems to contain a more diverse set of body poses. Indeed, this observation can be explained by the longer referent list (58 referents in [3] vs 15 in [59])."},{"header":"7 USER STUDY","body":"To further evaluate GestureMap, we recruited eight HCI researchers (7 male, 1 female) from three universities via e-mail for remote think-alound and interview sessions. Six were familiar with gesture elicitation studies, the other two were interested in analysing gesture sensor data. Five were familiar with machine learning."},{"header":"7.1 Procedure","body":"The interviews lasted 80 minutes and were conducted via screensharing using Skype/Zoom, with GestureMap hosted online such that people could use it on their own computer. We again used the dataset by Vatavu [59]. With peopleâ€™s consent we recorded the interviews. We encouraged them to think out loud and occasionally asked questions to better understand actions. We took notes and compiled a report from this material. Given the exploratory nature of the interactions and the diversity in peopleâ€™s approaches this was done in an inductive approach, leading to the themes in Section 7.2.\nThe interviews had four parts: 1) We introduced GestureMap (20 minutes), with a concept presentation, a guided walk through the tool and UI, and opportunities for questions. 2) In an exploratory, manual analysis task people were prompted to use GestureMap to identify groups of behaviors in the gesture proposals for two referents. In real use, researchers would conduct such analyses to better understand elicited data. 3) In a more confirmatory, automatic analysis task we asked them to build on their gained insights to initialize the clustering algorithm and refine the automatic clustering results. In real use, researchers might export this result, for example, for a report, calculations of agreement, etc. 4) The session concluded with a semi-structured interview of at least ten minutes. Here, we\ninquired into what people liked/disliked about GestureMap, and asked for ideas for improvements and additional features."},{"header":"7.2 Findings","body":"7.2.1 Initial Use. Upon first use, most people immediately animated a few gestures, saying that this was the most natural and familiar way to view the data Since the map visualization was unfamiliar to them, some had initial difficulties to understand the distinction of single poses (points) and entire gestures (paths). These people found the animation particularly important: Seeing the 3D skeleton and the 2D path animated in sync highlighted that a gesture was a path on the map and thus helped them to get familiar with the map concept. Summarising their initial experience, one person said: â€œAlthough, the learning curve [...] is steep, once you understand the core concepts, this tool offers a great overview of the entire behavior that is captured in the dataset.â€\n7.2.2 Statistical Plot Overlays. We asked the researchers to analyze the proposals for crouch and throw ball. Throughout the interview we noticed that all participants preferred the scatter plot over the density plot. When asked why they keep returning to the scatter plot, they said that it provided more detail and that density can also be estimated from scatter points. They also said that points were visually closer to the data (point=pose).\n7.2.3 Details of the Gesture Map View. When study participants paused their exploration for a longer period, we inquired why that was the case. Some people noted that they struggled to find a specific pose on the map. They suggested to increase the visibility of the poses by showing fewer and larger landmarks. Another researcher felt that the map should show more detail so it would be easier to judge differences and transitions of poses. Together, this feedback motivates a changeable grid size (our zoom was implemented to always keep an 11 Ã— 11 grid).\nSome found similar poses encoded in different map regions and noted that these should ideally reside in one area. This is an artefact of dimensionality reduction, as we discuss further in Section 8.2\n7.2.4 Exploration Strategies. When we asked the participants what the main aspect was that they used to determine interesting behavioral patterns, we observed diverse analysis strategies, but we broadly highlight two main ones:\n1) Shape driven analysis: Some started by skimming through gestures to get an overview of their different path shapes on the map. They stopped to examine gestures in more detail that differed largely from the shapes seen so far. In a sense, they searched for outlier behavior based on the path shapes. These participants noted that the 2D gesture path visualization offers a quick way to spot irregular behavior and that their analysis becomes an active search versus passively watching every gesture individually.\n2) Position driven analysis: In contrast, other participants focused entirely on the scatter points as template poses. Using expectations about possible behavior for a gesture proposal (e.g. left vs right hand throwing), they examined scatter points in those map regions that based on the landmark skeletons encoded related poses.\n7.2.5 Manually Forming Clusters. Regardless of their initial analysis strategy, when asked which feature they would use to group the\ngestures, people agreed on the path shapes as primary discerning feature (strategy 1). For the crouch referent, everyone distinguished two to three groups of behaviors. For throw ball, everyone found at least three (left/right/both handed throwing). Some also found the kicking behavior as described in Section 6. Overall, the researchers felt comfortable with grouping the proposals based on the path shapes. However, there were some complex paths (e.g. crossing over many poses on the map) that people were unable to assign to a group. One person suggested to create an outlier group for these.\n7.2.6 Interactive Clustering. We asked people to use the interactive clustering tool based on their observations in the first task.\nNext, they were asked to initialize the clustering algorithm using their knowledge from the previous task. Now, all participants specifically searched for individual gesture proposals as templates (strategy 2) and used those to initialize the algorithm.\nHowever, the resulting computed centroids often deviated from peopleâ€™s expectations, and thus did not immediately make sense to them. One user noted that one still has to inspect all gesture proposals in order to choose suitable initialisations for the k-means algorithm. On the positive side, the researchers liked the refinement step, where they could reassign proposals to another cluster. These reassignments, however, were not yet considered when rerunning the clustering algorithm in the current implementation.\nOverall, after being asked to give a final verdict over the interactive clustering feature, all deemed it important. However, they noted that it should be more accurate and manually refined assignments need to be respected when rerunning the clustering algorithm, thus enabling iterative, interactive use. Technically, this can be readily implemented by initialising k-means with the current (refined) assignments."},{"header":"8 DISCUSSION","body":""},{"header":"8.1 Extending the Gesture Elicitation Toolbox","body":"GestureMap builds on and extends functionalities of previous tools for gesture elicitation: It combines 1) gesture modeling and visualization, 2) automatic computation of elicitation metrics, and 3) interactive clustering to provide an integrated analysis platform.\nSeeing this and related work as a â€œtoolboxâ€, researchers may now consider various options: For example, AGATE 2.0 [61] is a highly specialized tool to compute agreement, which assumes a given labeled dataset. GestureMap could be used to label data and export it for analysis in tools like this.\nAlternatively, Ali et al. [1] proposed a crowd platform for annotation, yet without computational support for the workers, such as alternative gesture representations or similarity measures. Such support as shown in GestureMap could be combined with a crowd approach in the future. GestureMap is already implemented as a web-based tool, rendering it flexible and open to such integration.\nLooking ahead, new cloud elicitation tools [2, 36] yield large datasets. GestureMapâ€™s concepts support handling large data, visually summarised and explored via our map view.\nFinally, the â€œtoolboxâ€ in the literature includes several formalized agreement measures [56, 67]. These could be used also with our interactive clustering, for example, by plugging in the cluster cardinalities instead of subjective gesture group counts."},{"header":"8.2 Reflection on Model & Clustering Choices","body":"Here, we highlight model and clustering aspects to consider.\n8.2.1 Smoothness of the Latent Space. A smooth latent space facilitates suitable visualization by reducing â€œjumpsâ€ in gesture paths. These occur due to recording issues (e.g. sensor occlusion in some frames) or when subsequent poses are embedded far apart in the 2D space. While some models address this (e.g. we used a VAE instead of AE), there is no universal â€œnaturalâ€ 2D layout of body poses and some artifacts are likely to exist for most models and datasets. Besides technical model improvements, visualization concepts could be explored to address this as well (e.g. visually mark â€œjumpsâ€ along the gesture path).\n8.2.2 Cluster Approaches. A difficulty with k-means is setting the number of clusters. As an example strategy, to detect the subgroup behavior for the throw ball referent, we quickly skimmed through the gestures using the map and visually identified rough patterns. We then chose ğ‘˜ correspondingly. We chose k-means, because it readily integrates with the gesture map and the \"variance around mean gesture\" that we introduced in section 4.2. Color coding the cluster results can be done quickly. Jang et al. [24] proposed to use interactive hierarchical clustering. Integrating such a tree-like layout into the gesture map adds complexity and might be material for future endeavours. We can imagine that average gestures calculated with the DBA-algorithm can be used to visualize the non-leaf nodes in the hierarchical tree. In addition, interactive hierarchical clustering would eliminate the need for choosing the number of clusters beforehand.\n8.2.3 Feature Representation. Hand-engineered features [4, 24, 58] may help with the interpretation, however, they may be specific to a sensor and interaction setup. As an exploratory tool, GestureMapâ€™s learned space is applicable to new and changing setups, without developing hand-engineered features first. Furthermore, our learned representation supports gesture simulation useful to examine regions of the behavior space that were not covered by participants."},{"header":"8.3 Opportunities for Research & Applications","body":"Here we outline further ideas enabled or supported by GestureMap.\n8.3.1 Supporting Meta-Analysis and Consolidation. GestureMap empowers researchers to compare data across studies (cf. Section 6). As a community, we could consolidate our findings in a meta-map of many studies, as a sensor data-driven complement to literature surveys [63]. For instance, such a map might reveal which gestures and poses are most common or intensely studied. Separate maps could also compare gesture spaces for different contexts, devices, etc., for example, to better understand the influences of such factors.\n8.3.2 Enabling Map-based Gesture Authoring. GestureMap could be extended to define new gestures: For example, users could draw a gesture as a path on the map. Since the underlying latent variable model can simulate new behavior (decoding), such a drawn path implicitly defines a pose sequence that could be exported as a template-based gesture recogniser. As an alternative to drawing, users could demonstrate the gesture in front of the sensor, with a â€œcursorâ€ moving on the map live. Users could also select recorded\ngestures on the map, labelled manually or with help from our clustering tool, to train a classifier. Such a recognizer then also could be used in other tools to support sensor feed annotation (e.g. [41]).\n8.3.3 Enabling Analysis of Unseen Behavior. So far, elicitation has focused on observed gestures, yet it might also be relevant to examine why behavior was not observed. GestureMap enables this: Researchers can explore map areas without data, which may reveal unlikely behavior, or indicate issues with interaction (e.g. anatomically difficult or tiring gestures) or the sensor (e.g. gestures leading to self-occlusion of body parts). In this way, GestureMap supports the diagnosis of challenges and limitations in the joint user-sensor space of an interactive system (cf. [65]).\n8.3.4 Supporting Live Exploration and Monitoring. GestureMap could be extended to more than post-hoc analysis: For example, we could embed live sensor data and continuously update the underlying mode. This live embedding provides a monitoring tool, for example, for participants to see their currently performed gesture (e.g. shown as a â€œcursorâ€/point on the map), possibly to nudge them towards exploring new regions of the behavior space (cf. [65]). One could also predefine a gesture path to monitor live performances and to judge deviation from this â€œtemplateâ€, possibly to learn/teach a movement sequence. Related, gesture sets are mostly presented as drawings and videos today [37]. Instead, GestureMap could be used to show gestures to users, allowing them to reenact and explore them with live monitoring via the map."},{"header":"9 CONCLUSION","body":"As our key contribution, we presented a set of visualization and analysis concepts for gesture elicitation data and a tool that implements them: GestureMap is the first visual analytics tool for gesture elicitation which directly visualises the space of gestures, using a learned 2D embedding. It further leverages the computation of average gestures to enable researchers to 1) represent gesture groups with one gesture; 2) assess consensus as variance around this average gesture; and 3) cluster gestures automatically.\nExpert users especially liked the visual expressiveness of GestureMap, as it quickly summarizes the underlying dataset. The extensibility of GestureMap further encourages future work to employ machine learning as a tool for analysis of human behavior. With this work, we contribute to the vision of more widespread use of applicable computational methods in HCI, also to support more extensive and cost-efficient large-scale, data-driven HCI work. Given the proliferation of crowd platforms to collect large datasets, we expect computational methods and visual analytics as proposed here to become indispensable tools for many future HCI studies.\nGestureMap and further materials are available on the project website: https://osf.io/dzn5g/"},{"header":"ACKNOWLEDGMENTS","body":"This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt)."}],"type":"Sections"}