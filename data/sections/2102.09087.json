{"sections":[{"body":"CCS CONCEPTS • Human-centered computing→ Gestural input.\nKEYWORDS Back-of-device, gesture recognition, IMU\nACM Reference Format: Michael Xuelin Huang, Yang Li, Nazneen Nazneen, Alexander Chao, and Shumin Zhai. 2021. TapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3411764.3445626\n1TapNet Dataset\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\nCHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8096-6/21/05. https://doi.org/10.1145/3411764.3445626"},{"header":"1 INTRODUCTION","body":"Touchscreen as the sole method of mobile device input is increasingly challenged by its inherent limitations. The difficulty of onehanded use and the visual occlusion by the operating finger are two of them. Beyond research explorations, we began to see off-screen occlusion-free alternatives in mainstream products, such as double pressing power button to activate the camera, squeezing the Active Edge [23] of Google Pixel phones and long-press power button [30] of the iPhones for voice command invocation.\nIn addition to a tap event (presence of a tap) alone, potentially useful tap properties include tap direction [20, 34] (i.e. whether the tap is on the front, back, or side of the phone), tap location [17, 21] (i.e. which region the tap falls on the devices), and tap finger part [9] (i.e. tapping with finger pad vs fingernail). Prior studies has focused on the recognition of a single tap property, based on a limited amount of data as a proof-of-concept. This paper aims to address the needs in practice, by predicting comprehensive tap properties for diverse application purposes and advancing the state of the art of off-screen interaction towards a practical level of performance. Note that the state of the art for this topic can only be advanced i i li i i i i l . i .\nCHI ’21, May 8–13, 2021, Yokohama, Japan Huang, et al.\nif there are well-established benchmarks with accessible datasets and codebases. Our work is set to help create a benchmark with extensive dataset development, neural network model design, model training, and experimentation.\nAfter experimenting with multiple methods and architectures for tap detection, we found that a multi-input, multi-output (MIMO) convolutional neural network (CNN) gave the best results. The resulting method, TapNet, enables joint learning on cross-device data and joint prediction of multiple tap properties. Compared with the solution of one model for each task, a joint model (see Figure 1) can exploit the interrelation among multiple tap properties during training and also saves run-time computation and memory. TapNet contains shared convolutional layers for task-agnostic knowledge extraction, meanwhile each of its output branch retains the taskspecific information. TapNet uses the inertial measurement unit (IMU) signals as primary input and the phone form factor as “auxiliary information” [28] for cross-device training, i.e. joint training on cross-device data. Together, TapNet offers a one-model-for-all solution [13] across devices.\nOur technical investigation and evaluation shed lights on two important aspects for machine learning (ML) in HCI development: methods and data. First, thanks to the MIMO architecture, TapNet increased tap property recognition accuracy over the prior art methods, particularly for the more difficult tasks such as tap location recognition. Specifically, TapNet yields a mean distance error around 10% of the screen diagonal, similar to the between-icon distance on mobile phone home page. Although this resolution is much less precise compared to the touchscreen, such resolution can already enlarge the interaction design space, by for example enabling users to perform selection by tapping on the back of the phone, even when they wear gloves.\nSecond, we discuss an efficient data strategy for developing a ML based HCI application. Different from most computer vision tasks that heavily rely on multi-person data to achieve user generalizability [12, 44], a well-performing ML model for HCI (interactive) tasks may not necessarily demand multi-person training data. The key is to ensure the training data diversity. To test this hypothesis, we collected a one-person dataset for training and a separate multiperson dataset for testing (and optionally model adaptation). The results show that the one-person model could achieve a comparable level of user generalizability as a model tuned on multi-person data.\nOur contribution is four-fold. We 1) developed a multi-task network that can be jointly trained across devices. The network could simultaneously recognize a set of tap properties, including tap direction and location; 2) developed two datasets and conducted extensive evaluations that advanced the state of the art; 3) offered new perspectives on alleviating the data hurdle in ML based HCI research; 4) established a benchmark with opensource code and datasets for off-screen tapping recognition, which will be reproducible and accessible by others."},{"header":"2 RELATEDWORK","body":"This study is related to off-screen interaction, in particular, tapbased interaction, as well as multi-task neural networks [3]."},{"header":"2.1 Off-Screen Interaction","body":"Back-of-device (BoD) [5, 6, 14, 16] and edge-of-device interactions [10, 16, 37, 41] have attracted much attention, however, most of them relied on specialized sensors that are not readily available on phones. For instance, BackXPress studied finger pressure for BoD interaction using a sandwiched smartphone [5]. InfiniTouch recognized finger location from capacitive image outside the touchscreen [16]. Some exploited small widgets to enable offscreen gesture sensing. Wearing a magnet can allow magnetometer to track the 3D finger movement [4, 24]. Adding a mirror can make camera to detect finger location on the back [33] or around the device [38]. Acoustic sensing exploits sound propagation properties to recognize BoD gesture [29], grip force [32], and contact finger part [9], and electric field sensing detects around-device gestures in a non-intrusive manner [45, 47]. Unlike using the IMU, these detection methods require additional hardware installed on the already very compact mobile devices, increasing manufacturing and material cost and potentially reducing available space to the largest possible battery."},{"header":"2.2 Tap Detection from IMU Signals","body":"Despite the line of research on detecting tapping from motion signals captured by the built-in IMU sensors on smartphones [8, 26, 39, 40], there is room for improvement. SecTap allowed users to move a cursor by tilting and select by back tapping [19]. Bezel-Tap detected tap on the edge of the devices [27]. Granell and Leiva conducted feature engineering for tap detection [8]. In contrast to the detection of tap event alone, BackPat recognized tapping in three locations based on gyroscope and microphone signals [26]. BackTap [40] and BeyondTouch [39] classified four-corner tap based on accelerometer, gyroscope, and microphone signals. These studies relied on simple features (e.g. mean, kurtosis, and skewness) and statistical methods typically based on shallow neural networks [8, 19, 26, 39, 40], thus only achieved limited precision of tap location detection.\nMore recently, researchers started to explore neural networks for tap location classification for PIN code inference in the field of privacy and security [17, 21]. PINlogger classified tapping on ten buttons using a single layer neutral network [21]. Liang et al. applied a two-layer CNN model to estimate tap location from zaxis signal of accelerometer [17]. Although these studies yielded promising result, their network capacity was relatively small and the reported accuracy was still far from practical level."},{"header":"2.3 Multi-Task Learning and Training with","body":"Auxiliary Information\nTo improve tap recognition, we develop a multi-input and multioutput a neural network that can estimate multiple tap properties. This section reviews two related concepts: multi-task learning [3] and learning with auxiliary information. Multi-task learning with neural network leads to a multi-branch architecture. Each branch addresses one of the recognition tasks, such as different targets in tracking [22], languages in translation [13], and head poses in gaze estimation [43]. The shared layers of all branches extract the common knowledge across tasks, whose generalizability is ensured\nTapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input CHI ’21, May 8–13, 2021, Yokohama, Japan\nby the regularization effect across tasks. Multi-task models employ multiple loss functions thus more supervisions during training that can potentially helps learning. To our knowledge, no attempt has been made to apply multi-task learning for off-screen tap recognition and the multi-task architecture can conduce to the learning of each tap recognition task from their interrelationship.\nWe are also the first to exploit the form factor of the phone as auxiliary information, which has been shown to be beneficial for training. Zhang et al. investigated the benefit of using auxiliary information in training and found that it can improve performance in testing even without using the auxiliary information as input [42]. Stephenson et al. pointed out that conditioning on auxiliary information can achieve higher robustness than that of appending auxiliary information directly to the main features [28]. Liao et al. showed that integrating simple but essential auxiliary information can increase prediction accuracy [18]."},{"header":"3 RECOGNIZING TAP PROPERTIES","body":"Moving beyond the prior work in this space, this project aims at developing IMU-based input methods that meet the requirement of practical applications, by means of deep neural network design and training. The key objective is to achieve the five recognition tasks (i.e. the five network outputs) with one network (TapNet) as shown in Table 1. Each task aims to recognize one tap property, such as direction and location. We first describe the pipeline overview, followed by the core of the method (a MIMO network). We then discuss the gating component and signal filtering for recognition."},{"header":"3.1 Pipeline Overview","body":"As shown in Figure 2, the system listens to the six-channel IMU (three-axis accelerometer and three-axis gyroscope) data and maintains a 150-ms data window. To avoid unnecessary down-stream computation for recognizing tap properties, a heuristic-based gating mechanism using the z-axis signal of the accelerometer is performed to reject obvious non-tap motions. If the signal passes the gating, the six-channel IMU signals within a 120-ms feature window will be concatenated into a one-dimensional feature vector. The feature window is identified and aligned across tap samples by the tap-induced peak in the z-axis signal. TapNet, a multi-input, multioutput convolutional neural network, takes this feature vector and a device vector as input. The device vector depicts the phone size, IMU pointing direction, and installation location relative to the\nupper left corner of the device housing. TapNet then outputs the predictions of five tap properties at a time as shown in Table 1."},{"header":"3.2 Design of TapNet Architecture","body":"TapNet uses a multi-input and multi-output architecture. We use IMU signals as primary input and device vector that describes the phone form factor as auxiliary input. Using this auxiliary information helps to accommodate the difference of device form factor and achieves the cross-device training.\nThe output of TapNet contains multiple branches. Since different tap properties have a confound impact on IMU responses, we jointly learn the mappings from IMU signals onto these interrelated tap properties using a multi-layer CNN. In this architecture, different tap-related tasks share four convolutional layers, which extract the common shape patterns that are indicative across tap properties. Following the shared layers, there are branches of fully connected layers, each of which extracts property-specific patterns for individual task. In practice, shared layers for tap properties also means shared computation. Therefore, TapNet requires less computation and memory than multiple networks each trained for a single task.\nTraining over an intersection of multiple tap tasks confines the learning in a restrained feature embedding space and thus allows it to converge to a solution for all related tasks [13]. Multi-task learning allows for good alignment between feature embeddings of different tasks. Such additional guidance or supervision can potentially prevent over-fitting especially given few training samples."},{"header":"3.3 One-Channel Convolutional Layers","body":"As illustrated in Figure 2, a one-channel CNN is used for tap recognition. We exploit convolutional layers to extract shape features from IMU signals, because the convolutional filter offers temporal locality to capture signal dynamics and shared weights to reduce trainable parameters. In general, a convolutional filter in early layers describes the basic shape features, such as a peak, a valley, or a certain degree of slope. A convolutional filter in later layers, with a large receptive field, is more likely to contain shape semantics, such as a large magnitude peak or an impulse with double peaks.\nRegarding the options between one-channel vs multi-channel network, we see that the one-channel network can be more efficient as it allows for filter reuse across IMU channels. Conventional methods applied multi-channel CNN to describe signal alignment across channels, and it has been widely use to handle EEG [25] and IMU [36] data. However, a large number of filters are required to depict the fine-grained signal alignment combinations of the six-channel signals, and thus increases the model training difficulty and the demand for data. In contrast, we propose to concatenate the six-channel data into a one-dimensional vector and apply onechannel convolutional layers in TapNet. By doing so, each onechannel convolutional filter can be reused to detect shape features across channels, and then rely on the fully connected layers to draw decision by associating filter activations in different parts of the signals. Dividing the shape extraction and alignment analysis conceivably mitigates the model training difficulty and achieves an efficient use of data. We also evaluated this in an experiment.\nCHI ’21, May 8–13, 2021, Yokohama, Japan Huang, et al."},{"header":"3.4 Signal Gating for Neural Network","body":"Although TapNet is rather lightweight compared with vision-based convolutional networks, performing recognition per frame generates unnecessary overheads. Our observation, as well as previous findings [17], suggests that a peak or valley in the z-axis output is a necessary (high recall) but not sufficient (low precision) signal to a tapping event on the device. As such, we use the z-axis signal of accelerometer as a gating signal to the CNN in order to minimize the amount of computation (hence power consumption) on the mobile device. Gating signal alone may not be enough for accurate recognition of tap properties, it is sufficient for tap-like (high recall low precision) event detection, and thus qualified as gating for the CNN recognition. Therefore, we only perform CNN recognition when we observe a tap-like signal from the gating signal.\nSpecifically, we first perform a simple, linear complexity peak detection on the gating signal at the per-frame level (Figure 2 the pink region). This peak detection yields a set of peaks and valleys and their corresponding timestamps. We define a tap-like signal as the one has an impulse that contains at least one peak, and an impulse as a group of peaks between each pair the interval is less than the threshold, Tv , which is empirically set to 80 ms. We also apply a magnitude threshold on the peak and valley to control the sensitivity of the gating component."},{"header":"3.5 Sensor Signal and Temporal Alignment","body":"Sensor system on smartphones provides motion data in a number of formats, including the raw signal, its gravity excluded counterpart, and rotation vector. As we aim to detect tap event and identify tap properties in an orientation-invariant manner, we leverage the raw accelerometer and gyroscope data and compute their first-order derivative. Therefore, these signals represent the change of force on the housing of the phone and the resulting change of rotation velocity, and they stay at zero when the phone is stationary. In addition to orientation invariance, we also see that the first-order derivatives accelerometer and gyroscope signals have high shape\nsimilarity. As such, using this representation conduces to the reuse of convolutional filters and alleviates network training difficulty.\nAll tap feature vectors are temporally aligned. Specifically, The first major peak in the accelerometer z-axis is aligned to a designated frame (=106th) in the feature vector, and this determines the feature window location. Then 50 sensor samples (∼120ms) from each IMU channel are concatenated to form a 300-element feature vector for each tap-like event. Such temporal alignment allows the network to focus on the shape differences induced by a tap."},{"header":"4 DATASET DEVELOPMENT","body":"This section describes two datasets: the one-person dataset (135,260 samples) was used for TapNet training only (not testing) and the multi-person dataset (38,545 samples) was for performance testing and user generalizability tuning. Our hypothesis is that a welldesigned data collection protocol can ensure training data diversity even from one person and be sufficient for some tap recognition tasks. When we need to improve the model, it is relatively easy to enlarge the one-person training set, evaluate on the test set, and repeat this process in an iterative fashion. This is more efficient than collecting additional multi-person data."},{"header":"4.1 One-Person Training Dataset","body":"We used a single researcher training strategy. One of us produced the entire training data following a comprehensive data collection protocol, which aims to cover the diversity in real use. Tap data were collected through multiple sessions on Phone A and Phone B2. Each session aims to collect tap in a specific condition described by a set of characteristics of the phone, grip gestures, and the tapping itself (see Table 2). Visual indicator was presented on the screen to guide the experimenter. The experimenter can examine the finger alignment with the target location by turning the phone back and forth to ensure the annotation correctness. Please refer to the supplemental file for a detailed description.\n2Phone A = Pixel 3 and Phone B = Pixel 3 XL\nTapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input CHI ’21, May 8–13, 2021, Yokohama, Japan\nThe advantages of this strategy are three-fold: 1) it is low-cost and convenient for rapid development iteration, 2) it produces systematic and diverse data, and 3) the data quality is manageable and assessable. This strategy allows to use the researcher’s intuition of the problem space to push the recognition envelope to the fullest degree, for example, by including the training data with different grip gestures and forces. The risks of this strategy include the potential lack of diverse tapping patterns that a single person can not fully represent as well as the impact of the biomechanics. We mitigate the risks by evaluating on a separate multi-person dataset.\nWe repeated data collection under the same condition to mitigate bias. In the end, the one-person data collection took over 30 sessions (around half an hour for each) over 69 days. In total, the one-person training dataset contains 109,200 tapping actions, including 45,500 front taps, 48,450 back taps, 4,020 left taps, 4,020 right taps, 3,610 top taps, 3,600 bottom taps. Among these tapping actions, 94,764 were collected from a Phone A and 14,440 from a Phone B, and 85.7% of them are tapping with finger pad and the rest are with finger nail. We also collected non-tap motions (26,060 in total) in a number of scenarios: grasping (4,700) the phone, rubbing the phone case (6,000), releasing a tap (6,160), knocking on the phone placing surface (4,400), shaking and moving with the phone in pocket and in bag (4,800). The usage of this dataset (135,260 samples including tap and non-tap) was solely in training (or “teaching”). None was used in any testing to ensure the validity and generalizability of the efficacy measures reported later."},{"header":"4.2 Multi-Person Testing Dataset","body":"To evaluate TapNet performance, we collected amulti-person (n=31) dataset. We focused on collecting the natural tapping actions to simulate tapping data distribution in real use, in particular, with the common finger (thumb and index), in the finger comfort range, and for four most common one-handed and two-handed gestures [11, 15] as shown in Figure 3.\n4.2.1 Data collection design. The four grip gestures we studied are in a combination conditions of phone, grip gestures, and tapping actions as shown in Table 3. To keep data collection natural, we avoid continuous and extensive tapping. For every condition, participants were asked to tap for five to ten times and take rest at various intervals to avoid fatigue. In addition, data collection was informed by research conducted on the most common hand grips and finger placements [11, 15]. Unrealistic reach for targets on the back surface of the phone were excluded.\n4.2.2 Apparatus. We used a 5.5\" Phone A and a 6.3\" Phone B throughout the collection. To provide the most natural feeling, we assigned phone size in the collection based on participants’ personal phone size. To achieve a balance ratio, we selected participants according to their personal phone size in recruitment.\n4.2.3 Participants. We recruited 31 participants (13 female, age: 18-54). We marked down a number of factors that may affect tapping signal responses, including finger length, finger nail length,\nCHI ’21, May 8–13, 2021, Yokohama, Japan Huang, et al.\nhand size, and handedness. During data collection, the experiment interface would adapt to the handedness of individual participant to ensure the comfort range was valid.\nOverall, the multi-person dataset contains 38,545 taps, including 20,615 taps from front, 10,505 back, 1,705 left, 1,705 right, 1,975 top, and 2,040 bottom, among which 58.3% was collected on phones with rubber cases, and the rest without."},{"header":"5 MACHINE LEARNING EXPERIMENTS","body":"This section evaluates the overall performance of TapNet, followed by the comparison between one- and multi- channel CNN as well as ablation studies on the MIMO network design.\nWe measured classification performance by F1 score and regression by mean absolute error (MAE) and r2 score. F1 score is an weighted average metric of precision and recall, ranging from [0, 1]. The MAE of tap location, i.e. √ [(дx − tx )/w]2 + [(дy − ty )/h]2, is computed by the Euclidean distance between ground truth location (дx ,дy ) and TapNet output (tx , ty ) normalized by screen widthw and height h. Its range is [0, 1.414], and a baseline (always predicting the center) is 0.707. Similar to standard deviation, r2 measures how well the regression line fits the data with a range of [0, 1].\nIn the implementation, training to recognize the presence of tap event requires tap and non-tap data (e.g. phone shaking and rubbing motions). As non-tap data does not have annotations about tap finger part, direction, and location, it cannot be used to train the rest of the network. As such, the two parts of the network (tap event v.s. the rest of the tap properties) were trained in turn. The tap event branch was trained once after every ten epochs of training the rest of the tap property branches.\nWe applied ReLU and batch normalization after each convolutional layer and used Adam optimizer with a learning rate of 1e-4 and a momentum decay (1e-6). Each model was trained with sufficient number of epochs until the validation (5% data) loss converges, and training was done using Tensorflow [1] on a 4G memory GPU.\nThe datasets were collected on Phone A and Phone B that ran on Android 9, and the IMU sampling rate was approximately 416Hz. In run time, it took 0.56 ms on the Phone B main CPU to finish one TapNet inference. A simplified TapNet can also be run in real time (9ms) on the embedding DSP using Tensorflow Lite for microcontroller [31], but this is beyond the scope of this paper. The major latency (105 ms) of the whole pipeline lies in waiting to observe the complete tap signal in the feature window.\nDue to space limitation, we have put some of the implementation evaluations into the supplemental file. Interesting findings include: 1) high sensor sampling rate is crucial to tap recognition accuracy; 2) the use of gyroscope in our task configuration is important; and 3) data augmentation by temporally shifting the samples conduces to the performance gain but scaling the samples does not."},{"header":"5.1 Performance on Tap Recognition Tasks","body":"This section evaluates TapNet’s improvement over prior art and user generalizability when training on one-person data.\n5.1.1 Improving the state of the art. We implemented and trained four ML algorithms, including TapNet, to compare their relative performance. The overall performance measured by the weighted\naverage F1 score and MAE across participants and devices are shown in Table 4. Support Vector Machine (SVM) represents a line of studies [19, 39] that used traditional machine learning methods. TinyCNN is a replication of Liang et al.’s two-layer CNN [17]. SISO refers to a truncated single-input and single-output (SISO) TapNet. It uses the same training configuration (Adam optimizer and learning rate) as the MIMO TapNet, and it gives performance reference if TapNet is configured for a single task. All the methods were trained on the one-person dataset and tested on the multi-person dataset.\nTable 4: Weighted average F1 score of four classification tasks (no color shading) as well as MAE and r2 score of the tap location regression task (purple shading). SVM and TinyCNN are our re-implementation of relatedworks [17, 19, 39]. SISO TapNet is the truncated single-input and single-output variant. TapNet is the proposedMIMOmethod. Note that we have built one model per task for the single-output models. In contrast, TapNet is a multi-task network that gives predictions for all five tasks at a time. Finger part, direction, and location are not conditional on the event detection. The numbers are the higher the better for F1 and r2 scores, while lower the better for MAE.\nCHI ’21, May 8–13, 021, Y kohama, J pan Huang, et al.\nhand size, and han edness. During d ta collection, th experiment interface would adapt to the handedness of ind vidual part cipant to ensure the comfort range was valid.\nOverall, the multi-person d taset contains 38,545 taps, including 20,615 taps from front, 10, 05 back, 1,705 left, 1,705 right, 1,975 top, and 2, 40 bottom, among which 58.3% was collected on phones with rubber ca es, and the rest without.\n5 MACHINE LEARNING EXPERIMENTS This section evaluates the overall performance of TapNet, followed by the comparison between one- and multi- cha nel C N as well as ablation studies on the MIMO network design.\nWe measured classification performance by F1 score and regression by mean absolut er or (MAE) and r2 score. F1 score is an weighted average metric of precision and recall, ranging from [0, 1]. The MAE of tap location, i.e. √︃ [(𝑔𝑥 − 𝑡𝑥 )/𝑤]2 + [(𝑔𝑦 − 𝑡𝑦)/ℎ]2, is computed by the Euclidean distance between ground truth location (𝑔𝑥 , 𝑔𝑦) and TapNet output (𝑡𝑥 , 𝑡𝑦) normalized by screen width𝑤 and height ℎ. Its range is [0, 1.414], and a baseline (always predicting the center) is 0. 07. Similar to standar deviation, r2 measures ho well the regression line fits the d ta with a range of [0, 1].\nIn the implementation, trai ing to recognize the presence of tap event requires tap and non-tap data (e.g. phone shaking and rubbing motions). As non-tap data does not have annotations about tap finger part, direction, and location, it ca not be used to train the rest of the network. As such, the two parts of the network (tap event v.s. the rest of the tap properties) were trained in turn. The tap event branch was trained once after every ten epochs of training the rest of the tap property branches.\nWe applied ReLU and batch normalization after each convolutional layer and used Adam optimizer with a learning rate of 1e-4 and a momentum decay (1e-6). Each model was trained with sufficient number of epochs until the validation (5% data) loss converges, and training was done using Tensorflow [1] on a 4G memory GPU.\nThe datasets were collected on Phone A and Phone B that ran on Android 9, and the IMU sampling rate was approximately 416Hz. In run time, it took 0.56 ms on the Phone B main CPU to finish one TapNet inference. A simplified TapNet can also be run in real time (9ms) on the embedding DSP using Tensor Lite for microcontroller [31], but this is beyond the scope of this paper. The major latency (105 ms) of the whole pipeline lies in waiting to observe the complete tap signal in the feature window.\nDue to space limitation, we have put some of the implementation evaluations into the supplemental file. Interesting findings include: 1) high sensor sampling rate is crucial to tap recognition accuracy; 2) the use of gyroscope in our task configuration is important; and 3) data augmentation by temporally shifting the samples conduces to the performance gain but scaling the samples does not.\n5.1 Performance on Tap Recognition Tasks This section evaluates TapNet’s improvement over prior art and user generalizability when training on one-person data.\n5.1.1 Improving the state of the art. We implemented and trained four ML algorithms, including TapNet, to compare their relative performance. The overall performance measured by the weighted\naverage F1 score and MAE across participants and devices are shown in Table 4. Su port Vector Machine (SVM) represents a line of studies [19, 39] that used trad tional machine learning methods. TinyC N is a replication of Liang et al.’s two-layer C N [17]. SISO r fers to a truncated single-input and single-output (SISO) TapNet. It uses the same training configuration (Adam optimizer and learning rate) as the MIMO TapNet, and it gives performance reference if TapNet is configured for a single task. All the methods were trained on the one-person dataset and tested on the multi-person dataset.\nTable 4: Weighted average F1 score of four classification tasks (no color shading) as well as MAE and r2 score of the tap location regression task (purple shading). SVM and TinyC N are our re-implementation of relatedworks [17, 19, 39]. SISO TapNet is the truncated single-input and single-output variant. TapNet is the proposedMIMOmethod. Note that we have built one model per task for the single-output models. In contrast, TapNet is a multi-task network that gives predictions for all five tasks at a time. Finger part, direction, and location are not conditional on the event detection. The numbers are the higher the better for F1 and r2 scores, while lower the better for MAE.\nF1 score MAE (r2)\nMethod 2- class Event 2-class Finger part 6-class Direction 35-class Location Location regression\nSVM [19, 39] .73 .93 .55 .13 .20 (.28) TinyCNN [17] .89 .93 .47 .03 .40 (-2.4) SISO TapNet .88 .89 .82 .30 .19 (.12) TapNet .87 .94 .85 .34 .14 (.49)\nOverall, MIMO TapNet significantly outperforms the prior art [17, 19, 39]. Compared with the best performance among SVM and TinyCNN, TapNet achieved considerable improvements on tap direction (by 51.0%) and location (classification: 161.5% and regression: 30%). The SISO TapNet variant also outperforms related works by a marked margin. TapNet generally outperforms its SISO variant, implying that the MIMO architecture also contributes to performance improvement in addition to the computation and memory benefits.\nA close examination of the 35-class tap location classification reveals that TapNet can be usable despite its seemingly limited F1 score (0.34). Figure 4(a) shows the normalized confusion matrix of tap location classification. Neighboring region above or below the target in a 5x7 grid has an index offset of five. The three parallel lines along the diagonal with a five-cell offset in the confusion matrix indicates TapNet either predicts correctly or to nearby regions (see Figure 4(c)). This is in good agreement with the regression results (MAE:.14; ∼10% of the screen diagonal). Such location error is similar to the distance between two icons on the phone home screen (see the supplemental video figure). It thus indicates that IMU-based tap location recognition can be viable and useful in situations which does not require very high resolution and when capacitive sensing is inadequate (e.g. wearing gloves or under water).\nOverall, MIMO TapNet significantly outperforms the prior art [17, 19, 39]. Compared with the best performance among SVM and TinyCNN, TapNet achieved considerable improvements on tap direction (by 51.0%) and location (classification: 161.5% and regression: 30%). The SISO TapNet variant also outperforms related works by a marked margin. TapNet generally outperforms its SISO variant, implying that the MIMO architecture also contributes to performance improvement in addition to the computation and memory benefits.\nA close examination of the 35-class tap location classification reveals that TapNet can be usable despite its seemingly limited F1 score (0.34). Figure 4(a) shows the normalized confusion matrix of tap location classification. Neighboring region above or below the target in a 5x7 grid has an index offset of five. The three parallel lines along the diagonal with a five-cell offset in the confusion matrix indicates TapNet either predicts correctly or to nearby regions (see Figure 4(c)). This is in good agreement with the regression results (MAE:.14; ∼10% of the screen diagonal). Such location error is similar to the distance between two icons on the phone home screen (see the supplemental video figure). It thus indicates that IMU-based tap location recognition can be viable and useful in situations which does not require very high resolution and when capacitive sensing is inadequate (e.g. wearing gloves or under water).\nTapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input CHI ’21, May 8–13, 2021, Yokohama, Japan\nHowever, the performance difference on simple tasks (event and finger part classification) among different methods is marginal. This suggests that the minimal model capacity can be task-dependent; simple tasks may not be benefited from a model capacity increase, but the more complicated tasks can be.\n5.1.2 Achieving person generalizability with one-person data. We hypothesized that training on diverse but one-person data can still achieve generalizability for unseen users in some tap recognition tasks. To verify this, we performed evaluation in two paradigms:\n• One-to-n-participant evaluation: TapNet was trained on the one-person dataset and tested on the multi-person dataset. This is the same as the previous evaluation. • Leave-one-participant-out cross-validation: TapNet was pretrained on the one-person dataset, fine tuned on n-1 participants from the multi-person dataset, and tested on the leftout participant in an iterative fashion.\nBy comparing the one-to-n-participant with the leave-one-out evaluation (see Table 5), we see that training on a one-person data can still be effective. As expected, fine tuning on the n-1 participants further improved the performance, but some of the improvements are relatively moderate, for example, by 1% for tap event and 3.2% for finger part classification. This corroborates the hypothesis that training on the diverse data even from a single person can achieve viable user generalizability for tasks, such as event (F1:.92), finger part (F1:.93), and direction classification (F1:.85). That said, we also\nlearned that tap location recognition relies on very subtle IMU\nresponses, which may relate to personal biomechanics and are hard\nto simulate by a single person."},{"header":"5.2 Ablation Studies: Multi- Input and Output","body":"This section presents the ablation studies on the TapNet architec-\nture: the use of multi-task learning (i.e. multi-output) and cross-\ndevice training with auxiliary information (i.e. multi-input). We\nevaluated on tap direction classification as a representative task.\nTable 5: Performance comparison between the one-to-nparticipant and leave-one-participant-out evaluation. The columns with no color shading are classification tasks, and the one with purple shading is a regression task.\nTapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input CHI ’21, May 8–13, 2021, Yokohama, Japan\nThis is the same as the previous evaluation.\n• Leave-one-participant-out cross-validation: TapNet was pre-\ntrained on the one-person dataset, fine tuned on n-1 par-\nticipants from the multi-person dataset, and tested on the\nleftout participant in an iterative fashion.\nBy comparing the one-to-n-participant with the leave-one-out\nevaluation (see Table 5), we see that training on a one-person data\ncan still be effective. As expected, fine tuning on the n-1 participants\nfurther improved the performance, but some of the improvements are relatively moderate, for example, by 1% for tap event and 3.2% for fing part classific tion. This corr borates the hypothesis that training on the diverse data even from a single person can achieve viable user generalizability for tasks, such as event (F1:.92), fi ger part (F1:.93), and directio classification (F1:.85). That said, we also learned that ta location recog ition relies on very subtle IMU responses, which may relate to personal biomechanics and are hard to simulate by a single person.\n5.2 Ablation Studies: Multi- Input and Output This section presents the ablation studies on the TapNet architecture: the use of multi-task learning (i.e. multi-output) and crossdevice training with auxiliary information (i.e. multi-input). We evaluated on tap direction classification as a representative task.\nFigure 4: (a) Normalized confusion matrix of the 35-class location classification while training on one person and testing on multiple. (b) An enlarged part of the confusion matrix. (c) The region IDs in a five-by-seven grid. Each region refers to a cell in the grid. Neighboring areas above or below the target has an index offset of five. Predicting to the nearby areas therefore leads to three parallel lines along the diagonal in the confusion matrix.\nTable 5: Performance comparison between the one-to-nparticipant and leave-one-participant-out evaluation. The columns with no color shading are classification tasks, and the one with purple shading is a regression task.\nF1 score MAE (r2)\nEvaluation Paradigm\n2- class Event 2-class Finger part 6-class Direction 35-class Location Location regression\none-to-n .92 .93 .85 .42 .15 (.52) leave-one-out .93 .96 .92 .54 .11 (.73)\nFigure 5: Weighted average F1 score of tap direction classification using multi-task learning (SIMO TapNet) and singletask learning (SISO TapNet). TapNet (either SIMO or its truncated variant) can considerably outperform a SVM [19, 39] and a tiny CNNmodel [17]. TapNet withmulti-task learning shows advantage given small training data (e.g. 3K samples).\ntasks and memory saving to avoid running multiple models on\nsmall processing units are essential. One the other hand, both SIMO\nand SISO TapNets can considerably outperform TinyCNN (pur-\nple dashed) starting from 3K training samples, and outperform\nSVM (blue dashed) starting from 6K. Taken together, a multi-output\nTapNet is favorable over the state of art [17, 19, 39].\n5.2.1 Multi-task learning helps training with limited data. We first present the evaluation on multi-task learning. We compared against the single-output counterpart (SISO; a truncated TapNet variant) and recent related works, including a SVM [19, 39] and a tiny CNN model [17]. Limited by the tap samples on Phone B (∼15K) in the training dataset, we evaluated model performance averaged over two devices with incremental training samples from 1K to 15K. Figure 5 shows the comparison results.\nMulti-task learning contributes to tap direction classification, especially with a small amount (no more than 3K) of training samples. Nevertheless, with sufficient training samples (over 3K) performance of SIMO and SISO TapNets flats out with a comparable F1\nscore (SIMO:.81, SISO:.80). Although the performance improvement\nof SIMO over SISO is modest in this specific task, its advantages\nof reusing computation (for the shared layers) across recognition\ntasks and memory saving to avoid running multiple models on\nsmall processing units are essential. One the other hand, both SIMO\nand SISO TapNets can considerably outperform TinyCNN (pur-\nple dashed) starting from 3K training samples, and outperform SVM (blue dashed) starting from 6K. Taken together, a multi-output TapNet is favorable over the state of art [17, 19, 39].\nCHI ’21, May 8–13, 2021, Yokohama, Japan Huang, et al.\n5.2.2 Cross-device training utilizes data efficiently. To investigate cross-device model training, we evaluated model performance with incremental training samples from 1K to 15K, as previous experiments suggest that TapNet converges with around 15K training samples. Note that in the case of training with 15K samples, TapNet was jointly trained on 7.5K samples from Phone A and another 7.5K samples from Phone B. Similarly, its counterparts were pre-trained on 7.5K samples from one device and fine tuned on the other. As baselines, we also evaluated training on the 7.5K device-specific samples alone for each device.\nFigure 6 gives the performance comparison. A+B represents TapNet jointly trained on cross-device data; B->A and A->B denote tuning the pre-trained model from one device on the other; and A and B indicate models trained on device-specific data. The overall performance of different models increases with training samples and they flats out after 4.5K samples per device. More interestingly, cross-device training with sensor location as auxiliary information (A+B) can approach the performance upper bound earlier (with 1.5K samples per device) than the rest of the models."},{"header":"5.3 Signal Alignment in One-Channel CNN","body":"To verify the efficacy of one-channel CNN for tap recognition, we performed a comparison on tap direction classification, which is a representative task and demonstrated to require subtle signal alignment across channels. As input data dimension affects the number of trainable parameters (i.e. model capacity) of a specific network architecture, we evaluated similar architectures with commensurate number of trainable parameters, so as to compare models with comparable capacity. This experiment was conducted on the Phone A data using the one-to-n evaluation paradigm.\nFigure 7 shows the performance comparison of one-channel CNN against its multi-channel counterparts. The x-axis shows the number of training samples, and the y-axis the weighted average of\nF1 score. The solid lines denote the performance of large-capacity models with 163K (one-channel) and 144K (six-channel) trainable parameters, while the dashed lines those of small-capacity models with 11K (one-channel) and 9K (six-channel) trainable parameters. Most importantly, small-capacity one-channel CNN (purple dashed) and large-capacity six channel CNN (blue) have almost equivalent performance with 6K-12K training samples. Further, comparing the solid and dashed lines, large-capacity models generally outperforms small-capacity models with similar architecture and input format. Taken together, we conclude that the one-channel CNN can address across-channel signal alignment more efficiently than its multichannel counterpart for tap property recognition."},{"header":"6 DISCUSSION","body":"We designed, developed, and evaluated a set of deep learning methods for on-device IMU signal based off-screen input, in particular TapNet, a multi-task network that allows for cross-device training with phone form factor as auxiliary information and joint prediction of tap-related tasks, including tap event, direction, finger part, location classification, and the regression of tap location. This architecture not only shares knowledge across tap properties and devices during training, but also shares computation and memory at run time. Some of the TapNet building blocks are not novel in fields such as computer vision, but to bring them into building an interaction gesture to a practical performance level required original research. TapNet achieved a marked improvement over the prior art [17, 19, 39] on tap direction (by 51.0%) and location (161.5%) classifications as well as tap location regression (30%).\nWe also discovered encouraging generalizability across users when the model was trained (or \"taught\") by a one person. We have\nTapNet: The Design, Training, Implementation, and Applications of a Multi-Task Learning CNN for Off-Screen Mobile Input CHI ’21, May 8–13, 2021, Yokohama, Japan\ntested TapNet on those who had not been in any set of the data collection and encouraged them to explore the effects of TapNet freely on their own device in daily use (for such functions as taking screenshot). Their experience matched with the leave-one-participant-out test results reported here. This high generalizability is probably because the inter-class differences (e.g. finger pad vs. nail in the finger part classification task) are generally much greater than the inter-person differences, such that the personal biomechanics only imposes a neglectable effect.\nThe other advantage of our approach is that an expert design could intentionally push the variations of an intentional tap gesture in terms of speed, strength, angle, and hand posture, However, it is possible certain recognition tasks, such as higher resolution tap location classification, could demandmore fine-grained information only available in person-specific data. Training on incremental oneperson data can increase performance up to a certain level and then plateaus (see Figure 8). Further adapting the model to multi-person data may teach the model to understand the artifacts of personal biomechanics, and thus further improves the performance until its next plateau. This performance gain is from knowing how much people can vary. To reach the ideal performance, it becomes a must to know the person-specific information, i.e. how exactly the target user acts. In practice, especially for learning-based gesture studies, it can be beneficial to identify the curve in Figure 8, and then decide the needed number of training participants for specific tasks.\nTapNet opens up new interaction opportunities such as onehanded interaction that uses off-screen tapping or designated tap gestures. TapNet is robust to phone case and fabric, and thus can be used for wearable interaction without specialized smart fabrics [7]. This study also offers new potential for other research domains, including biometrics. For instance, it is possible to improve continuous and passive authentication [2, 35, 46] by using tap properties that contain biometric information, i.e. with a clear gap between the first two plateaus (orange area) in Figure 8.\nAlthough evaluation results demonstrated that TapNet benefited from cross-device training, we do not expect the current TapNet trained on these two devices would directly apply to unseen devices without further training, i.e. a cross-device model. However, the joint training architecture has been shown effective and this is a\npromising step toward a device adaptive model. We have set up the infrastructure and we plan to further investigate along this line by adding new devices and also opensource our implementation."},{"header":"7 HCI APPLICATIONS OF TAPNET","body":"Without adding new sensor hardware, TapNet enables many new input possibilities on smartphones. In addition to providing shortcuts to quick activations of apps or functions such as camera and screenshot, this section sketches out four applications enabled by TapNet’s capability of measuring multiple proprieties in addition to the presence of tap event, such as direction and location. Please refer to our supplemental video for these applications in action."},{"header":"7.1 AssistiveTap","body":"AssistiveTap allows users to complete a number of system interactions using back tap and tilting. Users can perform a back double tap to invoke the AssistiveTap interface (see Figure 9a), then tilt the phone (based on the same IMU signal input to TapNet) to select gesture, and back tap to perform the selected gesture (e.g. ’back’ gesture, scrolling, app switch). In other words, AssistiveTap provides the back-of-device alternatives for the most commonly used on-screen gestures, by using the gesture combination of back tap and tilting. It can be beneficial in situations, where one-handed interaction is preferable or even required."},{"header":"7.2 ExplorativeTap","body":"ExplorativeTap is a two-handed interaction method that exploits signals from both the front (touch) and back of the phone tap (see Figure 9b). Users can use the exploration finger on the screen to glide over the on-screen objects, hear them, and perform a back tap to confirm selection. This is an improvement for the visual impairment accessibility modes, such as Voice Over on iOS and Talk Back on Android. These accessibility modes occupy the common on-screen gestures (e.g. swipe and touch), and their users need to learn a more complicated system navigation gestures. In contrast, ExplorativeTap starts the accessibility mode by a double back tap, selects object by a single back tap, and exits by lifting the exploration finger. It, therefore, solves the conflicts with system navigation gestures, and thus can be helpful for users with low\nCHI ’21, May 8–13, 2021, Yokohama, Japan Huang, et al.\nvision and print disability, who need quick and temporary access to the accessibility mode."},{"header":"7.3 Interactive Wallpaper","body":"Off-screen tap recognition makes it possible to interact with objects living in different interface layers, such as background targets that do not react to on-screen touch. For instance, TapNet can enable users to interact with flashcards or news feeds shown in the wallpaper (see Figure 9c). They can back tap to change the flashcard or news feeds, or edge tap to switch a different set of them, and these can be done even on the lock screen. It enables the user to utilize the bits and pieces of time for their favorite spare time activity."},{"header":"7.4 Inertial Touch","body":"Inertial Touch estimates tap force from the IMU signals and tap location from the TapNet output. We can define Inertial Touch as a fast touch event with a strong tapping momentum. As TapNet estimates tap location from force and angle changes, it can function even when capacitive sensing fails (Figure 9d). Common use situations include when users are wearing gloves, having long fingernails, or when there are waterdrops on the touchscreen."},{"header":"7.5 Summary of Use Cases","body":"The aforementioned use cases can be particularly useful in challenging situations when one-handed and non-contact interactions are preferable or required. For example, AssistiveTap exploits back tap and titling to partially address the issue of limited thumb reaching area during one-handed interaction. ExplorativeTap allows for the coordination between on-screen and off-screen interactions and thus saves the need for learning additional set of on-screen gestures. Inertial Touch offers auxiliary impact information beyond capacitive sensing. These are just a few application examples of tapping into TapNet’s ability to detect multiple tap properties."},{"header":"8 CONCLUSION","body":"This paper presents the design, training, implementation and application of TapNet for off-screen mobile input. TapNet employs a multi-task convoluational neural network with motion signals as primary input and phone form factor as auxiliary information. It allows for joint learning on data across devices and simultaneous estimation of multiple tap properties. In comparison to many alternatives, this neural network architecture worked the best towards our goal of increasing the UI design space of off-screen interactions. To train and optimize this and other alternative ML models, we developed a one-person dataset for training and amulti-person dataset for testing. The evaluation results show that 1) TapNet significantly outperformed the state of the art especially in difficult recognition tasks such as tap location estimation; 2) multi-task learning is more data efficient, showing greater advantage particularly with a limited amount of training data; 3) cross-device training with phone form factor increases the efficiency of data utilization; and 4) one-channel CNN can achieve cross-channel signal alignment more efficiently than its multi-channel counterpart. We verified the hypothesis that training on the one-person data can still generalize well across users if the diversity of the training data can be ensured. This sheds light on the conceptual relation between model performance and\nML training strategy in IMU-based input systems. We demonstrated that many new interaction use cases could be enabled by TapNet. Taken together, the TapNet project made significant progress towards practically enabling and enlarging the off-screen interaction design space by deep learning from on-device IMU signals, establishing new benchmarks with reproducible results, datasets and codebase."}],"type":"Sections"}