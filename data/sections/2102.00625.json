{"sections":[{"body":"CCS CONCEPTS â€¢Human-centered computingâ†’ Empirical studies in HCI ; â€¢Applied computingâ†’ Psychology; Law.\nKEYWORDS AI, Moral Responsibility, Responsibility, Moral Judgment, Blame, Liability, COMPAS, Bail Decision-Making\nACM Reference Format: Gabriel Lima, Nina GrgiÄ‡-HlaÄa, and Meeyoung Cha. 2021. Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making. In CHI Conference on Human Factors in Computing Systems (CHI â€™21), May 8â€“13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3411764.3445260\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI â€™21, May 8â€“13, 2021, Yokohama, Japan Â© 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445260"},{"header":"1 INTRODUCTION","body":"Who should be held responsible for the harm caused by artificial intelligence (AI)? This question has been debated for over a decade since Matthiasâ€™ landmark essay on the responsibility gap of autonomous machines [68]. This gap is posed by highly autonomous and self-learning AI systems. Until now, scholars in multiple disciplines, including ethics, philosophy, computer science, and law, have suggested possible solutions to this moral and legal dilemma. Optimistic views proclaim that the gap can be bridged by proactive attitudes of AI designers, who should readily take responsibility for any harm [20, 72]. Some even propose to hold AI systems responsible per se [91], viewing human-AI collaborations as extended agencies [45, 48]. In contrast, pessimistic views question whether this gap can be bridged at all, since there might not exist appropriate subjects of retributive blame [26] nor it makes sense to hold inanimate and non-conscious entities responsible for their actions [16, 89, 96].\nMost research on the responsibility gap has been normative in that they prescribed ethical principles and proposed solutions. However, there is a growing need for practical and proactive guidelines; as Mittelstadt puts it, â€œprinciples alone cannot guarantee ethical AIâ€ [69]. Some even argue that normative approaches are inappropriate as they can hurt AIâ€™s adoption in the long run [12]. In contrast, relatively little attention has been paid to understanding the publicâ€™s views on this issue, who are likely the most affected stakeholder when AI systems are deployed [78].\nWe conducted two survey studies (ğ‘=200 each) that collect the public perception on moral responsibility of AI and human agents in high-stakes scenarios. We approached the pluralistic view of responsibility and considered eight distinct notions compiled from philosophy and psychology literature. Real-life adapted vignettes of AI-assisted bail decisions were used to observe how people attributed specific meanings of responsibility to i) AI advisors vs. human advisors and ii) AI decision-makers vs. human decisionmakers. Our study employed a within-subjects design where all participants were exposed to a diverse set of vignettes addressing distinct possible outcomes from bail decisions.\nOur findings suggest that the eight notions of responsibility considered can be re-grouped into two clusters: one encompasses present-looking and forward-looking notions (e.g., responsibility-astask, as-power, as-authority, as-obligation), and the other includes backward-looking notions (e.g., blame, praise, liability) and causal\nar X\niv :2\n10 2.\n00 62\n5v 1\n[ cs\n.C Y\n] 1\nF eb\n2 02\ndeterminations. We discuss how theories of moral responsibility can explain these clusters.\nIn comparing AI agents against human agents, we found a striking difference in the way people attribute responsibility. A substantially higher degree of the present- and forward-looking notions were attributed to human agents than AI agents. This means that AI agents were assigned the responsibility to complete and oversee the same task to a lesser extent than human agents. No difference, however, was observed for the backward-looking responsibility notions. This finding suggests that blame, liability, and causal responsibility were ascribed equally to AI and human agents, despite electronic agents not being appropriate subjects of liability and blame [16, 26, 89]. In addition to these findings, we found that people expect both human and AI agents to justify their decisions.\nThe findings of this study have several implications for the development and regulation of AI. Using the proposition of morality as a human-made social construct that aims to fulfill specific goals [91, 93], we highlight the importance of users and designers taking responsibility for their systems while being held responsible for any norm-violating outcomes. We also discuss the possibility of holding AI systems responsible per se [61] alongside other human agents, as a possible approach congruent to the public opinion."},{"header":"2 BACKGROUND","body":""},{"header":"2.1 Theories of (Moral) Responsibility","body":"Theories of moral responsibility date back to Aristotle, who argued that an entity should satisfy both freedom and epistemic conditions to appropriately be ascribed to moral responsibility. Agents must act freely, without coercion, and understand their actions. Although recent scholarly work does not directly challenge these Aristotelian conditions, they argue that moral responsibility cannot be explained as a single concept, but that it involves a relatively pluralistic definition of what it means to hold someone morally responsible [87, 102].\nScanlon [85] has proposed moral responsibility to be a bipartite concept. One is that there is an account of being responsible in rendering an agent worthy of moral appraisal. Another is that it is also possible to hold one responsible for specific actions and consequences. Expanding this bipartite concept, Shoemaker [87] has proposed three different concepts of moral responsibility: attributability, answerability, and accountability. Various other definitions have been proposed [102], including structured notions of what responsibility is [104] and how they are connected [14, 34].\nAttributing responsibility to an entity can be both descriptive (e.g., causal responsibility) and normative (e.g., blameworthiness). For the former, one might ask if an agent is responsible for an action or state-of-affairs, while the latter concerns whether one should attribute responsibility to an agent. Responsibility can also be divided into backward-looking notions if they evaluate a past action and possibly lead to reactive attitudes [106], or forwardlooking notions if they prescribe obligations.\nResponsibility can take many forms. It not only addresses the moral dimension of society but also tackles legal concepts and other descriptive notions. One can be held legally responsible (i.e., liable) regardless of their moral responsibility, as in the case of strict or vicarious liability. Stating that an agent is causally responsible for\na state-of-affairs does not necessarily prescribe a moral evaluation of the action.\nHolding an agent â€œresponsibleâ€ fulfills a wide range of social and legal functions. Legal scholars state that punishment (which could be seen as a form of holding an agent responsible, e.g., under criminal liability) aims to reform the wrongdoers, deter re-offenses and similar actions, and resolve retributive sentiments [4, 99]. Previous work has addressed how and why people assign responsibility to various agents. The general public might choose to hold a wrongdoer responsible for restoring moral coherence [22] or reaffirming a communal moral values [109]. Psychological research indicates that people base much of their responsibility attribution on retributive sentiments rather than deterrence [18], while overestimating utilitarian goals in their ascription of punishment (i.e., responsibility) [17]. Intentionality also determines how much responsibility is assigned to an entity [70]; people look for an intentional agent to hold responsible and infer other entitiesâ€™ intentionality upon failure to find one [40]."},{"header":"2.2 Techno-Responsibility Gaps","body":"AI systems and robots are being widely adopted across society. Algorithms are used to choose which candidate is most fit for a job position [111], decide which defendants are granted bail [33], guide health-related decision [73], and assess credit risk [49]. AI systems are often embedded into robots or machines, such as autonomous vehicles [13] and robot soldiers [3]. A natural question here is: if an AI system or a robot causes harm, who should be held responsible for their actions and consequences?\nIn answering this question, some scholars have defended the existence of a (techno-)responsibility gap [68] for autonomous and selflearning systems.1 The autonomous component of AI and robots challenges the control condition of responsibility attribution. Simultaneously, their self-learning capabilities and opacity do not allow users, designers, and manufacturers to foresee consequences. Similarly to the â€œproblem of many handsâ€ in the assignment of responsibility to collective agents [102], AI and robots suffer from the â€œproblem of many things,â€ i.e., current systems are composed of various interacting entities and technologies, making the search for a responsible entity harder [24]. Scholars have extensively discussed the assignment of responsibility for autonomous machinesâ€™ actions and have expanded this gap to more specific notions of responsibility [5, 8, 54] and its functions [26, 62].\nAlthough a clear separation is fuzzy, one may find two schools of thought on the responsibility gap issue. One side argues that designers and manufacturers should take responsibility for any harm caused by their â€œtools.â€ [16, 31] Supervisors and users of these systems should also take responsibility for their deployment, particularly in consequential environments like the military as argued by Champagne and Tonkens [20]. The exercise of agency by these systems can be viewed as a human-robot collaboration, in which humans supervise and manage the agency of AI and robots [72]. Humans should focus on their relationship to the patients of their responsibility to answer for the actions of autonomous systems [24].\n1Scholars also raise doubt on the existence of techno-responsibility gaps, arguing that moral institutions are dynamic and flexible and can deal with these new technological artifacts [53, 95].\nLikewise, other authors argue that society should hold humans responsible because doing so for a machine would be meaningless as it does not understand the consequences of their actions or the reactive attitudes towards them [89, 96], possibly undermining the definition of responsibility [47].\nOn the opposite side, some scholars propose autonomous systems could be held responsible per se [61]. From a legal perspective, non-human entities (e.g., corporations) can be held responsible for any damage that they may cause [103]. These scholars often view these human-AI collaborations as extended agencies where all entities should be held jointly responsible [45, 48]. AI and robots are part of the socio-technological ensemble, in which responsibility can be distributed across multiple entities with varying degrees [32]. These proposals arguably contribute to legal coherence [98], although it could also lead to various repercussions in moral and legal institutions [8]. Empirical findings indicate that people attribute responsibility to these systems [7, 62], although to a lesser extent than human agents. According to some scholars, holding AI and robots responsible per se could fulfill specific social goals [23] and promote critical social functions [11, 91].\nThe regulation of AI and robots poses new challenges to policymaking, as in the previously introduced techno-responsibility gap, which society must discuss at large [24]. The â€œalgorithmic social contractâ€ requires inputs from various stakeholders, whose opinion should be weighed for the holistic crafting of regulations [78]. It is crucial to understand how people perceive these systems before their wide deployment [80]. Our responsibility practices depend on folk-psychology [15] (i.e., how people perceive the agents involved in social practices [91]). Literature exists on the public perception of moral and legal issues concerning AI [6, 7, 62]. However, little datadriven research has collected public opinion on how responsibility should be attributed for AI and robotsâ€™ actions."},{"header":"2.3 Responsibility, Fairness, Trust in HCI Literature","body":"A growing number of HCI research has been devoted to understanding how people perceive algorithmic decisions and their consequences in society. For instance, Lee et al. studied peopleâ€™s perception of trust, fairness, and justice in the context of algorithmic decision-making [56, 57] and proposed how to embed these views into a policymaking framework [58]. Other scholars explored peopleâ€™s perceptions of procedural [41] and distributive [84, 90] aspects of algorithmic fairness and studied how they relate to individual differences [42, 76, 108]. Nonetheless, little attention is paid to the public attribution of (moral) responsibility to stakeholders (e.g., [43, 56, 81]), particularly the prospect of responsibility ascription to the AI system per se. The current study contributes by addressing the public perception of algorithmic decision-making through the lens of moral responsibility.\nExisting studies addressing how users might attribute blame to automated agents have mostly focused on robots. For instance, Malle et al. observed that peopleâ€™s moral judgments between human and robotic agents differed in that respondents blamed robots to a more considerable extent had they not taken a utilitarian action [67]. Furlough et al. found that respondents attributed similar levels of blame to robotic agents and humans when robots were described\nas autonomous and at the same time the leading cause of harm [37]. However, these studies and many others [52, 59, 105] tackle a singular notion of responsibility related to blameworthiness [102]. The present research explores multiple notions of moral responsibility of both human and AI agents involved in decision-making."},{"header":"3 METHODOLOGY","body":""},{"header":"3.1 Algorithmic Decision-Making","body":"AI-based algorithms are now used to assist humans in various scenarios, including high-stakes tasks such as medical diagnostics [35] and bail decisions [2]. These algorithms do not make decisions themselves, but rather â€œadviseâ€ humans in their decision-making processes. One such algorithm is the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) tool, used by the judicial system in the US to assist bail decisions and sentencing [2]. Several studies have analyzed the fairness and bias aspects of this risk assessment algorithm, e.g., [9, 33, 43].\nThis studymakes use of publicly available COMPAS data released by ProPublica [2] and considers the machine judgments as either an AI advisor (later in Study 1) or an AI decision-maker (in Study 2). As stimulus material, we use real-world data obtained from a previous analysis of the tool [2], which focused on its application in bail decision-making. This dataset contains information about 7,214 defendants subjected to COMPAS screening in Broward County, Florida, between 2013 and 2014.\nWe use 100 randomly selected cases from this dataset, the corresponding bail suggestions, and information about whether the defendant re-offended within two years of sentencing. The sampled data was balanced concerning these variables. Each defendantâ€™s COMPAS score ranges from 1 to 10, with ten indicating the highest risk of re-offense or nonappearance in court. In this study, scores 1 to 5 were labeled â€œgrant bailâ€ and 6 to 10 were labeled â€œdeny bail.â€"},{"header":"3.2 The Plurality of Responsibility","body":"Ascribing responsibility is a complex moral and legal practice that encompasses various functions, entities, and social practices [71, 91]. Responsibility has multiple distinct meanings depending on its purpose and requirements. The current study revisits eight notions of responsibility compiled from psychology and philosophy. All of these notions originated from Van de Poelâ€™s work [101, 102], except for responsibility-as-authority and as-power, which comes from Davisâ€™s discussion on professional responsibility [27]. We complement these notions with a wide range of literature ranging from philosophical theories of moral responsibility (e.g., [86, 87]) to approaches in the context of AI systems (e.g., [24, 96]). Although not exhaustive (e.g., we have not addressed virtue-based notions of responsibility as they cannot be easily adapted to AI systems), we highlight how our work differs from previous HCI approaches.\nâ€¢ Responsibility-as-obligation: E.g., â€œThe (agent) should ensure that the rights of the defendant are protected.â€ One could be held responsible-as-obligation through consequentialist, deontological, and virtue-based routes [102]. While an entity could be attributed suchmeaning of responsibility based on pre-determined consequentialist distribution\nprinciples, the latter two routes presuppose the agentâ€™s initiative or promise to see to it that a specific state-of-affairs is brought about. This notion differs from responsibility-astask as it does not imply that one should be the agent to bring about a specific state-of-affairs, but rather indicates that one should fulfill its supervisory duties in the process.\nâ€¢ Responsibility-as-task: â€œIt is the (agent)â€™s task to protect the defendantâ€™s rights.â€ This descriptive notion of responsibility ascribes a specific task to an entity. These assignments do not necessarily define a moral obligation per se [101] and are often accompanied by the understanding that an entity has to do something by itself [27]. In our experimental design, we highlight the agentâ€™s acting role in completing its task.\nâ€¢ Responsibility-as-authority: â€œThe (agent) has the authority to prevent further offenses.â€ To be responsible-as-authority implies that one is in charge of a specific action or state-of-affairs. This notion has also been posed as \"responsibility-as-office\" by Davis [27] in the context of engineersâ€™ professional responsibility. An important aspect of responsibility-as-authority is the possibility of delegating other complementing notions, such as responsibility-as-task, to other agents. We address this meaning of responsibility by explicitly indicating that the agent has the authority in bail decisions.\nâ€¢ Responsibility-as-power: â€œThe (agent) has the skills needed to protect the rights of the defendant.â€ If an entity has the skills needed to bring about an action or state-of-affairs, one might ascribe it responsibility-aspower [27]. In other words, having the ability, in terms of competency, knowledge, or expertise, might lead to the assignment of this notion of responsibility.\nâ€¢ Responsibility-as-answerability: â€œThe (agent) should justify their advice.â€ This is related to how oneâ€™s reasons for acting in a specific manner could be seen under moral scrutiny. Shoemaker proposed this notion of moral responsibility as a form of judgment of oneâ€™s actions grounded in moral evaluations [87]. Davis proposed a similar meaning of responsibility under a different name, responsibility-as-accountability [27], as the responsibility for explaining specific consequences. Coeckelbergh later applied this concept through a relational approach for actions and decisions made using AI [24].\nâ€¢ Responsibility-as-cause: â€œThe (agent)â€™s decision led to the prevention of the re-offense.â€ This meaning of responsibility has been further discussed depending on the nature of an actionâ€™s consequences [27], e.g., being causally responsible for a positive state-of-affairs could lead to the ascription of â€œgood-causation.â€ Causality is also an important pre-condition for other normative notions of responsibility, such as blame, as the blurring of a causal connection raises questions on the foreseeability and control of a specific action. [66, 102]\nâ€¢ Responsibility-as-blame/praise: â€œThe (agent) should be blamed for the violation of the rights of the defendant.â€ / â€œThe (agent) should be praised for the protection of the rights of the defendant.â€ Blaming an entity for the consequences of their actions has been debated as adopting certain reactive attitudes towards it [106]. Scholars have also argued that to blame someone is to respond to â€œthe impairment of a relationship,â€ [21, 86] especially towards its constitutive standards [87]. Scholars have debated the possibility of ascribing blame to an automated agent and agree that doing so would not be morally appropriate [26, 96]. Regardless of this consensus, previous studies have found that people attribute a similar degree of blame to robotic and human agents under specific conditions (e.g., [37, 67]). As an opposite concept of blame, one may consider â€œpraiseâ€ as a positive behavioral reinforcement [51] through which one conveys its values and expectations of the agent [28]. Hence, we consider both blame and praise as responsibility notions in this research.\nâ€¢ Responsibility-as-liability: â€œThe (agent) should compensate those harmed by the re-offense.â€ An entity that is ascribed this responsibility should remedy any harm caused by their actions [102]. Rather than dwelling on the discussion addressing the mental states of AI and robots and their arguable incompatibility with criminal law and its assumption of mens rea [39, 55, 60], we address this notion from a civil law perspective. Scholars propose â€˜making victims wholeâ€™ as the primary goal of tort law [77], and hence, we similarly address responsibility-as-liability. We also add that the idea of holding automated agents liable became prominent after the European Parliament considered adopting a specific legal status for â€œsophisticated autonomous robotsâ€ [29]. Nevertheless, it is important to note that current AI systems cannot compensate those harmed, as they do not possess any assets to be confiscated [16]."},{"header":"3.3 Survey Design","body":"â€¢ Study 1: AI as Advisor To study how the perceived responsibility for bail decisions differs when judges are advised by the COMPAS tool or by another human judge, we considered the following scenario:\nImagine that you read the following story in your local newspaper: A court in Broward County, Florida, is starting to use an artificial intelligence (AI) program to help them decide if a defendant can be released on bail before trial. Early career judges are taking turns receiving advice from this AI program and another human judge, hired to serve as an advisor.\nWe employed a factorial survey design [107] and showed participants eight vignettes that described a defendant from the ProPublica dataset, information about who the advisor was (i.e., an AI program or a human judge), which advice they gave, what the judgeâ€™s final decision was, and whether the defendant committed a new crime within the next two years (i.e., re-offended). All vignettes stated that the judgesâ€™ final decision followed the advice given, given the\nProPublica dataset does not provide this information. After reading the stimulus material, respondents were asked to indicate to what extent they agreed with a set of statements, presented in random order between participants, regarding the advisor on a 7-point Likert scale (-3 = Strongly Disagree, 3 = Strongly Agree).2 These statements aimed to capture different notions of responsibility (see Table 2 in the Appendix for the complete list). Figure 1 illustrates the survey methodology. Participants were also asked two attention check questions in between vignettes.\nEach participant in the study was exposed to a random subset of four cases with human advice and another four with AI advice. We ensured a balanced set was shown to each participant in terms of the advice (i.e., grant bail vs. deny bail) and recidivism. As a result, each respondent was shown one vignette of every possible combination of scenarios, encompassing eight (advice Ã— recidivism Ã— AI vs. human) variations. All vignettes were presented in random order to eliminate any order effect [44, 79].\nBail decisions aim to procure a balance between protecting future victims, e.g., prevent further offenses, and to impede any unnecessary burdens towards the defendant, e.g., by ensuring that their rights are protected [43]. The latter aspect of bail decisions is related to the assumption that one is innocent until proven otherwise beyond a reasonable doubt under criminal law [30]. To strike a balance between these two functions of bail decisions, we phrase statements addressing all notions of responsibility addressed in this work in two different forms: a human agent or an AI program could be held responsible for i) (not) protecting the rights of the defendant and ii) (not) preventing re-offense. Participants were randomly assigned to one of these treatment groups, and all statements followed the same phrasing style.\nTowards the end of the survey, we asked demographic questions (presented in Table 1). We also gathered responses to a modified questionnaire of NARS (Negative Attitude towards Robot\n2Questions related to responsibility-as-liability were shown in scenarios where i) the defendant re-offended and the phrasing style addressed the prevention of re-offenses, or ii) the defendants were denied bail and did not re-offend within two years while the statements focused on protecting their rights. The phrases tackling praise and blame were presented depending on the advice/decision and recidivism.\nScale) [92], whose subscale addressed â€œartificial intelligence programsâ€ rather than â€œrobotsâ€ to accommodate the COMPAS tool.\nâ€¢ Study 2: AI as Decision-Maker Unlike Study 1, where a human decision-maker is advised by either a human or an AI advisor, Study 2 explores a setting that has yet to be implemented in the real-world. We imagine the case where an AI algorithm makes a bail decision by itself. The survey instrument and experimental design are identical to Study 1, except that in the introductory text, we told participants, \"The court is taking turns employing human judges and this AI program when making bailing decisions,\" and updated the phrasing of the questions to match this setting accordingly. In each vignette, participants were asked to what extent they agreed with the eight notions of responsibility regarding the decision-maker, i.e., the AI program or the human judge, using the same 7-point Likert scale from Study 1. Both studies had been approved by the Institutional Review Board (IRB) at the first authorâ€™s institution.\nâ€¢ Pilot Study for Validation: Cognitive Interview We validated our survey instruments through a series of cognitive interviews. Cognitive interviews are a standard survey methodology approach for improving the quality of questionnaires [83]. During the interviews, respondents accessed our web-based survey questionnaire and were interviewed by the authors while completing the survey. We utilized a verbal probing approach [110], in which we tested the respondentsâ€™ interpretation of the survey questions, asked them to paraphrase the questions, and if they found the questions easy or difficult to understand and answer.\nWe interviewed six demographically diverse respondents. Three respondents were recruited through the online crowdsourcing platform Prolific [74], while the other three were our colleagues, who had prior experience designing and conducting human-subject studies. After each interview, we iteratively refined our survey instrument based on the respondentâ€™s feedback. We stopped gathering new responses once the feedback stopped leading to new insights. This process led to two significant changes in our survey instrument\ndesign. Firstly, we adapted the vignette presentation, which was initially adapted from previous work [33]. Our respondents unanimously stated that they found information about defendants easier to read, understand, and use when presented in a tabular format (shown in Figure 4 in the Appendix). Secondly, we rephrased some of the statements about the notions of responsibility we address in this work so that survey respondentsâ€™ understanding of these concepts is similar to the definitions introduced above."},{"header":"3.4 Participants and Recruitment","body":"We conducted a power analysis to calculate the minimum sample size. A Wilcoxon-Mann-Whitney two-tailed test, with a 0.8 power to detect an effect size of 0.5 at the significance level of 0.05, requires 67 respondents per treatment group. Hence, we recruited 400 respondents through the Prolific crowdsourcing platform [74] to compensate for attention-check failures. We targeted US residents who have previously completed at least 100 tasks on Prolific, with an approval rate of 95% or above. Each participant was randomly assigned to one of the two studies.\nThe respondentsâ€™ demographics are shown in Table 1. Prior studies of online crowdsourcing platforms have found that respondent samples tend to be younger, more educated, and consist of more women than the general US population [50]. Compared to the 2016 US census [100], our respondents are indeed younger and more highly educated. However, both of our studiesâ€™ samples have a smaller ratio of women than the US population. Asian ethnicity is slightly over-represented in our samples. Compared to Pew Research data on the US populationâ€™s political leaning [75], our samples are substantially more liberal.\nThe respondents were remunerated US$10.5 for taking part in the cognitive interviews and US$1.66 for completing the online surveys. The cognitive interviews lasted less than 30 minutes, while the online surveys took 10.36Â±5.43 minutes. Hence, all study participants were paid above the US minimum wage."},{"header":"4 RESULTS","body":""},{"header":"4.1 Responsibility in Bail Decisions","body":"Figure 2 shows how people attributed each notion of responsibility to AI and human agents in Study 1 (on the advisor role) and Study 2 (on the decision-maker role).\nFirst, responsibility-as-answerability (i.e., the bar in the middle) was the notion ascribed the highest to both human and AI advisors and decision-makers, followed by responsibility-as-obligation, astask, as-authority, and as-power (i.e., the first four bars). On the other hand, liability and blame were the least attributed responsibility notion in bail decisions. Responsibility-as-cause and praise were the most neutral notions, and their mean attribution is close to zero (i.e., the baseline) across all treatments (see Figure 5 in the Appendix).\nSecond, Figure 2 shows two distinct sets of responsibility notions, where these clusters can be observed from the pairwise Spearmanâ€™s correlation chart. A high correlation value indicates that those responsibility notions are perceived similarly by people. One group includes responsibility-as-task, as-authority, as-power, and as-obligation, all of which have positive mean values. The other group includes responsibility-as-cause, praise, blame, and liability. Responsibility-as-answerability belongs to neither of these groups.\nThird, we can quantify variations across vignette conditions. Each vignette shown to participants varied in the advice given, bail decision, and recidivism, allowing us to compare across these factors. Our data show that vignettes that grant bail (as opposed to denying bail) led to a higher assignment of all responsibility notions, particularly causal responsibility and blame (see Figure 5 in the Appendix). A similar effect was found depending on defendant recidivism. For instance, the first four responsibility notions were ascribed to a more considerable degree if the defendant did not reoffend. In contrast, responsibility-as-cause, blame, and liability were attributed to a lesser extent if the defendant re-offended within two years. These trends corroborate the responsibility clusters discussed above.\nFinally, our study participants were also assigned to one of two different phrasing styles addressing some of the bailing decisionsâ€™ objectives. Except for responsibility-as-answerability, addressing the violation or protection of a defendantâ€™s rights led to amarginally higher assignment of responsibility than the phrasing style focusing on preventing re-offenses."},{"header":"4.2 Responsibility Assignment to AI and Humans","body":"Our primary goal was to examine how people attribute responsibility to human and AI agents in high-stakes scenarios. To quantify the difference, we used a multivariate linear mixed model that included a random-effects term to control for each participant. This allowed us to account for repeated measures, i.e., explicitly model that each participant responded to questions on eight distinct defendants. We\nuse the standard .05 level of significance. In all models, we use our adapted scale of pre-attitude towards AI systems as a control variable. Figure 3 shows the results. The annotated numbers indicate the differences and significance levels between the two agents. We report the full regression coefficients in Table 3 in the Appendix.\nBoth Study 1 and Study 2 show consistent differences in responsibility attribution between agents, regardless of whether they informed a human judge (Study 1) or decided by themselves (Study 2). We note subtle differences in how people attribute responsibility to AI and humans. The first four responsibility concepts are correlated; the notions addressing tasks, supervisory roles, and the skills needed to assume them show a meaningful difference between\nagent types. The respondents attributed more of these notions of responsibility to humans than to AIs.\nResponsibility-as-answerability exhibits a marginal difference with respect to the agent type that assisted human judges in bail decisions; however, the same trend was not observed in Study 2. Nevertheless, our results suggest that humans and AI are judged similarly responsible with respect to causality, blame, and liability for bail decisions. Moreover, human decision-makers are praised to a considerably larger degree than AI decision-makers, although the same effect was not observed for human and AI advisors."},{"header":"5 DISCUSSION","body":""},{"header":"5.1 The Relation Between Notions of Responsibility","body":"So far, we have observed two clusters of responsibility concepts by their correlation. The first cluster is composed of responsibilityas-task, authority, power, and obligation â€” all of which were attributed to a greater degree to humans than AI systems (Î”>0.206, ğ‘<.001). The first three are descriptive and focus on oneâ€™s tasks (i.e., task, authority) and the necessary skills for their completion (i.e., power). Furthermore, responsibility-as-obligation is related to responsibility-as-task in prescribing a specific goal to the agent; it differs from the latter, however, in setting a supervisory role towards the task, rather than specifying that one should be the one to complete it.\nThe second cluster includes causal responsibility, blame, praise, and liability â€” all of which were attributed to a similar degree to humans and AI. This finding is in line with previous work on blame assignment, highlighting the significance of causality in peopleâ€™s ascription of blame and punishment. Human subject studies suggest that blame attribution is a two-step process; it is initiated by a causal connection between an agentâ€™s action and its consequences and is followed by evaluating its mental states, i.e., intentions [25]. Malle et al. [66] have also proposed a theory of blame that is dependent on the causal connection between an agent and a norm-violating event. Our data similarly reveal such a relationship, even when controlling for the advice given, bail decision, or re-offense.\nConcerning the phrasing styles, our experiment design addressed responsibility-as-liability as the duty to compensate those harmed by a wrongful action. However, previous work on the connection between liability (i.e., punishment) and causality focuses on the\nretributive aspect of punishment [25], often drawing a connection between punishment and blame. Therefore, we do not posit that peopleâ€™s ascription of liability is solely dependent on causality determinations. We hypothesize that the low assignment of liability is due to the current studyâ€™s bail decision-making context. For instance, those wrongfully convicted do not receive any compensation for years spent in prison in at least 21 US states [88]. Hence, people might not believe that compensation is needed or deserved, or attribute this notion of responsibility to other entities, such as the court or the government, leading to a lower ascription of liability to the advisor or decision-maker.\nOur findings indicate that participants who were presented with responsibility statements addressing the violation or protection of a defendantâ€™s rights (e.g., â€œIt is the AI programâ€™s task to protect the rights of the defendantâ€) were assigned higher responsibility levels across all notions. We posit that this effect results from the control that judges (humans and AIs) have over the consequences of their advice and decisions. Although a judgeâ€™s decision can directly affect a defendantâ€™s rights depending on the appropriateness of oneâ€™s jailing, preventing re-offenses is a complex task that encompasses diverse factors, such as policing and the defendantâ€™s decision to re-offend."},{"header":"5.2 Humans Are More Responsible for Their Tasks Than AI Programs","body":"Participants perceived human judges and advisors as more responsible for their tasks than their AI counterparts (see the leftmost bars in Figure 3). Humans are responsible for the tasks they are assigned, e.g., preventing re-offenses because they are in charge (i.e., authority) and have the skills necessary for completing them (i.e., power). These agents should either oversee (i.e., obligation) these\ntasks or take the lead (i.e., task). On the other hand, AI systems are ascribed lower levels of all these responsibility notions.\nThemeanings of responsibility addressing the attribution of tasks and their requirements are descriptive in the sense that they should be addressed in the present tense [27], e.g., one is responsible for a task, or is in charge of it. Although descriptive and present-looking, these notions lead to the prescription of forward-looking responsibilities, such as an obligation. For instance, to be responsible for a specific task because one has the authority and necessary skills prescribes that one should see to it that the task is completed, i.e., an obligation is prescribed, through consequentialist, deontological, or virtue-based routes [102].\nParticipants attributed lower levels of authority and power to AI. This indicates that these systems are not thought to possess the necessary abilities to make decisions and advise such high-stakes decisions. Therefore, it is not deemed the AI programâ€™s responsibility to complete the assigned task or see it to be fulfilled."},{"header":"5.3 The Need for Explanations","body":"One of the prominent findings of this work is the need of interpretable AI systems. Although our participants assign a marginally lower level of responsibility-as-answerability for AI advisors visÃ -vis their human counterparts (Î”=0.167, ğ‘<.05), they believe they should justify their decisions to the same extent as human judges, particularly if they are to make the final bail decision (ğ‘>.05).\nMoreover, our results suggest that an AI without a human-inthe-loop, i.e., AI judges in Study 2, could be held at the same level of scrutiny as human decision-makers for their decisions. This finding may imply that deploying black box AI in high-stakes scenarios, such as bail decision-making, will not be perceived well by the public. There exists empirical evidence that people might be averse to machines making moral decisions [10]. Previous work has not controlled for a systemâ€™s interpretability, and therefore such trends might either i) be caused by the lack of explanations or ii) be aggravated if people become aware that AI systems cannot justify their moral decisions.\nJudges should base their decisions on facts and be able to explain why they made such decisions. AI systems should also be capable of justifying their advice and decision-making process according to our results. This finding demonstrates the significance of these systemsâ€™ interpretability. Scholars have discussed the risks posed by the opacity of existing AI algorithms. They argue that understanding how these systems come to their conclusions is necessary for both safe deployment and wide adoption [36]. Explainable AI (XAI) [46] is a field of computer science that has been given much attention in the community [38], and our results suggest that people agree with its importance.\nPrevious work has found that oneâ€™s normative and epistemological values influence how explanations are comprehended [64]. Explanations involve both an explainer and explainee, meaning that conflicts might arise concerning how they are evaluated [69]. Therefore, we also posit that future work should delve deeper into what types of explanations the general public expects from AI systems. We highlight that those who are in charge of developing interpretable systems should not try to â€œnudgeâ€ recipients so they can be manipulated [63], e.g., for agency laundering [82]."},{"header":"5.4 AI and Human Agents Are Similarly Responsible for Consequences","body":"The four rightmost bars in Figure 3 suggest that AI and human agents are ascribed similar levels of backward-notions of responsibility, namely blame, liability, praise, and causal responsibility.\n5.4.1 The Relation Between Causality and Blame. Amodel that can explain our blameworthiness results is the Path Model of Blame, which proposes that blame is attributed through nested and sequential judgments of various aspects of the action and its agent [66]. After identifying a norm-violating event, the model states that one judges whether the agent is causally connected to the harmful outcome. If this causal evaluation is not successful, the model assigns little or no blame to the agent. Otherwise, the blamer evaluates the agentâ€™s intentionality. If the action is deemed intentional, the blamer evaluates the reasons behind it and ascribes blame accordingly. For unintentional actions, however, one evaluates whether the agent should have prevented the norm-violating event (i.e., had an obligation to prevent it) and could have done so (i.e., had the skills necessary), hence blaming the agent depending on the evaluation of these notions.\nOur results from both studies show that AI and human agents are blamed to a similar degree. These findings agree with the Path Model of Blame, which proposes causality as the initial step for blame mitigation. The model proposes that one can mitigate blame by i) challenging oneâ€™s causal connection to the wrongful action or ii) defending that it does not meet moral eligibility standards. We posit that the first excuse can explain why people blame human and AI advisors and decision-makers similarly. As their causal connection to the consequence is deemed alike, they are attributed to similar blame levels. Challenging oneâ€™s causal effect in an outcome has also been discussed as a possible excuse to avoid blame by other scholars [101].\n5.4.2 Praise in AI-Assisted Bail Decisions. The extent to which praise was assigned to human and AI agents varied depending on whether onewas an advisor or a decision-maker. Even though Study 1 shows no difference between the two (ğ‘>.05), human decisionmakers were more highly praised than AIs in Study 2 (Î”=0.461, ğ‘<.001). Previous work has proposed praise as a positive reinforcement [51] and a method through which one might convey information about its values and expectations to the praisee [28].\nRegarding the difference between advisors and decision-makers, we posit that the differences between human agents are caused by the level of control the latter has over its decision outcomes. Although an advisor influences the final decision, the judge is the one who acts on it and, hence, deserves praise. Moreover, taking praise as positive reinforcement, praising the decision-maker over an advisor might have a bigger influence over future outcomes.\nHowever, our results also indicate that AI decision-makers are not praised to the same level as human judges. Taking praise as a method of conveying social expectations and values, we highlight that people might perceive existing praising practices as inappropriate for AI. Similarly to the arguments against holding AI responsible per se, focusing on the fact that they do not have mental states required for existing responsibility practices [89, 96], praising an AI might lose its meaning if done as if it were towards humans.\nThe same argument could also be applied to the practice of blame [26]. If the general public believes praising an AI system does not make sense, people might perceive blameworthiness similarly, contradicting our results. However, studies have shown a public impulse to blame, driven by the desire to express social values and expectations [18]. Psychological evidence further suggests that humans are innate retributivists [17]. Likewise, HCI research has found that people attribute blame to robotic agents upon harm, particularly if they are described to be autonomous and serve the main cause of harm [37, 52, 67]. Hence, there is no contradiction in people attributing blame to AI systems for harms, although they should not be praised for opposing consequences.\n5.4.3 Liability as Compensation. Our findings indicating that AI and human agents should be held liable to a similar level goes against previous work, which has found that people attribute punishment to AI systems to a lesser degree than their human counterparts [62]. Punishment fulfills many societal goals, such as making victims whole, the satisfaction of retributive feelings, and offendersâ€™ reform. In the current study, we address one of these functions and phrase liability as the responsibility to compensate those harmed (i.e., make victims whole). Therefore, our results do not directly contradict earlier findings that had addressed punishment in its wide definition.\nThe results from our initial exploratory analysis in Section 4.1 show that trends found between causality and blame attributions across different phrasing styles do not directly transfer to liability judgments. Hence, we do not posit that similar causality judgments can explain the similar attribution of liability to AI and humans as in the case of blame. Still, we instead hypothesize that it results from two different factors based on our phrasing styles.\nRegarding the statements addressing the prevention of re-offenses, we posit that the lower attribution of liability to both agents is caused by a variation of the â€œproblem of many hands.â€ [102] Preventing defendants from re-offending does not rely solely on a judgeâ€™s decision but encompasses many other factors as discussed above. Therefore, liability is distributed across various entities, such as the government and the court per se. Addressing the statements focusing on protecting defendantsâ€™ rights, we hypothesize that people do not expect defendants to be compensated if their rights are violated. As examined above, much of the US legislature does not compensate those who have been unjustly incarcerated [88]. The respondents did not believe those harmed should, or even could, be made whole for the violation of their rights, and hence, both AI and human agents are attributed low and similar levels of liability."},{"header":"6 IMPLICATIONS","body":"Our findings indicate that people believe humans are, and should be, responsible for the assigned tasks, regardless of whether they are advisors or decision-makers. Our respondents perceive humans as having the skills necessary to complete these tasks, being in charge of them, and being able to ensure that they are completed. The responsibility notions that were attributed to human agents to a greater extent than to AIs are present- and forward-looking in the sense that they are descriptive, i.e., by stating a fact, and prescribe obligations. It is important to note that users of AI systems are also responsible in a backward-looking fashion such that they\nshould also be held responsible for the outcomes of their advice and decisions. Therefore, our findings agree with scholars who propose that users (and designers) should take responsibility for their automated systemsâ€™ actions and consequences [20, 72].\nNonetheless, our study shows that AIs could also be held responsible for their actions. Taking morality as a human-made construct [93], it may be inevitable to hold AI systems responsible alongside their users and designers so that this formulation is kept intact. Viewing responsibility concepts as social constructs that aim to achieve specific social goals, attributing backward-looking notions of responsibility to AI systems might emphasize these goals [91]. Our study indicates these practices might not need to focus on compensating those harmed by these systems given the low attribution of liability to all agents.3 We instead hypothesize that people might desire to hold these entities responsible for retributive motives, such as satisfying their needs for revenge [71] and bridging the retribution gap [26], as a result of human nature [25]. It is important to note that AI systems might not be appropriate subjects of (retributive) blame [26, 89], i.e., scholars argue that blaming automated agents would be wrong and unsuccessful. Future research can address which functions of responsibility attribution would satisfy this public attribution of backward-looking responsibilities to AI systems. Future studies can also address scenarios in which blame could be attributed to a higher degree, e.g., those with life-or-death consequences, such as self-driving vehicles and AI medical advisors.\nA common concern raised by scholarly work is that blaming or punishing an AI system might lead to social disruptions. From a legal perspective, attributing responsibility to these systemsmight obfuscate designers and usersâ€™ roles, creating human liability shields [16], i.e., stakeholders might use these automated systems as a form of protecting themselves from deserved punishment. Another possible issue is â€œagency laundering,â€ in which the systemsâ€™ designer distances itself frommorally suspect actions, regardless of intentionality, by blaming the algorithm, machine, or system [82]. This form of blame-shifting has been observed, for example, when Facebook called out its algorithm for autonomously creating anti-semitic categories in its advertisement platform [1, 97]. We highlight that any responsibility practice towards AI systems should not blur the responsibility prescribed and deserved by their designers and users. Our findings suggest that autonomous algorithms alone should not be held responsible by themselves, but rather alongside other stakeholders, so these concerns are not realized."},{"header":"7 CONCLUDING REMARKS","body":"This paper discussed the responsibility gap posed by the deployment of autonomous AI systems [68] and conducted a survey study to understand how differently people attribute responsibility to AI and humans. As a case study, we adapted vignettes from real-life algorithm-assisted bail decisions and employed a within-subjects experimental design to obtain public perceptions on various notions of moral responsibility. We conducted two studies; the former illustrated a realistic scenario in which AI advises human judges,\n3This finding does not imply that those harmed should not be compensated, but rather that respondents do not attribute this responsibility to AI systems per se. Some scholars propose that other stakeholders should take this responsibility [19], mainly because automated agents are not capable of doing so [16].\nand the latter described a fictional circumstance where AI is the decision-maker itself.\nThe current study focused on AI systems currently being used to advise bailing decisions, which is an important yet specific application of these algorithms. Therefore, our results might not be generalizable to all possible environments. For instance, some of our results partly conflict with previous work addressing self-driving vehicles [7] and medical systems [62]. Studies such as ours should be expanded to diverse AI applications, where they are used both in-the-loop (as in Study 1) and autonomously (as in Study 2). People have different opinions regarding how (and where) these systems should be deployed in relation to how autonomous they should be [65], which should affect how they ascribe responsibility for their actions.\nStudy 1 was designed so that the judgeâ€™s decision always followed the advice given to reduce complexity in the vignette design. However, future studies on similar topics should also consider scenarios in which AI systems and their human supervisors disagree. For instance, if a human judge chooses to disagree with advice, some of the advisorâ€™s responsibilities might be shifted towards the decision-maker regardless of the advisorâ€™s nature. In our case study, human-AI collaborations are such that there exists an AI-in-theloop; future work should address other collaboration variations, such as human-in-the-loop, i.e., humans assisting machines.\nThe current research considered eight notions of responsibility from related work. We recognize that other meanings of responsibility could be further considered, such as virtue-based notions where one might call an entity responsible in that it prescribes an evaluation of oneâ€™s traits and dispositions [87, 94]. These notions have been widely agreed upon as incompatible with AI systems due to their lack of metaphysical attributes [20, 89, 96]. Nevertheless, our research has found key clusters of responsibility notions concerning AI and human agents, opening further research directions.\nOur exploratory analysis identified two clusters of responsibility notions. One cluster encompasses meanings related to the attribution of tasks and obligations (i.e., responsibility-as-task, asobligation), their necessary skills (i.e., responsibility-as-power), and the ascription of authority (i.e., responsibility-as-authority). The other cluster includes meanings related to causal determinations (i.e., responsibility-as-cause) and backward-looking responsibility notions (i.e., blame, praise, and liability).\nAs our results demonstrate, people may hold AI to a similar level of moral scrutiny as humans for their actions and harms. Our respondents indicate that they expect decision-makers and advisors to justify their bailing decisions regardless of their nature. Our findings highlight the importance of interpretable and explainable algorithms, particularly in high-stakes scenarios, such as our casestudy. Finally, this study also showed that people judge AI and humans differently with respect to certain notions of responsibility, particularly those addressing present- and forward-looking meanings, such as responsibility-as-task and as-obligation. However, we have also found that people attribute similar levels of causal responsibility, blame, and liability to AI and human advisors and decision-makers for bail decisions."},{"header":"ACKNOWLEDGMENTS","body":"This work was supported by the Institute for Basic Science (IBSR029-C2).\nA APPENDIX"}],"type":"Sections"}