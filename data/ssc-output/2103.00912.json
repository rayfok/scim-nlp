{
  "2103.00912": [
    {
      "sentence": "In contrast, a complex gesture may be represented as an intricate path that may meander across the map.",
      "label": "Background",
      "prob": 0.9487094879150391
    },
    {
      "sentence": "So far, elicitation has focused on observed gestures, yet it might also be relevant to examine why behavior was not observed.",
      "label": "Background",
      "prob": 0.9393789768218994
    },
    {
      "sentence": "For instance, such a map might reveal which gestures and poses are most common or intensely studied.",
      "label": "Background",
      "prob": 0.9352055788040161
    },
    {
      "sentence": "Hand-engineered features [4, 24, 58] may help with the interpretation, however, they may be specific to a sensor and interaction setup.",
      "label": "Background",
      "prob": 0.9341276288032532
    },
    {
      "sentence": "These occur due to recording issues (e.g. sensor occlusion in some frames) or when subsequent poses are embedded far apart in the 2D space.",
      "label": "Background",
      "prob": 0.9315454959869385
    },
    {
      "sentence": "For example, a participants movement can be subtle, in which case the embedded gesture path is simple in shape and typically spans a small region in the gesture map.",
      "label": "Background",
      "prob": 0.9175797700881958
    },
    {
      "sentence": "Integrating such a tree-like layout into the gesture map adds complexity and might be material for future endeavours.",
      "label": "Background",
      "prob": 0.885658860206604
    },
    {
      "sentence": "Thus,bycomparingmultipleembeddedgesturepathsresearchers can visually assess gestures as similar or not.",
      "label": "Background",
      "prob": 0.8811983466148376
    },
    {
      "sentence": "One could also predefine a gesture path to monitor live performances and to judge deviation from this template, possibly to learn/teach a movement sequence.",
      "label": "Background",
      "prob": 0.8766360282897949
    },
    {
      "sentence": "Potentially, for the children the basic shapes afforded less flexible interpretation than a flower or crouching.",
      "label": "Background",
      "prob": 0.8715412616729736
    },
    {
      "sentence": "A fundamentally new capability of GestureMap is that unseen poses or gestures (i.e. not proposed by participants) can be simulated by decoding arbitrary 2D points in the learned space.",
      "label": "Background",
      "prob": 0.8679008483886719
    },
    {
      "sentence": "However, these highly abstract visualizations may occlude the nature of the underlying data.",
      "label": "Background",
      "prob": 0.8677189350128174
    },
    {
      "sentence": "Regions in the gesture map that contain multiple embedded data points from different referents may indicate that this region encodes shared generic behavior.",
      "label": "Background",
      "prob": 0.8619238138198853
    },
    {
      "sentence": "While elicitation studies have become a widely used staple in the HCI toolbox, they still present challenges (cf. [51, 63]), including the need for manual data analysis .",
      "label": "Background",
      "prob": 0.8613055348396301
    },
    {
      "sentence": "For example, it might be interesting to identify which referents share behavior and which are distinctive.",
      "label": "Background",
      "prob": 0.8576399683952332
    },
    {
      "sentence": "However, the resulting computed centroids often deviated from peoples expectations, and thus did not immediately make sense to them.",
      "label": "Background",
      "prob": 0.855877161026001
    },
    {
      "sentence": "This might be useful to adjust elicitation setup/instructions, for example to prompt people to also cover a previously empty part of the map.",
      "label": "Background",
      "prob": 0.8513308167457581
    },
    {
      "sentence": "Copyrights for components of this work owned by others than the author(s) must be honored.",
      "label": "Background",
      "prob": 0.8468316793441772
    },
    {
      "sentence": "The two dimensions of the map do not have a direct predefined meaning yet emerge from elicited data.",
      "label": "Background",
      "prob": 0.8347195982933044
    },
    {
      "sentence": "It might also be interesting to analyze outlier behavior which can be detected by examining scatter points that lie far from these clusters.",
      "label": "Background",
      "prob": 0.8344556093215942
    },
    {
      "sentence": "For example, in the elicitation context, we might be interested in comparing the behavior across different participants and experimental trials.",
      "label": "Background",
      "prob": 0.8268884420394897
    },
    {
      "sentence": "Upon first use, most people immediately animated a few gestures, saying that this was the most natural and familiar way to view the data Since the map visualization was unfamiliar to them, some had initial difficulties to understand the distinction of single poses (points) and entire gestures (paths).",
      "label": "Background",
      "prob": 0.8248803019523621
    },
    {
      "sentence": "The VAE here serves as an exemplar of a model with both powerful (non-linear) encoding and decoding capabilities.",
      "label": "Background",
      "prob": 0.8228567242622375
    },
    {
      "sentence": "After the initial data exploration it is often necessary to find concrete example for detected patterns.",
      "label": "Background",
      "prob": 0.8223602771759033
    },
    {
      "sentence": "However, there were some complex paths (e.g. crossing over many poses on the map) that people were unable to assign to a group.",
      "label": "Background",
      "prob": 0.8139544725418091
    },
    {
      "sentence": "The former targets questions that may span multiple referents or the entire dataset, while the latter focuses on a few gestures to identify specific behavioral idiosyncrasies.",
      "label": "Background",
      "prob": 0.8112993240356445
    },
    {
      "sentence": "Some additionally employ a sensor in elicitation (e.g. Leap [62], Kinect [56]), thus also potentially considering the senseable space .",
      "label": "Background",
      "prob": 0.8088038563728333
    },
    {
      "sentence": "GestureMap is fundamentally motivated by providing researchers with a visual overview of the elicited gesture space.",
      "label": "Background",
      "prob": 0.8068665266036987
    },
    {
      "sentence": "A difficulty with k-means is setting the number of clusters.",
      "label": "Background",
      "prob": 0.8065018653869629
    },
    {
      "sentence": "For example, these observations can inform researchers interested in building gesture recognizers in judging the difficulty of separating gestures for the various referents.",
      "label": "Background",
      "prob": 0.8055770993232727
    },
    {
      "sentence": "Scatter plots may contain too much detail and clutter the visualization.",
      "label": "Background",
      "prob": 0.8048622012138367
    },
    {
      "sentence": "If researchers animate a gesture, it is simultaneously animated in this view and on the map.",
      "label": "Background",
      "prob": 0.7979831695556641
    },
    {
      "sentence": "Scatter points may visually cluster near gesture poses that are characteristic for a particular referent.",
      "label": "Background",
      "prob": 0.7975893616676331
    },
    {
      "sentence": "Separate maps could also compare gesture spaces for different contexts, devices, etc., for example, to better understand the influences of such factors.",
      "label": "Background",
      "prob": 0.7847081422805786
    },
    {
      "sentence": "Thus, while these measures set standards on how to compute consensus from gesture proposal, they cannot avoid subjectivity per se.",
      "label": "Background",
      "prob": 0.7663143277168274
    },
    {
      "sentence": "This view lists all referents and gesture proposals in a compact way as numbers for quick reference and selection.",
      "label": "Background",
      "prob": 0.7633414268493652
    },
    {
      "sentence": "GestureMap is already implemented as a web-based tool, rendering it flexible and open to such integration.",
      "label": "Background",
      "prob": 0.7592709064483643
    },
    {
      "sentence": "These could be used also with our interactive clustering, for example, by plugging in the cluster cardinalities instead of subjective gesture group counts.",
      "label": "Background",
      "prob": 0.7521141171455383
    },
    {
      "sentence": "GestureMap addresses these needs as its 2D map shows observed gesture proposals and gives an idea of past behavior.",
      "label": "Background",
      "prob": 0.7504030466079712
    },
    {
      "sentence": "Intuitively, for example, a high value VAR  may inform an analyst that referent  contains quite varied gesture proposals (i.e. low consensus).",
      "label": "Background",
      "prob": 0.7496026158332825
    },
    {
      "sentence": "These reassignments, however, were not yet considered when rerunning the clustering algorithm in the current implementation.",
      "label": "Background",
      "prob": 0.7464646100997925
    },
    {
      "sentence": "The first of many analysis steps often involves developing an overview of the data to understand its underlying properties: Researchers here often use statistical plots to summarize the data and to identify broad patterns.",
      "label": "Background",
      "prob": 0.7426920533180237
    },
    {
      "sentence": "While some models address this (e.g. we used a VAE instead of AE), there is no universal natural 2D layout of body poses and some artifacts are likely to exist for most models and datasets.",
      "label": "Background",
      "prob": 0.7355559468269348
    },
    {
      "sentence": "For a typical elicitation study, such as this one by Vatavu [59], it is reasonable to expect clusters induced by the referents.",
      "label": "Background",
      "prob": 0.7346646189689636
    },
    {
      "sentence": "Expert users especially liked the visual expressiveness of GestureMap , as it quickly summarizes the underlying dataset.",
      "label": "Background",
      "prob": 0.7316936254501343
    },
    {
      "sentence": "In contrast, it did not separately find left hand and kicking, presumably since those were proposed only a few times.",
      "label": "Background",
      "prob": 0.7287719249725342
    },
    {
      "sentence": "[1] proposed a crowd platform for annotation, yet without computational support for the workers, such as alternative gesture representations or similarity measures.",
      "label": "Background",
      "prob": 0.7257596850395203
    },
    {
      "sentence": "GestureMap could be extended to define new gestures: For example, users could draw a gesture as a path on the map.",
      "label": "Background",
      "prob": 0.7217074632644653
    },
    {
      "sentence": "Thus, since gestures are sequences of poses, they are paths connecting multiple points on the map.",
      "label": "Background",
      "prob": 0.7163932919502258
    },
    {
      "sentence": "Technically, this can be readily implemented by initialising k-means with the current (refined) assignments.",
      "label": "Background",
      "prob": 0.7152486443519592
    },
    {
      "sentence": "They therefore modified elicitation such that people could choose from a predefined list of gesture proposals.",
      "label": "Background",
      "prob": 0.7125622630119324
    },
    {
      "sentence": "They stopped to examine gestures in more detail that differed largely from the shapes seen so far.",
      "label": "Background",
      "prob": 0.7114447355270386
    },
    {
      "sentence": "Another researcher felt that the map should show more detail so it would be easier to judge differences and transitions of poses.",
      "label": "Background",
      "prob": 0.7049029469490051
    },
    {
      "sentence": "[63] called for future work to include multiple representations of gestures.",
      "label": "Background",
      "prob": 0.7042439579963684
    },
    {
      "sentence": "Some found similar poses encoded in different map regions and noted that these should ideally reside in one area.",
      "label": "Background",
      "prob": 0.7014591097831726
    },
    {
      "sentence": "The map supports pan and zoom and accordingly recomputes the grid of landmarks (small skeletons).",
      "label": "Background",
      "prob": 0.698449969291687
    },
    {
      "sentence": "A key challenge in visual analytics is the effective visualization of high-dimensional data.",
      "label": "Background",
      "prob": 0.6975422501564026
    },
    {
      "sentence": "Analysts can examine if empty regions are anatomically not feasible (cf. 8.3.3) or if people did not show such behaviour.",
      "label": "Background",
      "prob": 0.6970677375793457
    },
    {
      "sentence": "Instead, GestureMap could be used to show gestures to users, allowing them to reenact and explore them with live monitoring via the map.",
      "label": "Background",
      "prob": 0.6959773302078247
    },
    {
      "sentence": "inquired into what people liked/disliked about GestureMap , and asked for ideas for improvements and additional features.",
      "label": "Background",
      "prob": 0.6953395009040833
    },
    {
      "sentence": "In real use, researchers would conduct such analyses to better understand elicited data.",
      "label": "Background",
      "prob": 0.690546452999115
    },
    {
      "sentence": "The gesture paths visit roughly similar main parts of the gesture space, yet the children do not find consensus.",
      "label": "Background",
      "prob": 0.6878363490104675
    },
    {
      "sentence": "Now, all participants specifically searched for individual gesture proposals as templates (strategy 2) and used those to initialize the algorithm.",
      "label": "Background",
      "prob": 0.6790635585784912
    },
    {
      "sentence": "GestureMap enables this: Researchers can explore map areas without data, which may reveal unlikely behavior, or indicate issues with interaction (e.g. anatomically difficult or tiring gestures) or the sensor (e.g. gestures leading to self-occlusion of body parts).",
      "label": "Background",
      "prob": 0.6732034683227539
    },
    {
      "sentence": "The key local observation in elicitation data is to examine individual gesture proposals .",
      "label": "Background",
      "prob": 0.6677907109260559
    },
    {
      "sentence": "As an exploratory tool, GestureMaps learned space is applicable to new and changing setups, without developing hand-engineered features first.",
      "label": "Background",
      "prob": 0.6675525307655334
    },
    {
      "sentence": "The frontend and backend modules communicate through a REST API through which the data is transmitted as a JSON formatted string.",
      "label": "Background",
      "prob": 0.6661408543586731
    },
    {
      "sentence": "In our prototype users can thus hover over the map to visualize 3D skeletons for any cursor location.",
      "label": "Background",
      "prob": 0.6641150116920471
    },
    {
      "sentence": "This allows researchers to view details on-demand e.g. to reduce the risk of information overload.",
      "label": "Background",
      "prob": 0.6635580658912659
    },
    {
      "sentence": "In a sense, they searched for outlier behavior based on the path shapes.",
      "label": "Background",
      "prob": 0.6619464755058289
    },
    {
      "sentence": "space of human behaviour (e.g. comfortable motion ranges of arm and hand), and 2) the space of senseable input in a system or context (e.g. tracking of up to X body joints in 3D).",
      "label": "Background",
      "prob": 0.6615847945213318
    },
    {
      "sentence": "Researchers can use it to detect overlapping or distinctive behavior across different referents.",
      "label": "Background",
      "prob": 0.6601327657699585
    },
    {
      "sentence": "Central to gesture elicitation studies is an in-depth analysis of the proposed data to find common behavior.",
      "label": "Background",
      "prob": 0.6592532992362976
    },
    {
      "sentence": "As an alternative to drawing, users could demonstrate the gesture in front of the sensor, with a cursor moving on the map live.",
      "label": "Background",
      "prob": 0.655504047870636
    },
    {
      "sentence": "These clusters can help researchers to form a mental model of the main poses that are characteristic for a group of gesture sequences.",
      "label": "Background",
      "prob": 0.6518129110336304
    },
    {
      "sentence": "To motivate a concrete example, citetJain2016 showed that observers can distinguish behavior of children and adults.",
      "label": "Background",
      "prob": 0.6464892029762268
    },
    {
      "sentence": "Vatavu [59] were the first to propose a data-driven consensus measure that does not rely on human judgement of gesture similarity.",
      "label": "Background",
      "prob": 0.6390749216079712
    },
    {
      "sentence": "Some additionally jumped at the end of their gesture proposals to get back onto their feet.",
      "label": "Background",
      "prob": 0.6308704614639282
    },
    {
      "sentence": "In addition, interactive hierarchical clustering would eliminate the need for choosing the number of clusters beforehand.",
      "label": "Background",
      "prob": 0.630668580532074
    },
    {
      "sentence": "Since the underlying latent variable model can simulate new behavior (decoding), such a drawn path implicitly defines a pose sequence that could be exported as a template-based gesture recogniser.",
      "label": "Background",
      "prob": 0.6304513216018677
    },
    {
      "sentence": "Seeing this and related work as a toolbox, researchers may now consider various options: For example, AGATE 2.0 [61] is a highly specialized tool to compute agreement, which assumes a given labeled dataset.",
      "label": "Background",
      "prob": 0.6292672753334045
    },
    {
      "sentence": "A smooth latent space facilitates suitable visualization by reducing jumps in gesture paths.",
      "label": "Background",
      "prob": 0.6207273602485657
    },
    {
      "sentence": "Researchers can zoom, pan, and hover over the gesture map, and overlay a scatter plot or a density plot (e.g. Figure 1c) to explore individual or multiple gesture poses.",
      "label": "Background",
      "prob": 0.6185068488121033
    },
    {
      "sentence": "Thus, it seems to contain a more diverse set of body poses.",
      "label": "Background",
      "prob": 0.6170458197593689
    },
    {
      "sentence": "An efficient analysis becomes even more important as large-scale gesture data sets can be collected online, for example, through cloud elicitation tools [2].",
      "label": "Background",
      "prob": 0.6155272126197815
    },
    {
      "sentence": "This limits elicitation studies, as well as the general endeavor of systematically exploring behavioursensor spaces in HCI, as characterised in the following paragraphs:",
      "label": "Background",
      "prob": 0.6140224933624268
    },
    {
      "sentence": "Although central to HCI, the field has developed few dedicated methods and tools for supporting the (joint) exploration of such user-sensor spaces (cf. [65]).",
      "label": "Background",
      "prob": 0.6108878254890442
    },
    {
      "sentence": "This feature helps to adjust the viewport to support exploration of datadense areas, and deal with the fact that landmark representations are discrete indicators for the continuous space.",
      "label": "Background",
      "prob": 0.6068832874298096
    },
    {
      "sentence": "[24] which also focuses on the analysis of gesture elicitation studies.",
      "label": "Background",
      "prob": 0.5926734209060669
    },
    {
      "sentence": "While the concepts introduced in this paper also enable researchers to better annotate sequences, our focus lies in particular on the exploration of elicited gesture data.",
      "label": "Background",
      "prob": 0.5859524011611938
    },
    {
      "sentence": "Concretely, the 3D skeleton view updates while the user moves the cursor over the 2D map to present the posture at that point in the gesture space.",
      "label": "Background",
      "prob": 0.5855236649513245
    },
    {
      "sentence": "Scatter points on top of the pose grid enable researchers to quickly identify which general poses were observed in the data.",
      "label": "Background",
      "prob": 0.5845122933387756
    },
    {
      "sentence": "The idea of clustering gesture elicitation data is motivated by two aspects:",
      "label": "Background",
      "prob": 0.5845074653625488
    },
    {
      "sentence": "The progress of the animation can be controlled via a play/pause button and slider.",
      "label": "Background",
      "prob": 0.5797955393791199
    },
    {
      "sentence": "As another such example, for throw ball , behavior can be categorized into four clusters: Most children used their right hand, others used two hands, and some kicked the ball.",
      "label": "Background",
      "prob": 0.5776860117912292
    },
    {
      "sentence": "It provides an overview of the gesture data and introduces a new continuous traceable 2D paths which represent gesture sequences.",
      "label": "Background",
      "prob": 0.5748072862625122
    },
    {
      "sentence": "Such a recognizer then also could be used in other tools to support sensor feed annotation (e.g. [41]).",
      "label": "Background",
      "prob": 0.573100209236145
    },
    {
      "sentence": "One user noted that one still has to inspect all gesture proposals in order to choose suitable initialisations for the k-means algorithm.",
      "label": "Background",
      "prob": 0.5662944912910461
    },
    {
      "sentence": "[19], we used a weight term to modulate the mix of KL-loss and reconstruction loss in early training.",
      "label": "Background",
      "prob": 0.5642474293708801
    },
    {
      "sentence": "Given the exploratory nature of the interactions and the diversity in peoples approaches this was done in an inductive approach, leading to the themes in Section 7.2.",
      "label": "Background",
      "prob": 0.563430905342102
    },
    {
      "sentence": "In this way, elicitation studies inform gestural interaction with user-driven exploration: Most studies focus on the human behaviour space and thus do not rely on a specific sensor; they typically video-record participants for manual gesture analysis (e.g. [14, 28]).",
      "label": "Background",
      "prob": 0.5612203478813171
    },
    {
      "sentence": "In contrast, a hierarchical treemap does not directly fit the map metaphor well.",
      "label": "Background",
      "prob": 0.5601934790611267
    },
    {
      "sentence": "This gesture map is a 2D plot with a grid of representative body poses shown as small human skeletons.",
      "label": "Background",
      "prob": 0.5506227612495422
    },
    {
      "sentence": "Scatter or density plots can be projected onto the map (e.g. Figure 1b 1  and Figure 1c).",
      "label": "Background",
      "prob": 0.5483476519584656
    },
    {
      "sentence": "Such support as shown in GestureMap could be combined with a crowd approach in the future.",
      "label": "Background",
      "prob": 0.5467376112937927
    },
    {
      "sentence": "In contrast, for instance, gestures proposed for crouch cover a different region (pink).",
      "label": "Background",
      "prob": 0.5452002882957458
    },
    {
      "sentence": "The analysis concept is structured further by differentiating between global observations and local observations.",
      "label": "Background",
      "prob": 0.5426563620567322
    },
    {
      "sentence": "GestureMap s concepts support handling large data, visually summarised and explored via our map view.",
      "label": "Background",
      "prob": 0.5420036315917969
    },
    {
      "sentence": "2) In an exploratory, manual analysis task people were prompted to use GestureMap to identify groups of behaviors in the gesture proposals for two referents.",
      "label": "Background",
      "prob": 0.5377793312072754
    },
    {
      "sentence": "To address this, Vatavu [59] has recently proposed a new, datadriven approach: It employs a distance measure as an objective basis for assessing consensus in elicitation studies.",
      "label": "Background",
      "prob": 0.5339697599411011
    },
    {
      "sentence": "Some people noted that they struggled to find a specific pose on the map.",
      "label": "Background",
      "prob": 0.5320863723754883
    },
    {
      "sentence": "GestureMap also offers this, to afford easy examination of a recorded gesture.",
      "label": "Background",
      "prob": 0.5257766246795654
    },
    {
      "sentence": "We motivate this choice by interpretability of the resulting centroids, versus the abstract representations in the hierarchical approach: In particular, the centroids (i.e. average/barycenter gestures) are more compatible with our 2D gesture map, on which they could be displayed as paths.",
      "label": "Background",
      "prob": 0.5174036026000977
    },
    {
      "sentence": "To find behavioral patterns, it employs a variation of the smallmultiples plot [52] and an interactive hierarchical clustering interface visualized in a tree layout.",
      "label": "Background",
      "prob": 0.5159532427787781
    },
    {
      "sentence": "To determine consensus for a referent they calculated the pairwise distances across all gesture proposals for this referent.",
      "label": "Background",
      "prob": 0.515209436416626
    },
    {
      "sentence": "Related, gesture sets are mostly presented as drawings and videos today [37].",
      "label": "Background",
      "prob": 0.5148009061813354
    },
    {
      "sentence": "Using expectations about possible behavior for a gesture proposal (e.g. left vs right hand throwing), they examined scatter points in those map regions that based on the landmark skeletons encoded related poses.",
      "label": "Background",
      "prob": 0.5127456188201904
    },
    {
      "sentence": "This section briefly describes gesture elicitation studies, followed by an overview of tools that support researchers across different tasks involved in analyzing elicitation data.",
      "label": "Background",
      "prob": 0.5126542448997498
    },
    {
      "sentence": "In real use, researchers might export this result, for example, for a report, calculations of agreement, etc.",
      "label": "Background",
      "prob": 0.5095052123069763
    },
    {
      "sentence": "GestureMap supports this as well: For instance, Figure 3 depicts a scatter plot projected on the gesture map.",
      "label": "Background",
      "prob": 0.508496105670929
    },
    {
      "sentence": "Furthermore, our learned representation supports gesture simulation useful to examine regions of the behavior space that were not covered by participants.",
      "label": "Background",
      "prob": 0.5080881714820862
    },
    {
      "sentence": "The extensibility of GestureMap further encourages future work to employ machine learning as a tool for analysis of human behavior.",
      "label": "Background",
      "prob": 0.5072886347770691
    },
    {
      "sentence": "Each scatter point corresponds to a pose from the dataset, whereas empty patches in the gesture map may indicate behavior that has not been observed (e.g. poses/gestures not proposed by participants during elicitation).",
      "label": "Background",
      "prob": 0.5066219568252563
    },
    {
      "sentence": "We introduce the concept of an average gesture sequence as a new computational capability in the context of gesture elicitation.",
      "label": "Background",
      "prob": 0.5062984228134155
    },
    {
      "sentence": "The third visualization is similar to the second, but additionally employs a heat-map to emphasize the time domain.",
      "label": "Background",
      "prob": 0.5047737956047058
    },
    {
      "sentence": "Designing effective interactions and user interfaces often involves exploring two potentially high-dimensional spaces [65]: 1) The",
      "label": "Background",
      "prob": 0.49375027418136597
    },
    {
      "sentence": "gestures on the map, labelled manually or with help from our clustering tool, to train a classifier.",
      "label": "Background",
      "prob": 0.49323052167892456
    },
    {
      "sentence": "Overall, after being asked to give a final verdict over the interactive clustering feature, all deemed it important.",
      "label": "Background",
      "prob": 0.49251788854599
    },
    {
      "sentence": "This live embedding provides a monitoring tool, for example, for participants to see their currently performed gesture (e.g. shown as a cursor/point on the map), possibly to nudge them towards exploring new regions of the behavior space (cf. [65]).",
      "label": "Background",
      "prob": 0.49106931686401367
    },
    {
      "sentence": "Regardless of their initial analysis strategy, when asked which feature they would use to group the",
      "label": "Background",
      "prob": 0.4905780553817749
    },
    {
      "sentence": "For throw ball , everyone found at least three (left/right/both handed throwing).",
      "label": "Background",
      "prob": 0.49052169919013977
    },
    {
      "sentence": "gestures, people agreed on the path shapes as primary discerning feature (strategy 1).",
      "label": "Background",
      "prob": 0.4894809424877167
    },
    {
      "sentence": "The interviews lasted 80 minutes and were conducted via screensharing using Skype/Zoom, with GestureMap hosted online such that people could use it on their own computer.",
      "label": "Background",
      "prob": 0.48787030577659607
    },
    {
      "sentence": "Proposals for crouch form two main clusters (pink points in Figure 3 left), one in the region of starting poses, another in sitting/crouching regions.",
      "label": "Background",
      "prob": 0.4856497645378113
    },
    {
      "sentence": "Complementary to the feature for postures, GestureMap accounts for the temporal nature of gesture data [24, 41] by offering linked animations of gesture paths (point moving on the path) and 3D skeletons (skeleton moving).",
      "label": "Background",
      "prob": 0.4779353141784668
    },
    {
      "sentence": "On the positive side, the researchers liked the refinement step, where they could reassign proposals to another cluster.",
      "label": "Background",
      "prob": 0.477340430021286
    },
    {
      "sentence": "GestureMap could be extended to more than post-hoc analysis: For example, we could embed live sensor data and continuously update the underlying mode.",
      "label": "Background",
      "prob": 0.4767807424068451
    },
    {
      "sentence": "2) motivated in calls for further improvements, and",
      "label": "Background",
      "prob": 0.4764689803123474
    },
    {
      "sentence": "These pose landmarks give an overview of the poses in the corresponding rectangular map region (Figure 1b 1  ).",
      "label": "Background",
      "prob": 0.47624483704566956
    },
    {
      "sentence": "When asked why they keep returning to the scatter plot, they said that it provided more detail and that density can also be estimated from scatter points.",
      "label": "Background",
      "prob": 0.4758124053478241
    },
    {
      "sentence": "Besides technical model improvements, visualization concepts could be explored to address this as well (e.g. visually mark jumps along the gesture path).",
      "label": "Background",
      "prob": 0.47312235832214355
    },
    {
      "sentence": "GestureMap could be used to label data and export it for analysis in tools like this.",
      "label": "Background",
      "prob": 0.4715951383113861
    },
    {
      "sentence": "1) We introduced GestureMap (20 minutes), with a concept presentation, a guided walk through the tool and UI, and opportunities for questions.",
      "label": "Background",
      "prob": 0.4712296724319458
    },
    {
      "sentence": "In this way, GestureMap supports the diagnosis of challenges and limitations in the joint user-sensor space of an interactive system (cf. [65]).",
      "label": "Background",
      "prob": 0.4705822169780731
    },
    {
      "sentence": "Centroids can be animated and once the clusters have been computed, users can toggle all gesture proposals that were assigned to a centroid.",
      "label": "Background",
      "prob": 0.45639511942863464
    },
    {
      "sentence": "We thus conceptualized the gesture map to enable researchers to seamlessly cycle between the detection of new observations and the assessment of supporting evidence.",
      "label": "Background",
      "prob": 0.45352113246917725
    },
    {
      "sentence": "We motivate the conceptual features via related work as summarized in Table 1 and elaborate on them in the following sections.",
      "label": "Background",
      "prob": 0.4509273171424866
    },
    {
      "sentence": "This dialog is unfolded with a button in Figure 1b 3  and lets users interactively cluster gesture proposals for a referent.",
      "label": "Background",
      "prob": 0.45068639516830444
    },
    {
      "sentence": "GestureMap realises this by linking the 2D map and the 3D skeleton.",
      "label": "Background",
      "prob": 0.44754934310913086
    },
    {
      "sentence": "Summarising their initial experience, one person said: Although, the learning curve [...] is steep, once you understand the core concepts, this tool offers a great overview of the entire behavior that is captured in the dataset.",
      "label": "Background",
      "prob": 0.4474853575229645
    },
    {
      "sentence": "The gesture map can serve as a common visual basis for such investigations: By projecting multiple gestures onto the map, researchers can evaluate each participants behavior individually.",
      "label": "Background",
      "prob": 0.4419821798801422
    },
    {
      "sentence": "2) Suitably visualising the projected data, considering the analysts tasks and goals.",
      "label": "Background",
      "prob": 0.4403804838657379
    },
    {
      "sentence": "We encouraged them to think out loud and occasionally asked questions to better understand actions.",
      "label": "Background",
      "prob": 0.43958428502082825
    },
    {
      "sentence": "They suggested to increase the visibility of the poses by showing fewer and larger landmarks.",
      "label": "Background",
      "prob": 0.43845266103744507
    },
    {
      "sentence": "The variance plot in GestureMap (Figure 3 right) indicates that proposals for crouch and draw flower vary more than for draw circle and draw square .",
      "label": "Background",
      "prob": 0.4373473823070526
    },
    {
      "sentence": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "label": "Other",
      "prob": 0.4592511057853699
    },
    {
      "sentence": "To the best of our knowledge, GestureMap is the first tool to use a latent variable model to analyze sensor-based motion data in the context of gesture elicitation studies.",
      "label": "Background",
      "prob": 0.43071797490119934
    },
    {
      "sentence": "Human-centered computing  Visualization systems and tools ; HCI design and evaluation methods .",
      "label": "Background",
      "prob": 0.43046802282333374
    },
    {
      "sentence": "[68] confirmed that new users do prefer the user-defined gesture set over the one created by experts.",
      "label": "Result",
      "prob": 0.4231392443180084
    },
    {
      "sentence": "We chose k-means, because it readily integrates with the gesture map and the \"variance around mean gesture\" that we introduced in section 4.2.",
      "label": "Background",
      "prob": 0.42147812247276306
    },
    {
      "sentence": "The gesture variance integrates well with GestureMap s visualization concept because this already displays the involved average gestures as visual elements.",
      "label": "Result",
      "prob": 0.47348475456237793
    },
    {
      "sentence": "We introduce a structured analysis approach based on a learned 2D gesture map, as realised in GestureMap .",
      "label": "Background",
      "prob": 0.4136236310005188
    },
    {
      "sentence": "It further leverages the computation of average gestures to enable researchers to",
      "label": "Background",
      "prob": 0.4077903628349304
    },
    {
      "sentence": "Prior work has extensively used scatter plots to analyze machine learned representations [35, 48].",
      "label": "Background",
      "prob": 0.40446627140045166
    },
    {
      "sentence": "For example, these plots may hide the structure of a 3D skeleton recording.",
      "label": "Result",
      "prob": 0.44078168272972107
    },
    {
      "sentence": "[50] (i.e. 4 hidden layers for both encoder and decoder) and used a 2D bottleneck layer.",
      "label": "Background",
      "prob": 0.40282174944877625
    },
    {
      "sentence": "Considering research interests in the elicitation context from the literature, for example, this might support researchers to examine if a participant can remember and repeat the same gesture proposal across multiple trials [40], or if behavior was influenced by a priming effect [6].",
      "label": "Background",
      "prob": 0.40212151408195496
    },
    {
      "sentence": "Together, this feedback motivates a changeable grid size (our zoom was implemented to always keep an 11  11 grid).",
      "label": "Background",
      "prob": 0.40180838108062744
    },
    {
      "sentence": "Other researchers noted that elicitation findings are spread across multiple venues and need to be consolidated [63].",
      "label": "Other",
      "prob": 0.5094207525253296
    },
    {
      "sentence": "These participants noted that the 2D gesture path visualization offers a quick way to spot irregular behavior and that their analysis becomes an active search versus passively watching every gesture individually.",
      "label": "Result",
      "prob": 0.483518123626709
    },
    {
      "sentence": "The analysis concepts introduced in this work are built on previous work spanning HCI, machine learning and visual analytics.",
      "label": "Background",
      "prob": 0.3974384367465973
    },
    {
      "sentence": "Overall, this indicates the potential of automated clustering, for example, when examining data from open elicitation with no given referents.",
      "label": "Result",
      "prob": 0.49588507413864136
    },
    {
      "sentence": "Researchers then analyse these gesture proposals, compute measures to identify common proposals (e.g. [59, 66]), and decide on a set of gestures to be used in an interactive system, typically composed of the gestures with high agreement among participants (e.g. [56, 67]).",
      "label": "Background",
      "prob": 0.3928951323032379
    },
    {
      "sentence": "This is an example for using GestureMap s spatial visualisation of gestures as paths for visual comparison via shape.",
      "label": "Result",
      "prob": 0.4029667377471924
    },
    {
      "sentence": "Several tools have been created for more effective and objective analyses.",
      "label": "Result",
      "prob": 0.38684871792793274
    },
    {
      "sentence": "1) Shape driven analysis: Some started by skimming through gestures to get an overview of their different path shapes on the map.",
      "label": "Background",
      "prob": 0.3840349614620209
    },
    {
      "sentence": "We asked people to use the interactive clustering tool based on their observations in the first task.",
      "label": "Background",
      "prob": 0.38385578989982605
    },
    {
      "sentence": "We can imagine that average gestures calculated with the DBA-algorithm can be used to visualize the non-leaf nodes in the hierarchical tree.",
      "label": "Background",
      "prob": 0.38066256046295166
    },
    {
      "sentence": "[66] to elicit users interaction preferences for new systems.",
      "label": "Background",
      "prob": 0.3791886568069458
    },
    {
      "sentence": "This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt).",
      "label": "Other",
      "prob": 0.44602930545806885
    },
    {
      "sentence": "Some researchers created specific visualizations to facilitate interpretation of the axes of a (2D) projection, to judge the variation of the data [27, 60] or the relative importance of the data attributes along an axis [31].",
      "label": "Background",
      "prob": 0.37505677342414856
    },
    {
      "sentence": "When hovering over an element, the corresponding gesture path is shown on the map for a moment.",
      "label": "Result",
      "prob": 0.3871883749961853
    },
    {
      "sentence": "2) automatic computation of elicitation metrics, and",
      "label": "Background",
      "prob": 0.37395647168159485
    },
    {
      "sentence": "a replacement of others: As a flexible tool, GestureMap can be extended to additionally display further such measures (e.g. the one by Vatavu [59]) to support researchers with the analysis.",
      "label": "Result",
      "prob": 0.37448638677597046
    },
    {
      "sentence": "In particular, we outline existing analysis concepts for high-dimensional data.",
      "label": "Background",
      "prob": 0.3700643479824066
    },
    {
      "sentence": "The trajectory of the embedded gesture paths can inform them on specific behavioral characteristics.",
      "label": "Result",
      "prob": 0.4980754256248474
    },
    {
      "sentence": "The map itself is continuous, that is, each 2D point represents a pose.",
      "label": "Result",
      "prob": 0.4088318347930908
    },
    {
      "sentence": "Judging Densities and Overlap of Referents.",
      "label": "Background",
      "prob": 0.3644682765007019
    },
    {
      "sentence": "For further inspection, one or more gestures can be selected (e.g. Figure 4) from a referents list of gesture proposals (Figure 1 3  ).",
      "label": "Background",
      "prob": 0.3633590340614319
    },
    {
      "sentence": "The scatterplot may help researchers to detect outlier body poses, while the density plot reveals regions with recorded data.",
      "label": "Result",
      "prob": 0.5616384148597717
    },
    {
      "sentence": "The gesture elicitation paradigm was first introduced by Wobbrock et al.",
      "label": "Other",
      "prob": 0.4758625030517578
    },
    {
      "sentence": "We therefore combine an abstract 2D mapping with a grid of representative 3D skeletons to give analysts a visual overview of the proposed gestures.",
      "label": "Background",
      "prob": 0.3560533821582794
    },
    {
      "sentence": "Using details on demand, users can hover over points to see the corresponding pose skeleton (Figure1b 2  ), and referent, participant and trial number in the detail view (Figure1b 6  ).",
      "label": "Background",
      "prob": 0.35437795519828796
    },
    {
      "sentence": "GestureMap builds on and extends functionalities of previous tools for gesture elicitation: It combines",
      "label": "Background",
      "prob": 0.3537076413631439
    },
    {
      "sentence": "Exploratory analysis seeks to uncover structural patterns in the dataset, identify anomalies, and single-out outliers [53].",
      "label": "Background",
      "prob": 0.3526724576950073
    },
    {
      "sentence": "Fittingly, recent related work highlighted the need and feasibility of more objective, computational measures [59], and called for further computational models and measures, based on a survey of 216 elicitation studies [63].",
      "label": "Other",
      "prob": 0.3859784007072449
    },
    {
      "sentence": "Our map view affords different plots on top of it, such as:",
      "label": "Background",
      "prob": 0.3428310453891754
    },
    {
      "sentence": "Six were familiar with gesture elicitation studies, the other two were interested in analysing gesture sensor data.",
      "label": "Result",
      "prob": 0.48808929324150085
    },
    {
      "sentence": "Second, they provided a visualization where only the moving joint is drawn on the canvas.",
      "label": "Result",
      "prob": 0.3701491057872772
    },
    {
      "sentence": "Being able to compute an average gesture enables the use of clustering methods that require average computations.",
      "label": "Method",
      "prob": 0.42678752541542053
    },
    {
      "sentence": "When study participants paused their exploration for a longer period, we inquired why that was the case.",
      "label": "Result",
      "prob": 0.4409838318824768
    },
    {
      "sentence": "Here we outline further ideas enabled or supported by GestureMap .",
      "label": "Background",
      "prob": 0.3382280170917511
    },
    {
      "sentence": "This typically involves two steps:",
      "label": "Background",
      "prob": 0.3339973986148834
    },
    {
      "sentence": "Video analysis has been the preferred evaluation method, but the annotation of individual video sequences can be timeconsuming [51].",
      "label": "Result",
      "prob": 0.3326881229877472
    },
    {
      "sentence": "4) The session concluded with a semi-structured interview of at least ten minutes.",
      "label": "Result",
      "prob": 0.4594939649105072
    },
    {
      "sentence": "To visualize temporal data, a common representation is a line plot, horizon plot [16], or a small-multiples plot [52].",
      "label": "Other",
      "prob": 0.37830135226249695
    },
    {
      "sentence": "Additionally, color codes facilitate the comparison of behavior across different referents.",
      "label": "Result",
      "prob": 0.5155782103538513
    },
    {
      "sentence": "This view either shows the raw skeleton recording or a reconstructed skeleton.",
      "label": "Result",
      "prob": 0.46706390380859375
    },
    {
      "sentence": "Researchers have therefore developed various measures to formalize the consensus among participants [56, 57, 61, 62, 67].",
      "label": "Other",
      "prob": 0.583759069442749
    },
    {
      "sentence": "Our work builds on this idea, extends its data-driven perspective with a visual analytics tool, and introduces a new measure fitting this visualization.",
      "label": "Objective",
      "prob": 0.47222650051116943
    },
    {
      "sentence": "First, they used a 3D animation of a Kinect skeleton.",
      "label": "Method",
      "prob": 0.3507300615310669
    },
    {
      "sentence": "Second, Section 7 follows the Usage strategy and reports on a user study with HCI researchers.",
      "label": "Result",
      "prob": 0.41581323742866516
    },
    {
      "sentence": "G  denotes the set of all gestures elicited for referent  .",
      "label": "Other",
      "prob": 0.3797045350074768
    },
    {
      "sentence": "Given the proliferation of crowd platforms to collect large datasets, we expect computational methods and visual analytics as proposed here to become indispensable tools for many future HCI studies.",
      "label": "Result",
      "prob": 0.45150601863861084
    },
    {
      "sentence": "Developing an Overview of the Gesture Space.",
      "label": "Objective",
      "prob": 0.3049478232860565
    },
    {
      "sentence": "We combine interactive k-means clustering, automatic metric computation, a new visualization, and analysis concepts to provide an integrated platform.",
      "label": "Method",
      "prob": 0.4881792664527893
    },
    {
      "sentence": "While we focus on researchers as users of this map in this paper, it could also be shown to participants as we described in Section 8.3.",
      "label": "Result",
      "prob": 0.38151854276657104
    },
    {
      "sentence": "They also said that points were visually closer to the data (point=pose).",
      "label": "Other",
      "prob": 0.41394758224487305
    },
    {
      "sentence": "We asked the researchers to analyze the proposals for crouch and throw ball .",
      "label": "Result",
      "prob": 0.2858894467353821
    },
    {
      "sentence": "GestureMap empowers researchers to compare data across studies (cf. Section 6).",
      "label": "Result",
      "prob": 0.428816556930542
    },
    {
      "sentence": "This view shows different metrics, namely variances around the average gesture sequence per selected referent (Section 4.2), the distributions of DTW distances of proposals to their average gesture sequence, and nearest neighbor distances for a selected gesture.",
      "label": "Result",
      "prob": 0.5371586680412292
    },
    {
      "sentence": "Motivatedbysuchinterestsinrelated work[24, 36, 41], we include an export functionality to easily share analyses with other researchers.",
      "label": "Result",
      "prob": 0.4328756332397461
    },
    {
      "sentence": "1) Projecting the data to 2D for display on a screen.",
      "label": "Result",
      "prob": 0.3876432776451111
    },
    {
      "sentence": "Thus, the map reveals the space of poses elicited by Vatavu [59] at a glance: For example, their referents included crouch , draw a flower , draw a circle , draw a square , applaud or raise your hands , which all match the poses in our map.",
      "label": "Result",
      "prob": 0.48579591512680054
    },
    {
      "sentence": "The features in GestureMap were informed by close examination of the literature on gesture elicitation and related concepts and tools: We collected features",
      "label": "Result",
      "prob": 0.34982216358184814
    },
    {
      "sentence": "Examining the map locally, in combination with gesture animations, reveals that some children sat on the floor, some on their heels, some crawled on hands/knees, and others stood with a stooped body posture.",
      "label": "Result",
      "prob": 0.6403753757476807
    },
    {
      "sentence": "With this work, we contribute to the vision of more widespread use of applicable computational methods in HCI, also to support more extensive and cost-efficient large-scale, data-driven HCI work.",
      "label": "Objective",
      "prob": 0.38584887981414795
    },
    {
      "sentence": "In contrast, we experimented with the k-means algorithm, using DBA to calculate the centroids.",
      "label": "Result",
      "prob": 0.3840974271297455
    },
    {
      "sentence": "Next, they were asked to initialize the clustering algorithm using their knowledge from the previous task.",
      "label": "Method",
      "prob": 0.42904049158096313
    },
    {
      "sentence": "Users could also select recorded",
      "label": "Result",
      "prob": 0.37165093421936035
    },
    {
      "sentence": "As Figure 4 shows, the children mostly stuck to their interpretation across multiple repeated trials for that referent, revealing consistency (cf. [5]).",
      "label": "Result",
      "prob": 0.5821952223777771
    },
    {
      "sentence": "However, they noted that it should be more accurate and manually refined assignments need to be respected when rerunning the clustering algorithm, thus enabling iterative, interactive use.",
      "label": "Result",
      "prob": 0.5496890544891357
    },
    {
      "sentence": "[41] created a tool to analyze recordings created by a Kinect camera sensor.",
      "label": "Result",
      "prob": 0.30593356490135193
    },
    {
      "sentence": "GestureMap supports this as it offers a platform to visualize and analyze multiple studies.",
      "label": "Result",
      "prob": 0.5400593280792236
    },
    {
      "sentence": "We used a Variational Autoencoder (VAE) [13] to embed the data as a 2D gesture map.",
      "label": "Method",
      "prob": 0.5504972338676453
    },
    {
      "sentence": "Addressing this, we extend the computational toolbox for analyzing gesture elicitation data with these contributions:",
      "label": "Result",
      "prob": 0.38937991857528687
    },
    {
      "sentence": "Density plots then offer a visualization of the most frequent gesture poses.",
      "label": "Result",
      "prob": 0.46294689178466797
    },
    {
      "sentence": "Indeed, this observation can be explained by the longer referent list (58 referents in [3] vs 15 in [59]).",
      "label": "Result",
      "prob": 0.4055183231830597
    },
    {
      "sentence": "For the crouch referent, everyone distinguished two to three groups of behaviors.",
      "label": "Result",
      "prob": 0.5682180523872375
    },
    {
      "sentence": "Moreover, this approach yields a one-number summary without a logistic regression model on top.",
      "label": "Result",
      "prob": 0.5530723929405212
    },
    {
      "sentence": "We further see, for example, sitting (Figure 2B), clapping (Figure 2D),",
      "label": "Result",
      "prob": 0.4583471417427063
    },
    {
      "sentence": "Overall, we see this approach as an additional measure, not",
      "label": "Result",
      "prob": 0.3721731901168823
    },
    {
      "sentence": "3) explicitly requested from future work.",
      "label": "Other",
      "prob": 0.4142139256000519
    },
    {
      "sentence": "We cached expensive computations such as the computed average sequences and distances matrices on MongoDB [21] to .",
      "label": "Other",
      "prob": 0.41680091619491577
    },
    {
      "sentence": "2) Position driven analysis: In contrast, other participants focused entirely on the scatter points as template poses.",
      "label": "Result",
      "prob": 0.6801379919052124
    },
    {
      "sentence": "As an example strategy, to detect the subgroup behavior for the throw ball referent, we quickly skimmed through the gestures using the map and visually identified rough patterns.",
      "label": "Result",
      "prob": 0.44092774391174316
    },
    {
      "sentence": "[48] filled this gap by providing a generic tool to visualize these embeddings.",
      "label": "Result",
      "prob": 0.4257377088069916
    },
    {
      "sentence": "The following paragraphs further introduce and motivate the features.",
      "label": "Result",
      "prob": 0.2769128084182739
    },
    {
      "sentence": "Thus, GestureMap visually reveals that people interpreted crouch in different ways, matching the high variance (Figure 3",
      "label": "Other",
      "prob": 0.3993496894836426
    },
    {
      "sentence": "8.3.3 Enabling Analysis of Unseen Behavior.",
      "label": "Other",
      "prob": 0.44522735476493835
    },
    {
      "sentence": "We experimented with different numbers of hidden neurons  : Overall, reconstruction loss decreases for larger models, regularized by the KL-loss, leading to diminishing returns and a decision for  = 512 here.",
      "label": "Result",
      "prob": 0.6542969346046448
    },
    {
      "sentence": "3.1.3 2D Map Overlays (Figure 1b 1  , Figure 1c).",
      "label": "Result",
      "prob": 0.444322407245636
    },
    {
      "sentence": "For full details, we provide the training scripts and model comparisons on the project website.",
      "label": "Result",
      "prob": 0.42828553915023804
    },
    {
      "sentence": "Overall, we implemented all UI views and interactions conceptually described in Section 3.",
      "label": "Method",
      "prob": 0.3363281190395355
    },
    {
      "sentence": "Following a cartographic approach [47], and in line with 2D projections in visual analytics (e.g. [27, 64]), we use a map metaphor to visually guide analysts through the elicited gesture space.",
      "label": "Method",
      "prob": 0.4996461272239685
    },
    {
      "sentence": "They included three visualizations.",
      "label": "Result",
      "prob": 0.46667975187301636
    },
    {
      "sentence": "In this way, the gesture map combines a line plots simplicity with the structural expressiveness of a small-multiples visualization [24].",
      "label": "Other",
      "prob": 0.5432063341140747
    },
    {
      "sentence": "This is repeated for several referents.",
      "label": "Result",
      "prob": 0.4598505198955536
    },
    {
      "sentence": "Furthermore, some researchers indicated that participants may struggle to propose gestures, if they are unfamiliar with the gesture design space [9, 12, 46].",
      "label": "Other",
      "prob": 0.6761714816093445
    },
    {
      "sentence": "To further evaluate GestureMap , we recruited eight HCI researchers (7 male, 1 female) from three universities via e-mail for remote think-alound and interview sessions.",
      "label": "Result",
      "prob": 0.48129600286483765
    },
    {
      "sentence": "Finally, to report a measure independent of the threshold value  , they used a logistic regression model to determine the consensus for a range of normalized threshold values and reported the growth rate as an indication of the overall consensus.",
      "label": "Result",
      "prob": 0.44693028926849365
    },
    {
      "sentence": "and raising an arm (Figure 2A).",
      "label": "Result",
      "prob": 0.4621785283088684
    },
    {
      "sentence": "Their calculations and analyses are based on hand-engineered features.",
      "label": "Result",
      "prob": 0.524777352809906
    },
    {
      "sentence": "In addition, we included further ideas.",
      "label": "Result",
      "prob": 0.39407315850257874
    },
    {
      "sentence": "To achieve this, they employed Dynamic Time Warping (DTW) distance computations to define a consensus measure: They considered two gesture sequences g  and g  as similar if the DTW distance was below a threshold   ( g  , g  )   .",
      "label": "Result",
      "prob": 0.5108783841133118
    },
    {
      "sentence": "[24] used an interactive hierarchical clustering approach with complete-linkage.",
      "label": "Method",
      "prob": 0.3777495324611664
    },
    {
      "sentence": "However, these measures rely on subjectively assessing the similarity of the observed gestures: They require researchers to group proposals into subgroups that they consider identical, which is usually done by manual annotation based on watching videos of the participants in the study [28, 39].",
      "label": "Other",
      "prob": 0.5955526232719421
    },
    {
      "sentence": "2) assess consensus as variance around this average gesture; and",
      "label": "Result",
      "prob": 0.5168026685714722
    },
    {
      "sentence": "Using overlays in GestureMap , we can identify similarity and differences between gestures across referents: For example, Figure 3 (left) shows that crouch, draw circle, draw flower, draw square share common behavior; their scatter points largely overlap in the region that encodes raised arm behavior.",
      "label": "Result",
      "prob": 0.537078320980072
    },
    {
      "sentence": "Thus, researchers devised different ways to distribute the work among people [1, 36].",
      "label": "Other",
      "prob": 0.5549484491348267
    },
    {
      "sentence": "CHI 21, May 813, 2021, Yokohama, Japan",
      "label": "Other",
      "prob": 0.7494847178459167
    },
    {
      "sentence": "1) represent gesture groups with one gesture;",
      "label": "Result",
      "prob": 0.3654191195964813
    },
    {
      "sentence": "In this work, we adapt similar visualization concepts with the goal to create an interpretable gesture space.",
      "label": "Objective",
      "prob": 0.6885150671005249
    },
    {
      "sentence": "Some also found the kicking behavior as described in Section 6.",
      "label": "Result",
      "prob": 0.569670557975769
    },
    {
      "sentence": "Looking ahead, new cloud elicitation tools [2, 36] yield large datasets.",
      "label": "Result",
      "prob": 0.6374057531356812
    },
    {
      "sentence": "1) proposed in related work,",
      "label": "Objective",
      "prob": 0.3522995114326477
    },
    {
      "sentence": "The gesture map which we propose in this work facilitates a richer exploration of the behavior space using machine learned features for the gesture poses.",
      "label": "Objective",
      "prob": 0.4409656524658203
    },
    {
      "sentence": "[35] used a cartographic approach to compare and analyze learned embedding spaces.",
      "label": "Method",
      "prob": 0.4266796112060547
    },
    {
      "sentence": "Only a few used the left hand.",
      "label": "Other",
      "prob": 0.5692354440689087
    },
    {
      "sentence": "Figure 1 shows the UI; the following sections refer to the numbers in the figure.",
      "label": "Result",
      "prob": 0.5468279123306274
    },
    {
      "sentence": "We then measure the DTW distance of every gesture proposal   for a referent  to the computed average gesture (i.e. barycenter) g  for  .",
      "label": "Result",
      "prob": 0.4932786524295807
    },
    {
      "sentence": "We implemented GestureMap as an analysis tool that integrates the described concepts of both the interactive gesture map (Section 3) and the DBA-based computations (Section 4).",
      "label": "Result",
      "prob": 0.4150466322898865
    },
    {
      "sentence": "Here we describe how users can interact with the map.",
      "label": "Result",
      "prob": 0.3137792944908142
    },
    {
      "sentence": "We first focus on the dataset by Vatavu [59] that consists of 1312 full body gestures elicited from children aged 3-6, recorded with a Kinect sensor.",
      "label": "Result",
      "prob": 0.49321427941322327
    },
    {
      "sentence": "Here, we describe the map concept in more detail.",
      "label": "Result",
      "prob": 0.33962371945381165
    },
    {
      "sentence": "These challenges motivate our work on new quantitative methods and tools for analyzing elicitation data.",
      "label": "Objective",
      "prob": 0.5218005180358887
    },
    {
      "sentence": "The frontend was implemented with NodeJS [18] and React [15].",
      "label": "Other",
      "prob": 0.4359472990036011
    },
    {
      "sentence": "8.3.4 Supporting Live Exploration and Monitoring.",
      "label": "Other",
      "prob": 0.5270288586616516
    },
    {
      "sentence": "With peoples consent we recorded the interviews.",
      "label": "Result",
      "prob": 0.4659133553504944
    },
    {
      "sentence": "[24] proposed to use interactive hierarchical clustering.",
      "label": "Method",
      "prob": 0.44434595108032227
    },
    {
      "sentence": "Related tools [24, 41] show a 3D skeleton view with animation.",
      "label": "Result",
      "prob": 0.5734173655509949
    },
    {
      "sentence": "We then inspected the mix of original referents present in the gestures assigned to each found cluster.",
      "label": "Result",
      "prob": 0.5329728126525879
    },
    {
      "sentence": "8.2.1 Smoothness of the Latent Space.",
      "label": "Result",
      "prob": 0.3487062156200409
    },
    {
      "sentence": "One successful method that has seen widespread use is the elicitation study paradigm [66], which helps HCI researchers and practitioners to explore the space of possible and intuitive or guessable (gesture) commands: Participants are shown a referent (often a system action, e.g. volume up ) and are asked to propose and perform a gesture they would use for it (e.g. turn wrist right ).",
      "label": "Method",
      "prob": 0.6036254167556763
    },
    {
      "sentence": "For the backend we used the Flask framework [10] and Pandas [11] to handle the data transformations and queries.",
      "label": "Method",
      "prob": 0.5790203213691711
    },
    {
      "sentence": "Here, we use k-means in particular.",
      "label": "Result",
      "prob": 0.4351004660129547
    },
    {
      "sentence": "3) interactive clustering to provide an integrated analysis platform.",
      "label": "Result",
      "prob": 0.43486618995666504
    },
    {
      "sentence": "A gesture sequence which consists of  gesture poses can be viewed as an ordered tuple of size  i.e., g = (  1 , . . .,  ) .",
      "label": "Other",
      "prob": 0.6794701218605042
    },
    {
      "sentence": "Finally, we report the variance of these DTW distances as a measure of consensus.",
      "label": "Result",
      "prob": 0.533868670463562
    },
    {
      "sentence": "We reflect on other possible choices in our discussion.",
      "label": "Result",
      "prob": 0.5417837500572205
    },
    {
      "sentence": "We took notes and compiled a report from this material.",
      "label": "Result",
      "prob": 0.5381044745445251
    },
    {
      "sentence": "Here demonstrate the use of GestureMap in a walkthrough of an explorative analysis: Examining the gesture map, the center (Figure 2C) reveals start/end poses (standing upright, arms at rest).",
      "label": "Result",
      "prob": 0.6741313338279724
    },
    {
      "sentence": "One person suggested to create an outlier group for these.",
      "label": "Result",
      "prob": 0.5544280409812927
    },
    {
      "sentence": "8.3.2 Enabling Map-based Gesture Authoring.",
      "label": "Other",
      "prob": 0.5701544284820557
    },
    {
      "sentence": "3.1.5 Linked Animations of Gestures.",
      "label": "Other",
      "prob": 0.6217973828315735
    },
    {
      "sentence": "While there exist many dimension reduction techniques [20, 32, 38, 54, 55], we use a Variational Autoencoder [30] to reduce the dimensions of the raw sensor data.",
      "label": "Method",
      "prob": 0.5662232637405396
    },
    {
      "sentence": "We repeated this ten times and made these observations:",
      "label": "Result",
      "prob": 0.5506330132484436
    },
    {
      "sentence": "We trained the VAE on the poses (frames) of the mentioned dataset [59] which has 60 dimensions (20 body joints  ,, ).",
      "label": "Result",
      "prob": 0.4924493134021759
    },
    {
      "sentence": "We return to ideas for improvements in our discussion.",
      "label": "Result",
      "prob": 0.5216348767280579
    },
    {
      "sentence": "Concretely, we ran the clustering with 15 sequences chosen randomly.",
      "label": "Result",
      "prob": 0.4131626784801483
    },
    {
      "sentence": "Therefore, to demonstrate our proposed clustering analysis we removed the referent labels and then evaluated if k-means finds clusters that match the original referents.",
      "label": "Result",
      "prob": 0.50834059715271
    },
    {
      "sentence": "The most similar work to ours is GestureAnalyzer by Jang et al.",
      "label": "Other",
      "prob": 0.7184195518493652
    },
    {
      "sentence": "For preprocessing, we followed the original authors [59] but left out the resampling step.",
      "label": "Method",
      "prob": 0.4272507131099701
    },
    {
      "sentence": "3.1.4 Linked Views of Postures.",
      "label": "Other",
      "prob": 0.617311418056488
    },
    {
      "sentence": "Throughout the interview we noticed that all participants preferred the scatter plot over the density plot.",
      "label": "Result",
      "prob": 0.7244487404823303
    },
    {
      "sentence": "This is an artefact of dimensionality reduction, as we discuss further in Section 8.2",
      "label": "Other",
      "prob": 0.48955076932907104
    },
    {
      "sentence": "For plotting, we use the PlotlyJs library [22].",
      "label": "Other",
      "prob": 0.4047999680042267
    },
    {
      "sentence": "Spotting Clusters and Outliers.",
      "label": "Result",
      "prob": 0.41143107414245605
    },
    {
      "sentence": "PyTorch [42] was used to develop the embedding model.",
      "label": "Result",
      "prob": 0.46075940132141113
    },
    {
      "sentence": "For further discussions on the comparison of these two systems we refer to section 8.2.2.",
      "label": "Other",
      "prob": 0.527463972568512
    },
    {
      "sentence": "As a second example, we compared behavior diversity across datasets.",
      "label": "Result",
      "prob": 0.6793426275253296
    },
    {
      "sentence": "8.3.1 Supporting Meta-Analysis and Consolidation.",
      "label": "Other",
      "prob": 0.3893917500972748
    },
    {
      "sentence": "[43] to compute an average gesture: Intuitively, this algorithm first aligns an initial sequence with every sequence in the set of gesture proposals, before computing a centroid (barycenter) for each aligned coordinate.",
      "label": "Method",
      "prob": 0.6573770642280579
    },
    {
      "sentence": "8.2.2 Cluster Approaches.",
      "label": "Result",
      "prob": 0.39275676012039185
    },
    {
      "sentence": "GestureMap also supports such analysis, as outlined here:",
      "label": "Other",
      "prob": 0.683510959148407
    },
    {
      "sentence": "Abstracting with credit is permitted.",
      "label": "Other",
      "prob": 0.8195109367370605
    },
    {
      "sentence": "[34] identified four evaluation strategies for toolkit contributions.",
      "label": "Result",
      "prob": 0.6907760500907898
    },
    {
      "sentence": "3) cluster gestures automatically.",
      "label": "Result",
      "prob": 0.43209901452064514
    },
    {
      "sentence": "When we asked the participants what the main aspect was that they used to determine interesting behavioral patterns, we observed diverse analysis strategies, but we broadly highlight two main ones:",
      "label": "Result",
      "prob": 0.7199457883834839
    },
    {
      "sentence": "Here, we highlight model and clustering aspects to consider.",
      "label": "Result",
      "prob": 0.6114641427993774
    },
    {
      "sentence": "To copy otherwise, or republish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.",
      "label": "Other",
      "prob": 0.8067464232444763
    },
    {
      "sentence": "3) In a more confirmatory, automatic analysis task we asked them to build on their gained insights to initialize the clustering algorithm and refine the automatic clustering results.",
      "label": "Result",
      "prob": 0.5265445113182068
    },
    {
      "sentence": "Five were familiar with machine learning.",
      "label": "Result",
      "prob": 0.5186808109283447
    },
    {
      "sentence": "These people found the animation particularly important: Seeing the 3D skeleton and the 2D path animated in sync highlighted that a gesture was a path on the map and thus helped them to get familiar with the map concept.",
      "label": "Result",
      "prob": 0.79006427526474
    },
    {
      "sentence": "We next describe the technical approach in more detail.",
      "label": "Method",
      "prob": 0.4139713943004608
    },
    {
      "sentence": "Here we describe the key implementation aspects.",
      "label": "Result",
      "prob": 0.5035780072212219
    },
    {
      "sentence": "1) gesture modeling and visualization,",
      "label": "Other",
      "prob": 0.6214454174041748
    },
    {
      "sentence": "This method was then specifically adapted to include gesture proposals to control surface tabletop computers [67].",
      "label": "Result",
      "prob": 0.3806281089782715
    },
    {
      "sentence": "We again used the dataset by Vatavu [59].",
      "label": "Other",
      "prob": 0.5554217100143433
    },
    {
      "sentence": "7.2.5 Manually Forming Clusters.",
      "label": "Result",
      "prob": 0.4021522104740143
    },
    {
      "sentence": "This has three practical values, which complement our tool:",
      "label": "Result",
      "prob": 0.5118571519851685
    },
    {
      "sentence": "The interviews had four parts:",
      "label": "Result",
      "prob": 0.4172672927379608
    },
    {
      "sentence": "Since then, this method has become a standard tool for the design of gesture input mappings for new interactive systems, for example to control a swarm of robots [28], smart-home appliances [26, 56], or AR/VR applications [44].",
      "label": "Result",
      "prob": 0.49743378162384033
    },
    {
      "sentence": "As a community, we could consolidate our findings in a meta-map of many studies, as a sensor data-driven complement to literature surveys [63].",
      "label": "Result",
      "prob": 0.5092175006866455
    },
    {
      "sentence": "In another experiment, we applied clustering to look for patterns within a referent: As mentioned, referents such as throw ball and crouch contained distinct patterns, revealed on the map.",
      "label": "Result",
      "prob": 0.7712903022766113
    },
    {
      "sentence": "We then chose  correspondingly.",
      "label": "Result",
      "prob": 0.46093234419822693
    },
    {
      "sentence": "Also related to our work are tools to analyze and visualize machine learned representations of complex data: Deep learning models are capable of learning human-understandable features of high-dimensional data: For example, Kingma and Welling [30] and Lawrence [33] sample multiple points from the learned space and visualize them to demonstrate that the learned space is continuous and smooth, but without providing interaction functionalities.",
      "label": "Result",
      "prob": 0.6423942446708679
    },
    {
      "sentence": "Table 1 shows an overview of the relation to related work.",
      "label": "Result",
      "prob": 0.7096903920173645
    },
    {
      "sentence": "Indeed, running k-means revealed some of them: For example, for throw ball k-means also detected throwing with the right hand vs using both hands.",
      "label": "Result",
      "prob": 0.8357210755348206
    },
    {
      "sentence": "We used a server-client architecture.",
      "label": "Method",
      "prob": 0.45552629232406616
    },
    {
      "sentence": "Overall, the researchers felt comfortable with grouping the proposals based on the path shapes.",
      "label": "Result",
      "prob": 0.8085469007492065
    },
    {
      "sentence": "7.2.3 Details of the Gesture Map View.",
      "label": "Other",
      "prob": 0.6172846555709839
    },
    {
      "sentence": "2021 Copyright held by the owner/author(s).",
      "label": "Other",
      "prob": 0.8625429272651672
    },
    {
      "sentence": "3.1.1 3D Skeleton View (Figure 1b 2  ).",
      "label": "Other",
      "prob": 0.5771134495735168
    },
    {
      "sentence": "7.2.2 Statistical Plot Overlays.",
      "label": "Result",
      "prob": 0.43139931559562683
    },
    {
      "sentence": "For further technical details we refer the reader to the related work [43].",
      "label": "Other",
      "prob": 0.7699777483940125
    },
    {
      "sentence": "Comparing Referents and Regions.",
      "label": "Result",
      "prob": 0.4426901936531067
    },
    {
      "sentence": "[3], next to the childrens proposals from Vatavu [59].",
      "label": "Other",
      "prob": 0.778897225856781
    },
    {
      "sentence": "3.2.2 Learning a Gesture Map.",
      "label": "Other",
      "prob": 0.7142537236213684
    },
    {
      "sentence": "Formally, this is noted as:",
      "label": "Other",
      "prob": 0.8161479830741882
    },
    {
      "sentence": "We trained for 2000 epochs with Adam [29] (lr=3   5 ).",
      "label": "Result",
      "prob": 0.5923717021942139
    },
    {
      "sentence": "3.3.4 Examining Unseen Poses.",
      "label": "Other",
      "prob": 0.6503677368164062
    },
    {
      "sentence": "Formally, let the set of all  individual gesture poses in the dataset be denoted by G = {   |  = [ 1 , . . .,  ]} ,    R  where  is the dimensionality of the raw sensor data (in our case D=20).",
      "label": "Result",
      "prob": 0.4163602292537689
    },
    {
      "sentence": "We employ the DTW Barycenter Averaging (DBA) algorithm by Petitjean et al.",
      "label": "Other",
      "prob": 0.7863219976425171
    },
    {
      "sentence": "We defined a consensus measure on this variability (Section 4.2): Comparing this variability between all referents, our results largely agree with Vatavu [59]: In particular, applaud, fly like a bird and hands up show high consensus while climb ladder, crouch, turn around have low consensus.",
      "label": "Result",
      "prob": 0.7997953295707703
    },
    {
      "sentence": "As our key contribution, we presented a set of visualization and analysis concepts for gesture elicitation data and a tool that implements them: GestureMap is the first visual analytics tool for gesture elicitation which directly visualises the space of gestures, using a learned 2D embedding.",
      "label": "Method",
      "prob": 0.6641702651977539
    },
    {
      "sentence": "Finally, the toolbox in the literature includes several formalized agreement measures [56, 67].",
      "label": "Other",
      "prob": 0.6453456282615662
    },
    {
      "sentence": "We consider four existing datasets: One explicit gesture elicitiation study by Vatavu [59], plus three datasets collected for gesture recognition systems [3, 7, 17].",
      "label": "Other",
      "prob": 0.4993724226951599
    },
    {
      "sentence": "Without knowing anything about the referents, Figure 6 already reveals that one dataset [3] ( blue ) covers a larger region than the other [59] ( orange ).",
      "label": "Result",
      "prob": 0.8275451064109802
    },
    {
      "sentence": "In a subsequent study, Morris et al.",
      "label": "Other",
      "prob": 0.8286507725715637
    },
    {
      "sentence": "As larger data sets are expected in the future [1, 24, 41], we also provide an interactive clustering method to reduce manual workload for identifying similar gesture (sub)groups.",
      "label": "Method",
      "prob": 0.6048805117607117
    },
    {
      "sentence": "3.1.2 2D Map View (Figure 1b 1  ).",
      "label": "Other",
      "prob": 0.6728895902633667
    },
    {
      "sentence": "7.2.4 Exploration Strategies.",
      "label": "Other",
      "prob": 0.7146883606910706
    },
    {
      "sentence": "3.3.1 Pan and Zoom.",
      "label": "Other",
      "prob": 0.8057421445846558
    },
    {
      "sentence": "5.1.3 3D Skeleton View Figure 1b 2  , Figure 1b 4  .",
      "label": "Other",
      "prob": 0.6107065081596375
    },
    {
      "sentence": "3.3.2 Examining Poses.",
      "label": "Other",
      "prob": 0.671564519405365
    },
    {
      "sentence": "Villarreal-Narvaez et al.",
      "label": "Other",
      "prob": 0.8987952470779419
    },
    {
      "sentence": "5.1.2 Experiment View Figure 1b 3  .",
      "label": "Other",
      "prob": 0.5519000291824341
    },
    {
      "sentence": "We demonstrate this by creating a gesture map using four datasets [3, 8, 17, 59].",
      "label": "Other",
      "prob": 0.6652123928070068
    },
    {
      "sentence": "We adapted the architecture from Spurr et al.",
      "label": "Other",
      "prob": 0.79192715883255
    },
    {
      "sentence": "ACM ISBN 978-1-4503-8096-6/21/05..",
      "label": "Other",
      "prob": 0.9147647023200989
    },
    {
      "sentence": "This work motivates us to further explore data-driven measures of consensus: We follow a similar approach, but instead of regressing on the DTW distance values, and relying on pairwise comparisons, we directly compute an average sequence from all gesture proposals in a referent group, using DBA.",
      "label": "Method",
      "prob": 0.7897073030471802
    },
    {
      "sentence": "We follow their perspective to evaluate GestureMap , combining two such strategies: First, here we follow the Demonstration strategy and provide a detailed analysis of examples on elicitation data from related work.",
      "label": "Result",
      "prob": 0.8090745806694031
    },
    {
      "sentence": "In line with Fu et al.",
      "label": "Other",
      "prob": 0.879107654094696
    },
    {
      "sentence": "GestureMap and further materials are available on the project website: https://osf.io/dzn5g/",
      "label": "Other",
      "prob": 0.9195396304130554
    },
    {
      "sentence": "Our variance measure also reflects this (Aloba adults 23 . 70, children 44 . 89; Vatavu children 54 . 94).",
      "label": "Other",
      "prob": 0.7769470810890198
    },
    {
      "sentence": "Considering the literature, Jang et al.",
      "label": "Other",
      "prob": 0.8885571956634521
    },
    {
      "sentence": "3.1.7 Sharing Results.",
      "label": "Result",
      "prob": 0.7419221997261047
    },
    {
      "sentence": "7.2.1 Initial Use.",
      "label": "Other",
      "prob": 0.77440345287323
    },
    {
      "sentence": "Alternatively, Ali et al.",
      "label": "Other",
      "prob": 0.9189261198043823
    },
    {
      "sentence": "3.1.6 Gesture Clustering.",
      "label": "Other",
      "prob": 0.8685869574546814
    },
    {
      "sentence": "3.4.2 Local Observations.",
      "label": "Other",
      "prob": 0.8569557666778564
    },
    {
      "sentence": "Publication rights licensed to ACM.",
      "label": "Other",
      "prob": 0.9333989024162292
    },
    {
      "sentence": "3.3.3 Examining Gestures.",
      "label": "Other",
      "prob": 0.8013737201690674
    },
    {
      "sentence": "Nebeling et al.",
      "label": "Other",
      "prob": 0.9243939518928528
    },
    {
      "sentence": "Figure 5 shows all 20 proposals for jump from the data by Aloba et al.",
      "label": "Other",
      "prob": 0.8036748170852661
    },
    {
      "sentence": "3.4.1 Global Observations.",
      "label": "Other",
      "prob": 0.8979580402374268
    },
    {
      "sentence": "Ledo et al.",
      "label": "Other",
      "prob": 0.922658383846283
    },
    {
      "sentence": "8.2.3 Feature Representation.",
      "label": "Other",
      "prob": 0.8381215929985046
    },
    {
      "sentence": "https://doi.org/10.1145/3411764.3445765",
      "label": "Other",
      "prob": 0.9444043040275574
    },
    {
      "sentence": "3.2.1 Core Visualization Concept.",
      "label": "Other",
      "prob": 0.8210697770118713
    },
    {
      "sentence": "5.1.1 Gesture Map Figure 1b 1  .",
      "label": "Other",
      "prob": 0.8304916620254517
    },
    {
      "sentence": "Jang et al.",
      "label": "Other",
      "prob": 0.9307536482810974
    },
    {
      "sentence": "Smilkov et al.",
      "label": "Other",
      "prob": 0.9376078844070435
    },
    {
      "sentence": "7.2.6 Interactive Clustering.",
      "label": "Other",
      "prob": 0.864769458770752
    },
    {
      "sentence": "5.1.5 Cluster View Figure 1b 6  .",
      "label": "Other",
      "prob": 0.7974557280540466
    },
    {
      "sentence": "5.1.4 Statistics View Figure 1b 5  .",
      "label": "Other",
      "prob": 0.8147938847541809
    },
    {
      "sentence": "Liu et al.",
      "label": "Other",
      "prob": 0.9273274540901184
    },
    {
      "sentence": "Color coding the cluster results can be done quickly.",
      "label": "Result",
      "prob": 0.903096616268158
    },
    {
      "sentence": "ACM Reference Format:",
      "label": "Other",
      "prob": 0.9455322623252869
    },
    {
      "sentence": "REFERENCES",
      "label": "Other",
      "prob": 0.9412382245063782
    }
  ]
}