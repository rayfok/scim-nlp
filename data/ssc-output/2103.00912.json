{
  "2103.00912": [
    {
      "sentence": "However, these highly abstract visualizations may occlude the nature of the underlying data.",
      "label": "Background",
      "prob": 0.9790772795677185
    },
    {
      "sentence": "For example, a participants movement can be subtle, in which case the embedded gesture path is simple in shape and typically spans a small region in the gesture map.",
      "label": "Background",
      "prob": 0.9786239862442017
    },
    {
      "sentence": "These occur due to recording issues (e.g. sensor occlusion in some frames) or when subsequent poses are embedded far apart in the 2D space.",
      "label": "Background",
      "prob": 0.972472608089447
    },
    {
      "sentence": "However, the resulting computed centroids often deviated from peoples expectations, and thus did not immediately make sense to them.",
      "label": "Background",
      "prob": 0.9688170552253723
    },
    {
      "sentence": "For instance, such a map might reveal which gestures and poses are most common or intensely studied.",
      "label": "Background",
      "prob": 0.968515932559967
    },
    {
      "sentence": "Hand-engineered features [4, 24, 58] may help with the interpretation, however, they may be specific to a sensor and interaction setup.",
      "label": "Background",
      "prob": 0.9680206775665283
    },
    {
      "sentence": "While some models address this (e.g. we used a VAE instead of AE), there is no universal natural 2D layout of body poses and some artifacts are likely to exist for most models and datasets.",
      "label": "Background",
      "prob": 0.9673253893852234
    },
    {
      "sentence": "Regions in the gesture map that contain multiple embedded data points from different referents may indicate that this region encodes shared generic behavior.",
      "label": "Background",
      "prob": 0.9672379493713379
    },
    {
      "sentence": "While elicitation studies have become a widely used staple in the HCI toolbox, they still present challenges (cf. [51, 63]), including the need for manual data analysis .",
      "label": "Background",
      "prob": 0.9645212292671204
    },
    {
      "sentence": "Looking ahead, new cloud elicitation tools [2, 36] yield large datasets.",
      "label": "Background",
      "prob": 0.9629247188568115
    },
    {
      "sentence": "So far, elicitation has focused on observed gestures, yet it might also be relevant to examine why behavior was not observed.",
      "label": "Background",
      "prob": 0.9628567099571228
    },
    {
      "sentence": "In contrast, a complex gesture may be represented as an intricate path that may meander across the map.",
      "label": "Background",
      "prob": 0.9622753262519836
    },
    {
      "sentence": "However, there were some complex paths (e.g. crossing over many poses on the map) that people were unable to assign to a group.",
      "label": "Background",
      "prob": 0.9618426561355591
    },
    {
      "sentence": "For a typical elicitation study, such as this one by Vatavu [59], it is reasonable to expect clusters induced by the referents.",
      "label": "Background",
      "prob": 0.9587881565093994
    },
    {
      "sentence": "Scatter points may visually cluster near gesture poses that are characteristic for a particular referent.",
      "label": "Background",
      "prob": 0.9565372467041016
    },
    {
      "sentence": "Scatter plots may contain too much detail and clutter the visualization.",
      "label": "Background",
      "prob": 0.9557032585144043
    },
    {
      "sentence": "After the initial data exploration it is often necessary to find concrete example for detected patterns.",
      "label": "Background",
      "prob": 0.9524074792861938
    },
    {
      "sentence": "Expert users especially liked the visual expressiveness of GestureMap , as it quickly summarizes the underlying dataset.",
      "label": "Background",
      "prob": 0.948514461517334
    },
    {
      "sentence": "Intuitively, for example, a high value VAR  may inform an analyst that referent  contains quite varied gesture proposals (i.e. low consensus).",
      "label": "Background",
      "prob": 0.9462858438491821
    },
    {
      "sentence": "Thus, while these measures set standards on how to compute consensus from gesture proposal, they cannot avoid subjectivity per se.",
      "label": "Background",
      "prob": 0.9453213810920715
    },
    {
      "sentence": "Some additionally employ a sensor in elicitation (e.g. Leap [62], Kinect [56]), thus also potentially considering the senseable space .",
      "label": "Background",
      "prob": 0.9442924857139587
    },
    {
      "sentence": "Although central to HCI, the field has developed few dedicated methods and tools for supporting the (joint) exploration of such user-sensor spaces (cf. [65]).",
      "label": "Background",
      "prob": 0.944179892539978
    },
    {
      "sentence": "However, they noted that it should be more accurate and manually refined assignments need to be respected when rerunning the clustering algorithm, thus enabling iterative, interactive use.",
      "label": "Background",
      "prob": 0.943099856376648
    },
    {
      "sentence": "Thus, since gestures are sequences of poses, they are paths connecting multiple points on the map.",
      "label": "Background",
      "prob": 0.9385406970977783
    },
    {
      "sentence": "They therefore modified elicitation such that people could choose from a predefined list of gesture proposals.",
      "label": "Background",
      "prob": 0.9366744160652161
    },
    {
      "sentence": "Integrating such a tree-like layout into the gesture map adds complexity and might be material for future endeavours.",
      "label": "Background",
      "prob": 0.9363294243812561
    },
    {
      "sentence": "GestureMap is already implemented as a web-based tool, rendering it flexible and open to such integration.",
      "label": "Background",
      "prob": 0.9345799088478088
    },
    {
      "sentence": "One user noted that one still has to inspect all gesture proposals in order to choose suitable initialisations for the k-means algorithm.",
      "label": "Background",
      "prob": 0.9325072765350342
    },
    {
      "sentence": "GestureMap is fundamentally motivated by providing researchers with a visual overview of the elicited gesture space.",
      "label": "Background",
      "prob": 0.930517852306366
    },
    {
      "sentence": "Thus,bycomparingmultipleembeddedgesturepathsresearchers can visually assess gestures as similar or not.",
      "label": "Background",
      "prob": 0.929327666759491
    },
    {
      "sentence": "For example, in the elicitation context, we might be interested in comparing the behavior across different participants and experimental trials.",
      "label": "Background",
      "prob": 0.9281622767448425
    },
    {
      "sentence": "In real use, researchers might export this result, for example, for a report, calculations of agreement, etc.",
      "label": "Background",
      "prob": 0.9226749539375305
    },
    {
      "sentence": "Seeing this and related work as a toolbox, researchers may now consider various options: For example, AGATE 2.0 [61] is a highly specialized tool to compute agreement, which assumes a given labeled dataset.",
      "label": "Background",
      "prob": 0.9207782745361328
    },
    {
      "sentence": "Some found similar poses encoded in different map regions and noted that these should ideally reside in one area.",
      "label": "Background",
      "prob": 0.9172312617301941
    },
    {
      "sentence": "The two dimensions of the map do not have a direct predefined meaning yet emerge from elicited data.",
      "label": "Background",
      "prob": 0.9146690368652344
    },
    {
      "sentence": "Vatavu [59] were the first to propose a data-driven consensus measure that does not rely on human judgement of gesture similarity.",
      "label": "Background",
      "prob": 0.9140530824661255
    },
    {
      "sentence": "One could also predefine a gesture path to monitor live performances and to judge deviation from this template, possibly to learn/teach a movement sequence.",
      "label": "Background",
      "prob": 0.9134908318519592
    },
    {
      "sentence": "The VAE here serves as an exemplar of a model with both powerful (non-linear) encoding and decoding capabilities.",
      "label": "Background",
      "prob": 0.9120665192604065
    },
    {
      "sentence": "If researchers animate a gesture, it is simultaneously animated in this view and on the map.",
      "label": "Background",
      "prob": 0.9100024104118347
    },
    {
      "sentence": "For example, these observations can inform researchers interested in building gesture recognizers in judging the difficulty of separating gestures for the various referents.",
      "label": "Background",
      "prob": 0.909361720085144
    },
    {
      "sentence": "For example, these plots may hide the structure of a 3D skeleton recording.",
      "label": "Background",
      "prob": 0.9083366394042969
    },
    {
      "sentence": "Given the proliferation of crowd platforms to collect large datasets, we expect computational methods and visual analytics as proposed here to become indispensable tools for many future HCI studies.",
      "label": "Background",
      "prob": 0.9064921736717224
    },
    {
      "sentence": "These reassignments, however, were not yet considered when rerunning the clustering algorithm in the current implementation.",
      "label": "Background",
      "prob": 0.9054760336875916
    },
    {
      "sentence": "These could be used also with our interactive clustering, for example, by plugging in the cluster cardinalities instead of subjective gesture group counts.",
      "label": "Background",
      "prob": 0.9022271037101746
    },
    {
      "sentence": "The frontend and backend modules communicate through a REST API through which the data is transmitted as a JSON formatted string.",
      "label": "Background",
      "prob": 0.9021387100219727
    },
    {
      "sentence": "For example, it might be interesting to identify which referents share behavior and which are distinctive.",
      "label": "Background",
      "prob": 0.8996086716651917
    },
    {
      "sentence": "However, these measures rely on subjectively assessing the similarity of the observed gestures: They require researchers to group proposals into subgroups that they consider identical, which is usually done by manual annotation based on watching videos of the participants in the study [28, 39].",
      "label": "Background",
      "prob": 0.8983210325241089
    },
    {
      "sentence": "Motivatedbysuchinterestsinrelated work[24, 36, 41], we include an export functionality to easily share analyses with other researchers.",
      "label": "Background",
      "prob": 0.8969908356666565
    },
    {
      "sentence": "In addition, interactive hierarchical clustering would eliminate the need for choosing the number of clusters beforehand.",
      "label": "Background",
      "prob": 0.8968374729156494
    },
    {
      "sentence": "A fundamentally new capability of GestureMap is that unseen poses or gestures (i.e. not proposed by participants) can be simulated by decoding arbitrary 2D points in the learned space.",
      "label": "Background",
      "prob": 0.8947875499725342
    },
    {
      "sentence": "Separate maps could also compare gesture spaces for different contexts, devices, etc., for example, to better understand the influences of such factors.",
      "label": "Background",
      "prob": 0.8888936638832092
    },
    {
      "sentence": "GestureMap could be extended to define new gestures: For example, users could draw a gesture as a path on the map.",
      "label": "Background",
      "prob": 0.8842754364013672
    },
    {
      "sentence": "The former targets questions that may span multiple referents or the entire dataset, while the latter focuses on a few gestures to identify specific behavioral idiosyncrasies.",
      "label": "Background",
      "prob": 0.8822651505470276
    },
    {
      "sentence": "Upon first use, most people immediately animated a few gestures, saying that this was the most natural and familiar way to view the data Since the map visualization was unfamiliar to them, some had initial difficulties to understand the distinction of single poses (points) and entire gestures (paths).",
      "label": "Background",
      "prob": 0.8795799016952515
    },
    {
      "sentence": "This limits elicitation studies, as well as the general endeavor of systematically exploring behavioursensor spaces in HCI, as characterised in the following paragraphs:",
      "label": "Background",
      "prob": 0.8749292492866516
    },
    {
      "sentence": "Scatter or density plots can be projected onto the map (e.g. Figure 1b 1  and Figure 1c).",
      "label": "Background",
      "prob": 0.8732956051826477
    },
    {
      "sentence": "In contrast, it did not separately find left hand and kicking, presumably since those were proposed only a few times.",
      "label": "Background",
      "prob": 0.8701392412185669
    },
    {
      "sentence": "Potentially, for the children the basic shapes afforded less flexible interpretation than a flower or crouching.",
      "label": "Background",
      "prob": 0.8678561449050903
    },
    {
      "sentence": "Centroids can be animated and once the clusters have been computed, users can toggle all gesture proposals that were assigned to a centroid.",
      "label": "Background",
      "prob": 0.8674376010894775
    },
    {
      "sentence": "The first of many analysis steps often involves developing an overview of the data to understand its underlying properties: Researchers here often use statistical plots to summarize the data and to identify broad patterns.",
      "label": "Background",
      "prob": 0.8627528548240662
    },
    {
      "sentence": "Since then, this method has become a standard tool for the design of gesture input mappings for new interactive systems, for example to control a swarm of robots [28], smart-home appliances [26, 56], or AR/VR applications [44].",
      "label": "Background",
      "prob": 0.8553449511528015
    },
    {
      "sentence": "They stopped to examine gestures in more detail that differed largely from the shapes seen so far.",
      "label": "Background",
      "prob": 0.8497090339660645
    },
    {
      "sentence": "Designing effective interactions and user interfaces often involves exploring two potentially high-dimensional spaces [65]: 1) The",
      "label": "Background",
      "prob": 0.849090039730072
    },
    {
      "sentence": "Several tools have been created for more effective and objective analyses.",
      "label": "Background",
      "prob": 0.847399890422821
    },
    {
      "sentence": "For further inspection, one or more gestures can be selected (e.g. Figure 4) from a referents list of gesture proposals (Figure 1 3  ).",
      "label": "Background",
      "prob": 0.8460531830787659
    },
    {
      "sentence": "As an exploratory tool, GestureMaps learned space is applicable to new and changing setups, without developing hand-engineered features first.",
      "label": "Background",
      "prob": 0.8441444635391235
    },
    {
      "sentence": "Related, gesture sets are mostly presented as drawings and videos today [37].",
      "label": "Background",
      "prob": 0.8438710570335388
    },
    {
      "sentence": "GestureMap could be extended to more than post-hoc analysis: For example, we could embed live sensor data and continuously update the underlying mode.",
      "label": "Background",
      "prob": 0.8438650965690613
    },
    {
      "sentence": "space of human behaviour (e.g. comfortable motion ranges of arm and hand), and 2) the space of senseable input in a system or context (e.g. tracking of up to X body joints in 3D).",
      "label": "Background",
      "prob": 0.8408676981925964
    },
    {
      "sentence": "Thus, it seems to contain a more diverse set of body poses.",
      "label": "Background",
      "prob": 0.8401253819465637
    },
    {
      "sentence": "Researchers can zoom, pan, and hover over the gesture map, and overlay a scatter plot or a density plot (e.g. Figure 1c) to explore individual or multiple gesture poses.",
      "label": "Background",
      "prob": 0.8364393711090088
    },
    {
      "sentence": "GestureMap s concepts support handling large data, visually summarised and explored via our map view.",
      "label": "Background",
      "prob": 0.8363021612167358
    },
    {
      "sentence": "The gesture variance integrates well with GestureMap s visualization concept because this already displays the involved average gestures as visual elements.",
      "label": "Background",
      "prob": 0.8360487222671509
    },
    {
      "sentence": "Besides technical model improvements, visualization concepts could be explored to address this as well (e.g. visually mark jumps along the gesture path).",
      "label": "Background",
      "prob": 0.8356713652610779
    },
    {
      "sentence": "In contrast, a hierarchical treemap does not directly fit the map metaphor well.",
      "label": "Background",
      "prob": 0.8352630138397217
    },
    {
      "sentence": "In our prototype users can thus hover over the map to visualize 3D skeletons for any cursor location.",
      "label": "Background",
      "prob": 0.8336086273193359
    },
    {
      "sentence": "Analysts can examine if empty regions are anatomically not feasible (cf. 8.3.3) or if people did not show such behaviour.",
      "label": "Background",
      "prob": 0.8332023024559021
    },
    {
      "sentence": "The gesture paths visit roughly similar main parts of the gesture space, yet the children do not find consensus.",
      "label": "Background",
      "prob": 0.8330774307250977
    },
    {
      "sentence": "GestureMap could be used to label data and export it for analysis in tools like this.",
      "label": "Background",
      "prob": 0.8315410614013672
    },
    {
      "sentence": "inquired into what people liked/disliked about GestureMap , and asked for ideas for improvements and additional features.",
      "label": "Background",
      "prob": 0.8313083648681641
    },
    {
      "sentence": "Such support as shown in GestureMap could be combined with a crowd approach in the future.",
      "label": "Background",
      "prob": 0.8302266001701355
    },
    {
      "sentence": "Some additionally jumped at the end of their gesture proposals to get back onto their feet.",
      "label": "Background",
      "prob": 0.8297563791275024
    },
    {
      "sentence": "It might also be interesting to analyze outlier behavior which can be detected by examining scatter points that lie far from these clusters.",
      "label": "Background",
      "prob": 0.8287321925163269
    },
    {
      "sentence": "As another such example, for throw ball , behavior can be categorized into four clusters: Most children used their right hand, others used two hands, and some kicked the ball.",
      "label": "Background",
      "prob": 0.8286361694335938
    },
    {
      "sentence": "Instead, GestureMap could be used to show gestures to users, allowing them to reenact and explore them with live monitoring via the map.",
      "label": "Background",
      "prob": 0.8280689120292664
    },
    {
      "sentence": "Each scatter point corresponds to a pose from the dataset, whereas empty patches in the gesture map may indicate behavior that has not been observed (e.g. poses/gestures not proposed by participants during elicitation).",
      "label": "Background",
      "prob": 0.8279383182525635
    },
    {
      "sentence": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "label": "Background",
      "prob": 0.8263400197029114
    },
    {
      "sentence": "In this way, elicitation studies inform gestural interaction with user-driven exploration: Most studies focus on the human behaviour space and thus do not rely on a specific sensor; they typically video-record participants for manual gesture analysis (e.g. [14, 28]).",
      "label": "Background",
      "prob": 0.8234948515892029
    },
    {
      "sentence": "Since the underlying latent variable model can simulate new behavior (decoding), such a drawn path implicitly defines a pose sequence that could be exported as a template-based gesture recogniser.",
      "label": "Background",
      "prob": 0.8194036483764648
    },
    {
      "sentence": "Prior work has extensively used scatter plots to analyze machine learned representations [35, 48].",
      "label": "Background",
      "prob": 0.8144180774688721
    },
    {
      "sentence": "Concretely, the 3D skeleton view updates while the user moves the cursor over the 2D map to present the posture at that point in the gesture space.",
      "label": "Background",
      "prob": 0.8138614892959595
    },
    {
      "sentence": "gestures, people agreed on the path shapes as primary discerning feature (strategy 1).",
      "label": "Background",
      "prob": 0.8130034804344177
    },
    {
      "sentence": "Researchers have therefore developed various measures to formalize the consensus among participants [56, 57, 61, 62, 67].",
      "label": "Background",
      "prob": 0.8104516267776489
    },
    {
      "sentence": "The gesture map can serve as a common visual basis for such investigations: By projecting multiple gestures onto the map, researchers can evaluate each participants behavior individually.",
      "label": "Background",
      "prob": 0.8083434104919434
    },
    {
      "sentence": "A gesture sequence which consists of  gesture poses can be viewed as an ordered tuple of size  i.e., g = (  1 , . . .,  ) .",
      "label": "Background",
      "prob": 0.8070160150527954
    },
    {
      "sentence": "[1] proposed a crowd platform for annotation, yet without computational support for the workers, such as alternative gesture representations or similarity measures.",
      "label": "Background",
      "prob": 0.8060231804847717
    },
    {
      "sentence": "This view lists all referents and gesture proposals in a compact way as numbers for quick reference and selection.",
      "label": "Background",
      "prob": 0.8035265803337097
    },
    {
      "sentence": "Some researchers created specific visualizations to facilitate interpretation of the axes of a (2D) projection, to judge the variation of the data [27, 60] or the relative importance of the data attributes along an axis [31].",
      "label": "Background",
      "prob": 0.802155613899231
    },
    {
      "sentence": "This dialog is unfolded with a button in Figure 1b 3  and lets users interactively cluster gesture proposals for a referent.",
      "label": "Background",
      "prob": 0.799765408039093
    },
    {
      "sentence": "This is an example for using GestureMap s spatial visualisation of gestures as paths for visual comparison via shape.",
      "label": "Background",
      "prob": 0.7993595600128174
    },
    {
      "sentence": "In this way, GestureMap supports the diagnosis of challenges and limitations in the joint user-sensor space of an interactive system (cf. [65]).",
      "label": "Background",
      "prob": 0.7983816266059875
    },
    {
      "sentence": "This might be useful to adjust elicitation setup/instructions, for example to prompt people to also cover a previously empty part of the map.",
      "label": "Background",
      "prob": 0.797996997833252
    },
    {
      "sentence": "Proposals for crouch form two main clusters (pink points in Figure 3 left), one in the region of starting poses, another in sitting/crouching regions.",
      "label": "Background",
      "prob": 0.7977614998817444
    },
    {
      "sentence": "These people found the animation particularly important: Seeing the 3D skeleton and the 2D path animated in sync highlighted that a gesture was a path on the map and thus helped them to get familiar with the map concept.",
      "label": "Background",
      "prob": 0.7916227579116821
    },
    {
      "sentence": "[50] (i.e. 4 hidden layers for both encoder and decoder) and used a 2D bottleneck layer.",
      "label": "Background",
      "prob": 0.7892186045646667
    },
    {
      "sentence": "A difficulty with k-means is setting the number of clusters.",
      "label": "Background",
      "prob": 0.7860539555549622
    },
    {
      "sentence": "In real use, researchers would conduct such analyses to better understand elicited data.",
      "label": "Background",
      "prob": 0.7857587337493896
    },
    {
      "sentence": "The map itself is continuous, that is, each 2D point represents a pose.",
      "label": "Background",
      "prob": 0.7824535369873047
    },
    {
      "sentence": "Complementary to the feature for postures, GestureMap accounts for the temporal nature of gesture data [24, 41] by offering linked animations of gesture paths (point moving on the path) and 3D skeletons (skeleton moving).",
      "label": "Background",
      "prob": 0.7729598879814148
    },
    {
      "sentence": "Technically, this can be readily implemented by initialising k-means with the current (refined) assignments.",
      "label": "Background",
      "prob": 0.7728241086006165
    },
    {
      "sentence": "Fittingly, recent related work highlighted the need and feasibility of more objective, computational measures [59], and called for further computational models and measures, based on a survey of 216 elicitation studies [63].",
      "label": "Background",
      "prob": 0.7723680734634399
    },
    {
      "sentence": "We experimented with different numbers of hidden neurons  : Overall, reconstruction loss decreases for larger models, regularized by the KL-loss, leading to diminishing returns and a decision for  = 512 here.",
      "label": "Background",
      "prob": 0.7723240256309509
    },
    {
      "sentence": "Furthermore, some researchers indicated that participants may struggle to propose gestures, if they are unfamiliar with the gesture design space [9, 12, 46].",
      "label": "Background",
      "prob": 0.7713199853897095
    },
    {
      "sentence": "This gesture map is a 2D plot with a grid of representative body poses shown as small human skeletons.",
      "label": "Background",
      "prob": 0.7672011256217957
    },
    {
      "sentence": "To visualize temporal data, a common representation is a line plot, horizon plot [16], or a small-multiples plot [52].",
      "label": "Background",
      "prob": 0.7665504217147827
    },
    {
      "sentence": "This is an artefact of dimensionality reduction, as we discuss further in Section 8.2",
      "label": "Background",
      "prob": 0.7603051066398621
    },
    {
      "sentence": "Some people noted that they struggled to find a specific pose on the map.",
      "label": "Background",
      "prob": 0.7571152448654175
    },
    {
      "sentence": "Copyrights for components of this work owned by others than the author(s) must be honored.",
      "label": "Background",
      "prob": 0.7569791674613953
    },
    {
      "sentence": "The map supports pan and zoom and accordingly recomputes the grid of landmarks (small skeletons).",
      "label": "Background",
      "prob": 0.7562993168830872
    },
    {
      "sentence": "Thus, researchers devised different ways to distribute the work among people [1, 36].",
      "label": "Background",
      "prob": 0.7529559135437012
    },
    {
      "sentence": "GestureMap enables this: Researchers can explore map areas without data, which may reveal unlikely behavior, or indicate issues with interaction (e.g. anatomically difficult or tiring gestures) or the sensor (e.g. gestures leading to self-occlusion of body parts).",
      "label": "Background",
      "prob": 0.7526871562004089
    },
    {
      "sentence": "On the positive side, the researchers liked the refinement step, where they could reassign proposals to another cluster.",
      "label": "Background",
      "prob": 0.7520017623901367
    },
    {
      "sentence": "Researchers can use it to detect overlapping or distinctive behavior across different referents.",
      "label": "Background",
      "prob": 0.7517331838607788
    },
    {
      "sentence": "GestureMap addresses these needs as its 2D map shows observed gesture proposals and gives an idea of past behavior.",
      "label": "Background",
      "prob": 0.744971513748169
    },
    {
      "sentence": "We can imagine that average gestures calculated with the DBA-algorithm can be used to visualize the non-leaf nodes in the hierarchical tree.",
      "label": "Background",
      "prob": 0.744713544845581
    },
    {
      "sentence": "The progress of the animation can be controlled via a play/pause button and slider.",
      "label": "Background",
      "prob": 0.7424717545509338
    },
    {
      "sentence": "gestures on the map, labelled manually or with help from our clustering tool, to train a classifier.",
      "label": "Background",
      "prob": 0.7417284846305847
    },
    {
      "sentence": "They suggested to increase the visibility of the poses by showing fewer and larger landmarks.",
      "label": "Background",
      "prob": 0.7350314259529114
    },
    {
      "sentence": "Indeed, running k-means revealed some of them: For example, for throw ball k-means also detected throwing with the right hand vs using both hands.",
      "label": "Background",
      "prob": 0.7349362969398499
    },
    {
      "sentence": "Judging Densities and Overlap of Referents.",
      "label": "Background",
      "prob": 0.7321397066116333
    },
    {
      "sentence": "Being able to compute an average gesture enables the use of clustering methods that require average computations.",
      "label": "Background",
      "prob": 0.7307833433151245
    },
    {
      "sentence": "For the crouch referent, everyone distinguished two to three groups of behaviors.",
      "label": "Background",
      "prob": 0.7306246757507324
    },
    {
      "sentence": "Finally, the toolbox in the literature includes several formalized agreement measures [56, 67].",
      "label": "Background",
      "prob": 0.7287349700927734
    },
    {
      "sentence": "Such a recognizer then also could be used in other tools to support sensor feed annotation (e.g. [41]).",
      "label": "Background",
      "prob": 0.7283063530921936
    },
    {
      "sentence": "Examining the map locally, in combination with gesture animations, reveals that some children sat on the floor, some on their heels, some crawled on hands/knees, and others stood with a stooped body posture.",
      "label": "Background",
      "prob": 0.72557532787323
    },
    {
      "sentence": "Video analysis has been the preferred evaluation method, but the annotation of individual video sequences can be timeconsuming [51].",
      "label": "Background",
      "prob": 0.7246283292770386
    },
    {
      "sentence": "A key challenge in visual analytics is the effective visualization of high-dimensional data.",
      "label": "Background",
      "prob": 0.723578155040741
    },
    {
      "sentence": "GestureMap builds on and extends functionalities of previous tools for gesture elicitation: It combines",
      "label": "Background",
      "prob": 0.7235720157623291
    },
    {
      "sentence": "The trajectory of the embedded gesture paths can inform them on specific behavioral characteristics.",
      "label": "Background",
      "prob": 0.7234307527542114
    },
    {
      "sentence": "Regardless of their initial analysis strategy, when asked which feature they would use to group the",
      "label": "Background",
      "prob": 0.7215372323989868
    },
    {
      "sentence": "Now, all participants specifically searched for individual gesture proposals as templates (strategy 2) and used those to initialize the algorithm.",
      "label": "Background",
      "prob": 0.7183035612106323
    },
    {
      "sentence": "In a sense, they searched for outlier behavior based on the path shapes.",
      "label": "Background",
      "prob": 0.7176507115364075
    },
    {
      "sentence": "Other researchers noted that elicitation findings are spread across multiple venues and need to be consolidated [63].",
      "label": "Background",
      "prob": 0.7175413966178894
    },
    {
      "sentence": "A smooth latent space facilitates suitable visualization by reducing jumps in gesture paths.",
      "label": "Background",
      "prob": 0.7163487076759338
    },
    {
      "sentence": "As a community, we could consolidate our findings in a meta-map of many studies, as a sensor data-driven complement to literature surveys [63].",
      "label": "Background",
      "prob": 0.7105164527893066
    },
    {
      "sentence": "This allows researchers to view details on-demand e.g. to reduce the risk of information overload.",
      "label": "Background",
      "prob": 0.7040313482284546
    },
    {
      "sentence": "[68] confirmed that new users do prefer the user-defined gesture set over the one created by experts.",
      "label": "Background",
      "prob": 0.69769287109375
    },
    {
      "sentence": "These pose landmarks give an overview of the poses in the corresponding rectangular map region (Figure 1b 1  ).",
      "label": "Background",
      "prob": 0.6943053007125854
    },
    {
      "sentence": "[48] filled this gap by providing a generic tool to visualize these embeddings.",
      "label": "Background",
      "prob": 0.6925443410873413
    },
    {
      "sentence": "Overall, this indicates the potential of automated clustering, for example, when examining data from open elicitation with no given referents.",
      "label": "Background",
      "prob": 0.6867765784263611
    },
    {
      "sentence": "As Figure 4 shows, the children mostly stuck to their interpretation across multiple repeated trials for that referent, revealing consistency (cf. [5]).",
      "label": "Background",
      "prob": 0.6832642555236816
    },
    {
      "sentence": "Another researcher felt that the map should show more detail so it would be easier to judge differences and transitions of poses.",
      "label": "Background",
      "prob": 0.6819535493850708
    },
    {
      "sentence": "Considering research interests in the elicitation context from the literature, for example, this might support researchers to examine if a participant can remember and repeat the same gesture proposal across multiple trials [40], or if behavior was influenced by a priming effect [6].",
      "label": "Background",
      "prob": 0.6785931587219238
    },
    {
      "sentence": "[63] called for future work to include multiple representations of gestures.",
      "label": "Background",
      "prob": 0.6772617101669312
    },
    {
      "sentence": "As an alternative to drawing, users could demonstrate the gesture in front of the sensor, with a cursor moving on the map live.",
      "label": "Background",
      "prob": 0.6763535737991333
    },
    {
      "sentence": "To the best of our knowledge, GestureMap is the first tool to use a latent variable model to analyze sensor-based motion data in the context of gesture elicitation studies.",
      "label": "Background",
      "prob": 0.6743679046630859
    },
    {
      "sentence": "It provides an overview of the gesture data and introduces a new continuous traceable 2D paths which represent gesture sequences.",
      "label": "Background",
      "prob": 0.6737820506095886
    },
    {
      "sentence": "Given the exploratory nature of the interactions and the diversity in peoples approaches this was done in an inductive approach, leading to the themes in Section 7.2.",
      "label": "Background",
      "prob": 0.6724895238876343
    },
    {
      "sentence": "This live embedding provides a monitoring tool, for example, for participants to see their currently performed gesture (e.g. shown as a cursor/point on the map), possibly to nudge them towards exploring new regions of the behavior space (cf. [65]).",
      "label": "Background",
      "prob": 0.6695253849029541
    },
    {
      "sentence": "These participants noted that the 2D gesture path visualization offers a quick way to spot irregular behavior and that their analysis becomes an active search versus passively watching every gesture individually.",
      "label": "Background",
      "prob": 0.6650331020355225
    },
    {
      "sentence": "We chose k-means, because it readily integrates with the gesture map and the \"variance around mean gesture\" that we introduced in section 4.2.",
      "label": "Background",
      "prob": 0.6640427708625793
    },
    {
      "sentence": "In this way, the gesture map combines a line plots simplicity with the structural expressiveness of a small-multiples visualization [24].",
      "label": "Background",
      "prob": 0.6636861562728882
    },
    {
      "sentence": "GestureMap also offers this, to afford easy examination of a recorded gesture.",
      "label": "Background",
      "prob": 0.66358482837677
    },
    {
      "sentence": "To motivate a concrete example, citetJain2016 showed that observers can distinguish behavior of children and adults.",
      "label": "Background",
      "prob": 0.6587110757827759
    },
    {
      "sentence": "Related tools [24, 41] show a 3D skeleton view with animation.",
      "label": "Background",
      "prob": 0.6583000421524048
    },
    {
      "sentence": "The third visualization is similar to the second, but additionally employs a heat-map to emphasize the time domain.",
      "label": "Background",
      "prob": 0.6565327048301697
    },
    {
      "sentence": "While we focus on researchers as users of this map in this paper, it could also be shown to participants as we described in Section 8.3.",
      "label": "Background",
      "prob": 0.6551241278648376
    },
    {
      "sentence": "When hovering over an element, the corresponding gesture path is shown on the map for a moment.",
      "label": "Background",
      "prob": 0.654478132724762
    },
    {
      "sentence": "GestureMap supports this as it offers a platform to visualize and analyze multiple studies.",
      "label": "Background",
      "prob": 0.6525885462760925
    },
    {
      "sentence": "2) motivated in calls for further improvements, and",
      "label": "Background",
      "prob": 0.6462364792823792
    },
    {
      "sentence": "The analysis concepts introduced in this work are built on previous work spanning HCI, machine learning and visual analytics.",
      "label": "Background",
      "prob": 0.6429978609085083
    },
    {
      "sentence": "GestureMap supports this as well: For instance, Figure 3 depicts a scatter plot projected on the gesture map.",
      "label": "Background",
      "prob": 0.6426324844360352
    },
    {
      "sentence": "These clusters can help researchers to form a mental model of the main poses that are characteristic for a group of gesture sequences.",
      "label": "Background",
      "prob": 0.6390113830566406
    },
    {
      "sentence": "This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt).",
      "label": "Background",
      "prob": 0.6314090490341187
    },
    {
      "sentence": "a replacement of others: As a flexible tool, GestureMap can be extended to additionally display further such measures (e.g. the one by Vatavu [59]) to support researchers with the analysis.",
      "label": "Background",
      "prob": 0.6313115954399109
    },
    {
      "sentence": "[66] to elicit users interaction preferences for new systems.",
      "label": "Background",
      "prob": 0.6289749145507812
    },
    {
      "sentence": "When asked why they keep returning to the scatter plot, they said that it provided more detail and that density can also be estimated from scatter points.",
      "label": "Background",
      "prob": 0.6216526627540588
    },
    {
      "sentence": "and raising an arm (Figure 2A).",
      "label": "Background",
      "prob": 0.6206263303756714
    },
    {
      "sentence": "Human-centered computing  Visualization systems and tools ; HCI design and evaluation methods .",
      "label": "Background",
      "prob": 0.6187490820884705
    },
    {
      "sentence": "The extensibility of GestureMap further encourages future work to employ machine learning as a tool for analysis of human behavior.",
      "label": "Background",
      "prob": 0.612255334854126
    },
    {
      "sentence": "An efficient analysis becomes even more important as large-scale gesture data sets can be collected online, for example, through cloud elicitation tools [2].",
      "label": "Background",
      "prob": 0.611197292804718
    },
    {
      "sentence": "To find behavioral patterns, it employs a variation of the smallmultiples plot [52] and an interactive hierarchical clustering interface visualized in a tree layout.",
      "label": "Background",
      "prob": 0.6090490221977234
    },
    {
      "sentence": "For throw ball , everyone found at least three (left/right/both handed throwing).",
      "label": "Background",
      "prob": 0.6055814623832703
    },
    {
      "sentence": "GestureMap realises this by linking the 2D map and the 3D skeleton.",
      "label": "Background",
      "prob": 0.6043011546134949
    },
    {
      "sentence": "[3], next to the childrens proposals from Vatavu [59].",
      "label": "Background",
      "prob": 0.6027065515518188
    },
    {
      "sentence": "1) Shape driven analysis: Some started by skimming through gestures to get an overview of their different path shapes on the map.",
      "label": "Background",
      "prob": 0.6024032235145569
    },
    {
      "sentence": "Using details on demand, users can hover over points to see the corresponding pose skeleton (Figure1b 2  ), and referent, participant and trial number in the detail view (Figure1b 6  ).",
      "label": "Background",
      "prob": 0.5994364023208618
    },
    {
      "sentence": "G  denotes the set of all gestures elicited for referent  .",
      "label": "Background",
      "prob": 0.5980613231658936
    },
    {
      "sentence": "Indeed, this observation can be explained by the longer referent list (58 referents in [3] vs 15 in [59]).",
      "label": "Background",
      "prob": 0.5974006652832031
    },
    {
      "sentence": "2) automatic computation of elicitation metrics, and",
      "label": "Background",
      "prob": 0.5936516523361206
    },
    {
      "sentence": "Additionally, color codes facilitate the comparison of behavior across different referents.",
      "label": "Background",
      "prob": 0.5875900983810425
    },
    {
      "sentence": "Together, this feedback motivates a changeable grid size (our zoom was implemented to always keep an 11  11 grid).",
      "label": "Background",
      "prob": 0.5843318104743958
    },
    {
      "sentence": "Furthermore, our learned representation supports gesture simulation useful to examine regions of the behavior space that were not covered by participants.",
      "label": "Background",
      "prob": 0.5817855596542358
    },
    {
      "sentence": "Second, they provided a visualization where only the moving joint is drawn on the canvas.",
      "label": "Background",
      "prob": 0.580661416053772
    },
    {
      "sentence": "We encouraged them to think out loud and occasionally asked questions to better understand actions.",
      "label": "Background",
      "prob": 0.5786506533622742
    },
    {
      "sentence": "Only a few used the left hand.",
      "label": "Background",
      "prob": 0.5770123600959778
    },
    {
      "sentence": "We then measure the DTW distance of every gesture proposal   for a referent  to the computed average gesture (i.e. barycenter) g  for  .",
      "label": "Background",
      "prob": 0.5765424370765686
    },
    {
      "sentence": "We motivate this choice by interpretability of the resulting centroids, versus the abstract representations in the hierarchical approach: In particular, the centroids (i.e. average/barycenter gestures) are more compatible with our 2D gesture map, on which they could be displayed as paths.",
      "label": "Background",
      "prob": 0.576178789138794
    },
    {
      "sentence": "For full details, we provide the training scripts and model comparisons on the project website.",
      "label": "Background",
      "prob": 0.5680022835731506
    },
    {
      "sentence": "The variance plot in GestureMap (Figure 3 right) indicates that proposals for crouch and draw flower vary more than for draw circle and draw square .",
      "label": "Background",
      "prob": 0.5658418536186218
    },
    {
      "sentence": "We further see, for example, sitting (Figure 2B), clapping (Figure 2D),",
      "label": "Background",
      "prob": 0.5649787783622742
    },
    {
      "sentence": "Second, Section 7 follows the Usage strategy and reports on a user study with HCI researchers.",
      "label": "Background",
      "prob": 0.5639771223068237
    },
    {
      "sentence": "Moreover, this approach yields a one-number summary without a logistic regression model on top.",
      "label": "Background",
      "prob": 0.5639098286628723
    },
    {
      "sentence": "We cached expensive computations such as the computed average sequences and distances matrices on MongoDB [21] to .",
      "label": "Background",
      "prob": 0.5564745664596558
    },
    {
      "sentence": "We thus conceptualized the gesture map to enable researchers to seamlessly cycle between the detection of new observations and the assessment of supporting evidence.",
      "label": "Background",
      "prob": 0.5561126470565796
    },
    {
      "sentence": "[24] which also focuses on the analysis of gesture elicitation studies.",
      "label": "Background",
      "prob": 0.551794707775116
    },
    {
      "sentence": "Summarising their initial experience, one person said: Although, the learning curve [...] is steep, once you understand the core concepts, this tool offers a great overview of the entire behavior that is captured in the dataset.",
      "label": "Background",
      "prob": 0.5485305786132812
    },
    {
      "sentence": "Their calculations and analyses are based on hand-engineered features.",
      "label": "Background",
      "prob": 0.5412988066673279
    },
    {
      "sentence": "The key local observation in elicitation data is to examine individual gesture proposals .",
      "label": "Background",
      "prob": 0.5383466482162476
    },
    {
      "sentence": "Scatter points on top of the pose grid enable researchers to quickly identify which general poses were observed in the data.",
      "label": "Background",
      "prob": 0.5379990339279175
    },
    {
      "sentence": "The idea of clustering gesture elicitation data is motivated by two aspects:",
      "label": "Background",
      "prob": 0.5372456312179565
    },
    {
      "sentence": "The scatterplot may help researchers to detect outlier body poses, while the density plot reveals regions with recorded data.",
      "label": "Background",
      "prob": 0.5359485149383545
    },
    {
      "sentence": "GestureMap empowers researchers to compare data across studies (cf. Section 6).",
      "label": "Background",
      "prob": 0.5328224301338196
    },
    {
      "sentence": "Here we outline further ideas enabled or supported by GestureMap .",
      "label": "Background",
      "prob": 0.5302459001541138
    },
    {
      "sentence": "While the concepts introduced in this paper also enable researchers to better annotate sequences, our focus lies in particular on the exploration of elicited gesture data.",
      "label": "Background",
      "prob": 0.5275987386703491
    },
    {
      "sentence": "8.3.3 Enabling Analysis of Unseen Behavior.",
      "label": "Background",
      "prob": 0.526389479637146
    },
    {
      "sentence": "This section briefly describes gesture elicitation studies, followed by an overview of tools that support researchers across different tasks involved in analyzing elicitation data.",
      "label": "Background",
      "prob": 0.5254287719726562
    },
    {
      "sentence": "2) In an exploratory, manual analysis task people were prompted to use GestureMap to identify groups of behaviors in the gesture proposals for two referents.",
      "label": "Background",
      "prob": 0.5240931510925293
    },
    {
      "sentence": "It further leverages the computation of average gestures to enable researchers to",
      "label": "Background",
      "prob": 0.5209218263626099
    },
    {
      "sentence": "Central to gesture elicitation studies is an in-depth analysis of the proposed data to find common behavior.",
      "label": "Background",
      "prob": 0.5195791125297546
    },
    {
      "sentence": "In particular, we outline existing analysis concepts for high-dimensional data.",
      "label": "Background",
      "prob": 0.5194014310836792
    },
    {
      "sentence": "Six were familiar with gesture elicitation studies, the other two were interested in analysing gesture sensor data.",
      "label": "Background",
      "prob": 0.5188890099525452
    },
    {
      "sentence": "[41] created a tool to analyze recordings created by a Kinect camera sensor.",
      "label": "Background",
      "prob": 0.5120238065719604
    },
    {
      "sentence": "In contrast, for instance, gestures proposed for crouch cover a different region (pink).",
      "label": "Background",
      "prob": 0.5103574991226196
    },
    {
      "sentence": "One person suggested to create an outlier group for these.",
      "label": "Background",
      "prob": 0.5058150291442871
    },
    {
      "sentence": "Here, we use k-means in particular.",
      "label": "Background",
      "prob": 0.5046289563179016
    },
    {
      "sentence": "They also said that points were visually closer to the data (point=pose).",
      "label": "Background",
      "prob": 0.5037593245506287
    },
    {
      "sentence": "We reflect on other possible choices in our discussion.",
      "label": "Background",
      "prob": 0.5029831528663635
    },
    {
      "sentence": "This view shows different metrics, namely variances around the average gesture sequence per selected referent (Section 4.2), the distributions of DTW distances of proposals to their average gesture sequence, and nearest neighbor distances for a selected gesture.",
      "label": "Background",
      "prob": 0.4981641471385956
    },
    {
      "sentence": "Developing an Overview of the Gesture Space.",
      "label": "Background",
      "prob": 0.4956001341342926
    },
    {
      "sentence": "Using expectations about possible behavior for a gesture proposal (e.g. left vs right hand throwing), they examined scatter points in those map regions that based on the landmark skeletons encoded related poses.",
      "label": "Background",
      "prob": 0.4930725693702698
    },
    {
      "sentence": "[34] identified four evaluation strategies for toolkit contributions.",
      "label": "Background",
      "prob": 0.49110347032546997
    },
    {
      "sentence": "3.1.3 2D Map Overlays (Figure 1b 1  , Figure 1c).",
      "label": "Background",
      "prob": 0.4906233847141266
    },
    {
      "sentence": "Spotting Clusters and Outliers.",
      "label": "Background",
      "prob": 0.4895206093788147
    },
    {
      "sentence": "While there exist many dimension reduction techniques [20, 32, 38, 54, 55], we use a Variational Autoencoder [30] to reduce the dimensions of the raw sensor data.",
      "label": "Background",
      "prob": 0.48947837948799133
    },
    {
      "sentence": "1) represent gesture groups with one gesture;",
      "label": "Background",
      "prob": 0.48881807923316956
    },
    {
      "sentence": "This view either shows the raw skeleton recording or a reconstructed skeleton.",
      "label": "Background",
      "prob": 0.48566973209381104
    },
    {
      "sentence": "Some also found the kicking behavior as described in Section 6.",
      "label": "Background",
      "prob": 0.4822326600551605
    },
    {
      "sentence": "CHI 21, May 813, 2021, Yokohama, Japan",
      "label": "Background",
      "prob": 0.4795061945915222
    },
    {
      "sentence": "Overall, after being asked to give a final verdict over the interactive clustering feature, all deemed it important.",
      "label": "Background",
      "prob": 0.4775969982147217
    },
    {
      "sentence": "[19], we used a weight term to modulate the mix of KL-loss and reconstruction loss in early training.",
      "label": "Background",
      "prob": 0.476092129945755
    },
    {
      "sentence": "Without knowing anything about the referents, Figure 6 already reveals that one dataset [3] ( blue ) covers a larger region than the other [59] ( orange ).",
      "label": "Background",
      "prob": 0.4746173918247223
    },
    {
      "sentence": "The analysis concept is structured further by differentiating between global observations and local observations.",
      "label": "Background",
      "prob": 0.46722906827926636
    },
    {
      "sentence": "Here we describe how users can interact with the map.",
      "label": "Background",
      "prob": 0.4631199240684509
    },
    {
      "sentence": "The following paragraphs further introduce and motivate the features.",
      "label": "Background",
      "prob": 0.4592466652393341
    },
    {
      "sentence": "Overall, we implemented all UI views and interactions conceptually described in Section 3.",
      "label": "Background",
      "prob": 0.45910149812698364
    },
    {
      "sentence": "1) We introduced GestureMap (20 minutes), with a concept presentation, a guided walk through the tool and UI, and opportunities for questions.",
      "label": "Background",
      "prob": 0.4584619104862213
    },
    {
      "sentence": "To determine consensus for a referent they calculated the pairwise distances across all gesture proposals for this referent.",
      "label": "Background",
      "prob": 0.45746371150016785
    },
    {
      "sentence": "We introduce the concept of an average gesture sequence as a new computational capability in the context of gesture elicitation.",
      "label": "Background",
      "prob": 0.4417264461517334
    },
    {
      "sentence": "Exploratory analysis seeks to uncover structural patterns in the dataset, identify anomalies, and single-out outliers [53].",
      "label": "Background",
      "prob": 0.4415275752544403
    },
    {
      "sentence": "2) Suitably visualising the projected data, considering the analysts tasks and goals.",
      "label": "Background",
      "prob": 0.4397083520889282
    },
    {
      "sentence": "The interviews lasted 80 minutes and were conducted via screensharing using Skype/Zoom, with GestureMap hosted online such that people could use it on their own computer.",
      "label": "Background",
      "prob": 0.43380647897720337
    },
    {
      "sentence": "Thus, the map reveals the space of poses elicited by Vatavu [59] at a glance: For example, their referents included crouch , draw a flower , draw a circle , draw a square , applaud or raise your hands , which all match the poses in our map.",
      "label": "Background",
      "prob": 0.42884501814842224
    },
    {
      "sentence": "This is repeated for several referents.",
      "label": "Background",
      "prob": 0.42538583278656006
    },
    {
      "sentence": "When study participants paused their exploration for a longer period, we inquired why that was the case.",
      "label": "Background",
      "prob": 0.4245372414588928
    },
    {
      "sentence": "This typically involves two steps:",
      "label": "Background",
      "prob": 0.4244213402271271
    },
    {
      "sentence": "To achieve this, they employed Dynamic Time Warping (DTW) distance computations to define a consensus measure: They considered two gesture sequences g  and g  as similar if the DTW distance was below a threshold   ( g  , g  )   .",
      "label": "Background",
      "prob": 0.4239144027233124
    },
    {
      "sentence": "For the backend we used the Flask framework [10] and Pandas [11] to handle the data transformations and queries.",
      "label": "Method",
      "prob": 0.48275840282440186
    },
    {
      "sentence": "Thus, GestureMap visually reveals that people interpreted crouch in different ways, matching the high variance (Figure 3",
      "label": "Background",
      "prob": 0.42165932059288025
    },
    {
      "sentence": "For further discussions on the comparison of these two systems we refer to section 8.2.2.",
      "label": "Other",
      "prob": 0.45004066824913025
    },
    {
      "sentence": "1) Projecting the data to 2D for display on a screen.",
      "label": "Background",
      "prob": 0.4160296618938446
    },
    {
      "sentence": "2) Position driven analysis: In contrast, other participants focused entirely on the scatter points as template poses.",
      "label": "Result",
      "prob": 0.4304659962654114
    },
    {
      "sentence": "[35] used a cartographic approach to compare and analyze learned embedding spaces.",
      "label": "Method",
      "prob": 0.4352474808692932
    },
    {
      "sentence": "We consider four existing datasets: One explicit gesture elicitiation study by Vatavu [59], plus three datasets collected for gesture recognition systems [3, 7, 17].",
      "label": "Background",
      "prob": 0.40720653533935547
    },
    {
      "sentence": "We trained the VAE on the poses (frames) of the mentioned dataset [59] which has 60 dimensions (20 body joints  ,, ).",
      "label": "Background",
      "prob": 0.4064587652683258
    },
    {
      "sentence": "We therefore combine an abstract 2D mapping with a grid of representative 3D skeletons to give analysts a visual overview of the proposed gestures.",
      "label": "Background",
      "prob": 0.4058217406272888
    },
    {
      "sentence": "For further technical details we refer the reader to the related work [43].",
      "label": "Other",
      "prob": 0.4490288197994232
    },
    {
      "sentence": "We motivate the conceptual features via related work as summarized in Table 1 and elaborate on them in the following sections.",
      "label": "Background",
      "prob": 0.40435460209846497
    },
    {
      "sentence": "First, they used a 3D animation of a Kinect skeleton.",
      "label": "Background",
      "prob": 0.4041270911693573
    },
    {
      "sentence": "Following a cartographic approach [47], and in line with 2D projections in visual analytics (e.g. [27, 64]), we use a map metaphor to visually guide analysts through the elicited gesture space.",
      "label": "Method",
      "prob": 0.45513930916786194
    },
    {
      "sentence": "We combine interactive k-means clustering, automatic metric computation, a new visualization, and analysis concepts to provide an integrated platform.",
      "label": "Method",
      "prob": 0.5030596852302551
    },
    {
      "sentence": "3.1.5 Linked Animations of Gestures.",
      "label": "Other",
      "prob": 0.49486634135246277
    },
    {
      "sentence": "Formally, let the set of all  individual gesture poses in the dataset be denoted by G = {   |  = [ 1 , . . .,  ]} ,    R  where  is the dimensionality of the raw sensor data (in our case D=20).",
      "label": "Background",
      "prob": 0.3894844949245453
    },
    {
      "sentence": "These challenges motivate our work on new quantitative methods and tools for analyzing elicitation data.",
      "label": "Objective",
      "prob": 0.5370967388153076
    },
    {
      "sentence": "Density plots then offer a visualization of the most frequent gesture poses.",
      "label": "Background",
      "prob": 0.3868832290172577
    },
    {
      "sentence": "1) gesture modeling and visualization,",
      "label": "Background",
      "prob": 0.3843458592891693
    },
    {
      "sentence": "Figure 1 shows the UI; the following sections refer to the numbers in the figure.",
      "label": "Background",
      "prob": 0.3843187987804413
    },
    {
      "sentence": "8.3.4 Supporting Live Exploration and Monitoring.",
      "label": "Other",
      "prob": 0.4610585868358612
    },
    {
      "sentence": "Researchers then analyse these gesture proposals, compute measures to identify common proposals (e.g. [59, 66]), and decide on a set of gestures to be used in an interactive system, typically composed of the gestures with high agreement among participants (e.g. [56, 67]).",
      "label": "Background",
      "prob": 0.3810026943683624
    },
    {
      "sentence": "8.3.2 Enabling Map-based Gesture Authoring.",
      "label": "Other",
      "prob": 0.4695674777030945
    },
    {
      "sentence": "Our variance measure also reflects this (Aloba adults 23 . 70, children 44 . 89; Vatavu children 54 . 94).",
      "label": "Other",
      "prob": 0.4426015317440033
    },
    {
      "sentence": "Addressing this, we extend the computational toolbox for analyzing gesture elicitation data with these contributions:",
      "label": "Background",
      "prob": 0.3775925040245056
    },
    {
      "sentence": "We introduce a structured analysis approach based on a learned 2D gesture map, as realised in GestureMap .",
      "label": "Background",
      "prob": 0.3774734139442444
    },
    {
      "sentence": "Here we describe the key implementation aspects.",
      "label": "Background",
      "prob": 0.37690719962120056
    },
    {
      "sentence": "To address this, Vatavu [59] has recently proposed a new, datadriven approach: It employs a distance measure as an objective basis for assessing consensus in elicitation studies.",
      "label": "Background",
      "prob": 0.3764491677284241
    },
    {
      "sentence": "We return to ideas for improvements in our discussion.",
      "label": "Background",
      "prob": 0.37300899624824524
    },
    {
      "sentence": "Our map view affords different plots on top of it, such as:",
      "label": "Background",
      "prob": 0.3729996681213379
    },
    {
      "sentence": "We used a Variational Autoencoder (VAE) [13] to embed the data as a 2D gesture map.",
      "label": "Method",
      "prob": 0.5276365280151367
    },
    {
      "sentence": "4) The session concluded with a semi-structured interview of at least ten minutes.",
      "label": "Background",
      "prob": 0.37081778049468994
    },
    {
      "sentence": "Color coding the cluster results can be done quickly.",
      "label": "Result",
      "prob": 0.43293142318725586
    },
    {
      "sentence": "This method was then specifically adapted to include gesture proposals to control surface tabletop computers [67].",
      "label": "Background",
      "prob": 0.3678147494792938
    },
    {
      "sentence": "Overall, the researchers felt comfortable with grouping the proposals based on the path shapes.",
      "label": "Result",
      "prob": 0.5087741613388062
    },
    {
      "sentence": "Here, we highlight model and clustering aspects to consider.",
      "label": "Background",
      "prob": 0.3635556697845459
    },
    {
      "sentence": "3) explicitly requested from future work.",
      "label": "Background",
      "prob": 0.3602916896343231
    },
    {
      "sentence": "The frontend was implemented with NodeJS [18] and React [15].",
      "label": "Background",
      "prob": 0.3586866855621338
    },
    {
      "sentence": "This feature helps to adjust the viewport to support exploration of datadense areas, and deal with the fact that landmark representations are discrete indicators for the continuous space.",
      "label": "Objective",
      "prob": 0.5904760956764221
    },
    {
      "sentence": "3) interactive clustering to provide an integrated analysis platform.",
      "label": "Background",
      "prob": 0.3557474613189697
    },
    {
      "sentence": "Using overlays in GestureMap , we can identify similarity and differences between gestures across referents: For example, Figure 3 (left) shows that crouch, draw circle, draw flower, draw square share common behavior; their scatter points largely overlap in the region that encodes raised arm behavior.",
      "label": "Result",
      "prob": 0.4327031373977661
    },
    {
      "sentence": "8.2.1 Smoothness of the Latent Space.",
      "label": "Other",
      "prob": 0.35539042949676514
    },
    {
      "sentence": "Here, we describe the map concept in more detail.",
      "label": "Background",
      "prob": 0.3445492684841156
    },
    {
      "sentence": "As an example strategy, to detect the subgroup behavior for the throw ball referent, we quickly skimmed through the gestures using the map and visually identified rough patterns.",
      "label": "Method",
      "prob": 0.5230112075805664
    },
    {
      "sentence": "The gesture map which we propose in this work facilitates a richer exploration of the behavior space using machine learned features for the gesture poses.",
      "label": "Objective",
      "prob": 0.4982280433177948
    },
    {
      "sentence": "3.1.4 Linked Views of Postures.",
      "label": "Other",
      "prob": 0.5697479844093323
    },
    {
      "sentence": "The features in GestureMap were informed by close examination of the literature on gesture elicitation and related concepts and tools: We collected features",
      "label": "Background",
      "prob": 0.32965710759162903
    },
    {
      "sentence": "In addition, we included further ideas.",
      "label": "Result",
      "prob": 0.3309747576713562
    },
    {
      "sentence": "We trained for 2000 epochs with Adam [29] (lr=3   5 ).",
      "label": "Background",
      "prob": 0.3240985870361328
    },
    {
      "sentence": "We took notes and compiled a report from this material.",
      "label": "Result",
      "prob": 0.36447030305862427
    },
    {
      "sentence": "This has three practical values, which complement our tool:",
      "label": "Result",
      "prob": 0.3249465525150299
    },
    {
      "sentence": "[24] used an interactive hierarchical clustering approach with complete-linkage.",
      "label": "Method",
      "prob": 0.4862031638622284
    },
    {
      "sentence": "We asked people to use the interactive clustering tool based on their observations in the first task.",
      "label": "Method",
      "prob": 0.3999269902706146
    },
    {
      "sentence": "We used a server-client architecture.",
      "label": "Method",
      "prob": 0.49831056594848633
    },
    {
      "sentence": "We first focus on the dataset by Vatavu [59] that consists of 1312 full body gestures elicited from children aged 3-6, recorded with a Kinect sensor.",
      "label": "Method",
      "prob": 0.3130910396575928
    },
    {
      "sentence": "GestureMap also supports such analysis, as outlined here:",
      "label": "Other",
      "prob": 0.5345388054847717
    },
    {
      "sentence": "Five were familiar with machine learning.",
      "label": "Result",
      "prob": 0.3945367932319641
    },
    {
      "sentence": "Our work builds on this idea, extends its data-driven perspective with a visual analytics tool, and introduces a new measure fitting this visualization.",
      "label": "Objective",
      "prob": 0.5869761109352112
    },
    {
      "sentence": "Users could also select recorded",
      "label": "Other",
      "prob": 0.44503697752952576
    },
    {
      "sentence": "We repeated this ten times and made these observations:",
      "label": "Result",
      "prob": 0.3464221954345703
    },
    {
      "sentence": "To further evaluate GestureMap , we recruited eight HCI researchers (7 male, 1 female) from three universities via e-mail for remote think-alound and interview sessions.",
      "label": "Result",
      "prob": 0.3154996633529663
    },
    {
      "sentence": "Comparing Referents and Regions.",
      "label": "Result",
      "prob": 0.31220707297325134
    },
    {
      "sentence": "Table 1 shows an overview of the relation to related work.",
      "label": "Result",
      "prob": 0.47420555353164673
    },
    {
      "sentence": "For plotting, we use the PlotlyJs library [22].",
      "label": "Other",
      "prob": 0.4461350739002228
    },
    {
      "sentence": "We then inspected the mix of original referents present in the gestures assigned to each found cluster.",
      "label": "Method",
      "prob": 0.4203380048274994
    },
    {
      "sentence": "We asked the researchers to analyze the proposals for crouch and throw ball .",
      "label": "Result",
      "prob": 0.3166945278644562
    },
    {
      "sentence": "With peoples consent we recorded the interviews.",
      "label": "Result",
      "prob": 0.36441853642463684
    },
    {
      "sentence": "2) assess consensus as variance around this average gesture; and",
      "label": "Result",
      "prob": 0.3635687828063965
    },
    {
      "sentence": "[24] proposed to use interactive hierarchical clustering.",
      "label": "Method",
      "prob": 0.32786864042282104
    },
    {
      "sentence": "PyTorch [42] was used to develop the embedding model.",
      "label": "Method",
      "prob": 0.5347669720649719
    },
    {
      "sentence": "Here demonstrate the use of GestureMap in a walkthrough of an explorative analysis: Examining the gesture map, the center (Figure 2C) reveals start/end poses (standing upright, arms at rest).",
      "label": "Result",
      "prob": 0.5036492347717285
    },
    {
      "sentence": "We demonstrate this by creating a gesture map using four datasets [3, 8, 17, 59].",
      "label": "Other",
      "prob": 0.40856343507766724
    },
    {
      "sentence": "They included three visualizations.",
      "label": "Other",
      "prob": 0.3999897241592407
    },
    {
      "sentence": "8.3.1 Supporting Meta-Analysis and Consolidation.",
      "label": "Other",
      "prob": 0.6351384520530701
    },
    {
      "sentence": "Overall, we see this approach as an additional measure, not",
      "label": "Result",
      "prob": 0.3627135455608368
    },
    {
      "sentence": "We implemented GestureMap as an analysis tool that integrates the described concepts of both the interactive gesture map (Section 3) and the DBA-based computations (Section 4).",
      "label": "Method",
      "prob": 0.5697280168533325
    },
    {
      "sentence": "When we asked the participants what the main aspect was that they used to determine interesting behavioral patterns, we observed diverse analysis strategies, but we broadly highlight two main ones:",
      "label": "Result",
      "prob": 0.5193062424659729
    },
    {
      "sentence": "Figure 5 shows all 20 proposals for jump from the data by Aloba et al.",
      "label": "Other",
      "prob": 0.5610432624816895
    },
    {
      "sentence": "The most similar work to ours is GestureAnalyzer by Jang et al.",
      "label": "Other",
      "prob": 0.6459288597106934
    },
    {
      "sentence": "In contrast, we experimented with the k-means algorithm, using DBA to calculate the centroids.",
      "label": "Method",
      "prob": 0.4882603585720062
    },
    {
      "sentence": "3) In a more confirmatory, automatic analysis task we asked them to build on their gained insights to initialize the clustering algorithm and refine the automatic clustering results.",
      "label": "Method",
      "prob": 0.4562084674835205
    },
    {
      "sentence": "In this work, we adapt similar visualization concepts with the goal to create an interpretable gesture space.",
      "label": "Objective",
      "prob": 0.7108747363090515
    },
    {
      "sentence": "1) proposed in related work,",
      "label": "Other",
      "prob": 0.42085108160972595
    },
    {
      "sentence": "With this work, we contribute to the vision of more widespread use of applicable computational methods in HCI, also to support more extensive and cost-efficient large-scale, data-driven HCI work.",
      "label": "Objective",
      "prob": 0.7373347282409668
    },
    {
      "sentence": "3) cluster gestures automatically.",
      "label": "Result",
      "prob": 0.27727746963500977
    },
    {
      "sentence": "3.1.1 3D Skeleton View (Figure 1b 2  ).",
      "label": "Other",
      "prob": 0.5436909794807434
    },
    {
      "sentence": "We again used the dataset by Vatavu [59].",
      "label": "Other",
      "prob": 0.5483952760696411
    },
    {
      "sentence": "As a second example, we compared behavior diversity across datasets.",
      "label": "Result",
      "prob": 0.5776397585868835
    },
    {
      "sentence": "In another experiment, we applied clustering to look for patterns within a referent: As mentioned, referents such as throw ball and crouch contained distinct patterns, revealed on the map.",
      "label": "Method",
      "prob": 0.5998620986938477
    },
    {
      "sentence": "Next, they were asked to initialize the clustering algorithm using their knowledge from the previous task.",
      "label": "Method",
      "prob": 0.5493542551994324
    },
    {
      "sentence": "3.1.2 2D Map View (Figure 1b 1  ).",
      "label": "Other",
      "prob": 0.6135436296463013
    },
    {
      "sentence": "Finally, we report the variance of these DTW distances as a measure of consensus.",
      "label": "Result",
      "prob": 0.5378255248069763
    },
    {
      "sentence": "7.2.3 Details of the Gesture Map View.",
      "label": "Other",
      "prob": 0.7051398158073425
    },
    {
      "sentence": "Also related to our work are tools to analyze and visualize machine learned representations of complex data: Deep learning models are capable of learning human-understandable features of high-dimensional data: For example, Kingma and Welling [30] and Lawrence [33] sample multiple points from the learned space and visualize them to demonstrate that the learned space is continuous and smooth, but without providing interaction functionalities.",
      "label": "Result",
      "prob": 0.403007447719574
    },
    {
      "sentence": "The interviews had four parts:",
      "label": "Result",
      "prob": 0.3983750343322754
    },
    {
      "sentence": "Abstracting with credit is permitted.",
      "label": "Other",
      "prob": 0.7607738375663757
    },
    {
      "sentence": "The gesture elicitation paradigm was first introduced by Wobbrock et al.",
      "label": "Other",
      "prob": 0.6565297245979309
    },
    {
      "sentence": "For preprocessing, we followed the original authors [59] but left out the resampling step.",
      "label": "Method",
      "prob": 0.6368563175201416
    },
    {
      "sentence": "We next describe the technical approach in more detail.",
      "label": "Method",
      "prob": 0.45715761184692383
    },
    {
      "sentence": "ACM ISBN 978-1-4503-8096-6/21/05..",
      "label": "Other",
      "prob": 0.7817044258117676
    },
    {
      "sentence": "Concretely, we ran the clustering with 15 sequences chosen randomly.",
      "label": "Method",
      "prob": 0.4069487452507019
    },
    {
      "sentence": "5.1.3 3D Skeleton View Figure 1b 2  , Figure 1b 4  .",
      "label": "Other",
      "prob": 0.6498304605484009
    },
    {
      "sentence": "2021 Copyright held by the owner/author(s).",
      "label": "Other",
      "prob": 0.7865018248558044
    },
    {
      "sentence": "We adapted the architecture from Spurr et al.",
      "label": "Other",
      "prob": 0.6241175532341003
    },
    {
      "sentence": "To copy otherwise, or republish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.",
      "label": "Other",
      "prob": 0.7741380333900452
    },
    {
      "sentence": "We employ the DTW Barycenter Averaging (DBA) algorithm by Petitjean et al.",
      "label": "Other",
      "prob": 0.6708232164382935
    },
    {
      "sentence": "As larger data sets are expected in the future [1, 24, 41], we also provide an interactive clustering method to reduce manual workload for identifying similar gesture (sub)groups.",
      "label": "Method",
      "prob": 0.7028299570083618
    },
    {
      "sentence": "Therefore, to demonstrate our proposed clustering analysis we removed the referent labels and then evaluated if k-means finds clusters that match the original referents.",
      "label": "Method",
      "prob": 0.551300585269928
    },
    {
      "sentence": "Throughout the interview we noticed that all participants preferred the scatter plot over the density plot.",
      "label": "Result",
      "prob": 0.7654393315315247
    },
    {
      "sentence": "[43] to compute an average gesture: Intuitively, this algorithm first aligns an initial sequence with every sequence in the set of gesture proposals, before computing a centroid (barycenter) for each aligned coordinate.",
      "label": "Method",
      "prob": 0.6533504724502563
    },
    {
      "sentence": "Formally, this is noted as:",
      "label": "Other",
      "prob": 0.7516193389892578
    },
    {
      "sentence": "We follow their perspective to evaluate GestureMap , combining two such strategies: First, here we follow the Demonstration strategy and provide a detailed analysis of examples on elicitation data from related work.",
      "label": "Method",
      "prob": 0.5473037362098694
    },
    {
      "sentence": "7.2.5 Manually Forming Clusters.",
      "label": "Other",
      "prob": 0.6166887879371643
    },
    {
      "sentence": "GestureMap and further materials are available on the project website: https://osf.io/dzn5g/",
      "label": "Other",
      "prob": 0.823172390460968
    },
    {
      "sentence": "3.3.4 Examining Unseen Poses.",
      "label": "Other",
      "prob": 0.7455735802650452
    },
    {
      "sentence": "We then chose  correspondingly.",
      "label": "Result",
      "prob": 0.35032564401626587
    },
    {
      "sentence": "As our key contribution, we presented a set of visualization and analysis concepts for gesture elicitation data and a tool that implements them: GestureMap is the first visual analytics tool for gesture elicitation which directly visualises the space of gestures, using a learned 2D embedding.",
      "label": "Method",
      "prob": 0.6468658447265625
    },
    {
      "sentence": "Villarreal-Narvaez et al.",
      "label": "Other",
      "prob": 0.8319438695907593
    },
    {
      "sentence": "3.2.2 Learning a Gesture Map.",
      "label": "Other",
      "prob": 0.7878992557525635
    },
    {
      "sentence": "Finally, to report a measure independent of the threshold value  , they used a logistic regression model to determine the consensus for a range of normalized threshold values and reported the growth rate as an indication of the overall consensus.",
      "label": "Method",
      "prob": 0.4619119465351105
    },
    {
      "sentence": "5.1.2 Experiment View Figure 1b 3  .",
      "label": "Other",
      "prob": 0.7990810871124268
    },
    {
      "sentence": "https://doi.org/10.1145/3411764.3445765",
      "label": "Other",
      "prob": 0.8627009987831116
    },
    {
      "sentence": "One successful method that has seen widespread use is the elicitation study paradigm [66], which helps HCI researchers and practitioners to explore the space of possible and intuitive or guessable (gesture) commands: Participants are shown a referent (often a system action, e.g. volume up ) and are asked to propose and perform a gesture they would use for it (e.g. turn wrist right ).",
      "label": "Method",
      "prob": 0.7143219113349915
    },
    {
      "sentence": "5.1.1 Gesture Map Figure 1b 1  .",
      "label": "Other",
      "prob": 0.8352081179618835
    },
    {
      "sentence": "5.1.4 Statistics View Figure 1b 5  .",
      "label": "Other",
      "prob": 0.8105301260948181
    },
    {
      "sentence": "7.2.2 Statistical Plot Overlays.",
      "label": "Other",
      "prob": 0.7821096777915955
    },
    {
      "sentence": "5.1.5 Cluster View Figure 1b 6  .",
      "label": "Other",
      "prob": 0.809388279914856
    },
    {
      "sentence": "Publication rights licensed to ACM.",
      "label": "Other",
      "prob": 0.8686001300811768
    },
    {
      "sentence": "REFERENCES",
      "label": "Other",
      "prob": 0.8192349076271057
    },
    {
      "sentence": "3.4.1 Global Observations.",
      "label": "Other",
      "prob": 0.8555047512054443
    },
    {
      "sentence": "We defined a consensus measure on this variability (Section 4.2): Comparing this variability between all referents, our results largely agree with Vatavu [59]: In particular, applaud, fly like a bird and hands up show high consensus while climb ladder, crouch, turn around have low consensus.",
      "label": "Result",
      "prob": 0.7923374176025391
    },
    {
      "sentence": "In line with Fu et al.",
      "label": "Other",
      "prob": 0.8021535277366638
    },
    {
      "sentence": "In a subsequent study, Morris et al.",
      "label": "Other",
      "prob": 0.8221391439437866
    },
    {
      "sentence": "8.2.2 Cluster Approaches.",
      "label": "Other",
      "prob": 0.8276113867759705
    },
    {
      "sentence": "3.4.2 Local Observations.",
      "label": "Other",
      "prob": 0.8638469576835632
    },
    {
      "sentence": "Considering the literature, Jang et al.",
      "label": "Other",
      "prob": 0.8372552990913391
    },
    {
      "sentence": "This work motivates us to further explore data-driven measures of consensus: We follow a similar approach, but instead of regressing on the DTW distance values, and relying on pairwise comparisons, we directly compute an average sequence from all gesture proposals in a referent group, using DBA.",
      "label": "Method",
      "prob": 0.6436030268669128
    },
    {
      "sentence": "Ledo et al.",
      "label": "Other",
      "prob": 0.8518528342247009
    },
    {
      "sentence": "7.2.4 Exploration Strategies.",
      "label": "Other",
      "prob": 0.8744440078735352
    },
    {
      "sentence": "3.2.1 Core Visualization Concept.",
      "label": "Other",
      "prob": 0.8601219654083252
    },
    {
      "sentence": "Jang et al.",
      "label": "Other",
      "prob": 0.8637502789497375
    },
    {
      "sentence": "ACM Reference Format:",
      "label": "Other",
      "prob": 0.8979548215866089
    },
    {
      "sentence": "8.2.3 Feature Representation.",
      "label": "Other",
      "prob": 0.8738797903060913
    },
    {
      "sentence": "Nebeling et al.",
      "label": "Other",
      "prob": 0.8776620626449585
    },
    {
      "sentence": "Liu et al.",
      "label": "Other",
      "prob": 0.8692583441734314
    },
    {
      "sentence": "3.3.3 Examining Gestures.",
      "label": "Other",
      "prob": 0.8676232099533081
    },
    {
      "sentence": "3.3.1 Pan and Zoom.",
      "label": "Other",
      "prob": 0.8975616693496704
    },
    {
      "sentence": "7.2.6 Interactive Clustering.",
      "label": "Other",
      "prob": 0.9018144607543945
    },
    {
      "sentence": "3.1.6 Gesture Clustering.",
      "label": "Other",
      "prob": 0.9061189293861389
    },
    {
      "sentence": "Alternatively, Ali et al.",
      "label": "Other",
      "prob": 0.8954233527183533
    },
    {
      "sentence": "7.2.1 Initial Use.",
      "label": "Other",
      "prob": 0.8800748586654663
    },
    {
      "sentence": "Smilkov et al.",
      "label": "Other",
      "prob": 0.9080905318260193
    },
    {
      "sentence": "3.1.7 Sharing Results.",
      "label": "Other",
      "prob": 0.8851813673973083
    },
    {
      "sentence": "3.3.2 Examining Poses.",
      "label": "Other",
      "prob": 0.914002001285553
    }
  ]
}