{
  "1908.07816": [
    {
      "sentence": "Recent development in neural language modeling has generated significant excitement in the open-domain dialog generation community.",
      "label": "Background",
      "prob": 0.9549102783203125
    },
    {
      "sentence": "In multi-turn settings, where a context with multiple history utterances is given, the same structure often ignores the hierarchical characteristic of the context.",
      "label": "Background",
      "prob": 0.9357916712760925
    },
    {
      "sentence": "Both approaches require an emotion label as input (either given or handcrafted), which might be unpractical in real dialog scenarios.",
      "label": "Background",
      "prob": 0.9323793649673462
    },
    {
      "sentence": "More recently, researchers started incorporating affect information into neural dialog models.",
      "label": "Background",
      "prob": 0.9134788513183594
    },
    {
      "sentence": "However, these multiturn dialog models do not take into account the turn-taking emotional changes of the dialog.",
      "label": "Background",
      "prob": 0.9126339554786682
    },
    {
      "sentence": "on millions of tweets with emoji labels and is more suitable for tweet-like conversations.",
      "label": "Background",
      "prob": 0.911562442779541
    },
    {
      "sentence": "Recent advances in natural language understanding have proposed new network architectures to process text input.",
      "label": "Background",
      "prob": 0.9033182263374329
    },
    {
      "sentence": "Human evaluation has been widely used to evaluate open-domain dialog generation tasks.",
      "label": "Background",
      "prob": 0.8933848142623901
    },
    {
      "sentence": "It could be replaced by other well-established affect recognizer or one that is more appropriate to the target domain.",
      "label": "Background",
      "prob": 0.8835139274597168
    },
    {
      "sentence": "Several participants even noted a chatbot is ideal for sensitive content that is too embarrassing to ask another human.",
      "label": "Background",
      "prob": 0.869884729385376
    },
    {
      "sentence": "BLEU score is often used to measure the quality of machinetranslated text.",
      "label": "Background",
      "prob": 0.864924967288971
    },
    {
      "sentence": "by randomly sampling the dialogs from the dataset, it may include out-of-context dialogs, causing confusion and ambiguity for human evaluators.",
      "label": "Background",
      "prob": 0.8489238023757935
    },
    {
      "sentence": "However, literature in affective science does not necessarily validate such rules.",
      "label": "Background",
      "prob": 0.8488907217979431
    },
    {
      "sentence": "[14] developed a customer support neural chatbot, capable of generating dialogs similar to the humans in terms of empathic and passionate tones, potentially serving as proxy customer support agents on social media platforms.",
      "label": "Background",
      "prob": 0.8485616445541382
    },
    {
      "sentence": "Furthermore, to the best of our knowledge, the psychology and social science literature does not provide clear rules for emotional interaction.",
      "label": "Background",
      "prob": 0.8349536061286926
    },
    {
      "sentence": "Recent work [1, 48, 49] on affect-rich conversational chatbots turned to human opinion to evaluate both fluency and emotionality of their models.",
      "label": "Background",
      "prob": 0.8315838575363159
    },
    {
      "sentence": "However, the choice of emotion classifier is not strictly limited to LIWC.",
      "label": "Background",
      "prob": 0.8266450762748718
    },
    {
      "sentence": "As mentioned in the related work, Asghars affective dialog model, the affect-rich conversation model, and the Emotional Chatting Machine do not learn the emotional exchanges in the dialogs.",
      "label": "Background",
      "prob": 0.8217653632164001
    },
    {
      "sentence": "Usually the probability distribution p ( y | X ) can be modeled by an RNN language model conditioned on X .",
      "label": "Background",
      "prob": 0.811713695526123
    },
    {
      "sentence": "But such human experiments are sensitive to risk factors if the experiment is not carefully designed.",
      "label": "Background",
      "prob": 0.8060311675071716
    },
    {
      "sentence": "Including HRAN instead of other neural dialog models with affect information was not an easy decision.",
      "label": "Background",
      "prob": 0.8053818941116333
    },
    {
      "sentence": "Recent work on incorporating affect information into natural language processing tasks has inspired our current work.",
      "label": "Background",
      "prob": 0.7997351884841919
    },
    {
      "sentence": "They can be mainly described as affect language models and emotional dialog systems.",
      "label": "Background",
      "prob": 0.7925631403923035
    },
    {
      "sentence": "Copyrights for thirdparty components of this work must be honored.",
      "label": "Background",
      "prob": 0.7862178087234497
    },
    {
      "sentence": "Some earlier work of dialog response generation [17, 18] adopted this metric to measure the performance of chatbots.",
      "label": "Background",
      "prob": 0.7798153162002563
    },
    {
      "sentence": "Some researchers [19, 34, 48] argue that perplexity score is not the ideal measurement because for a given context history, one should allow many responses.",
      "label": "Background",
      "prob": 0.7783232927322388
    },
    {
      "sentence": "While using a hierarchical mechanism to track the conversation history in multi-turn dialogs is not new (e.g., HRAN by Xing et al. [45]), to combine it with an additional emotion RNN to process the emotional information in each history utterance has never been attempted",
      "label": "Background",
      "prob": 0.7745667695999146
    },
    {
      "sentence": "The Emotional Chatting Machine (ECM) [49] takes a post and generates a response in a predefined emotion category.",
      "label": "Background",
      "prob": 0.7701318264007568
    },
    {
      "sentence": "This approach is free of human-defined heuristic rules, and hence, is more robust and fundamental than those described in existing work.",
      "label": "Background",
      "prob": 0.7692219614982605
    },
    {
      "sentence": "of dialog response generation, perplexity does not align with human judgement.",
      "label": "Background",
      "prob": 0.7654263973236084
    },
    {
      "sentence": "VAD is a vector model, as opposed to a categorical model (LIWC), representing a given emotion in each of the valence, arousal, and dominance axes.",
      "label": "Background",
      "prob": 0.7589316964149475
    },
    {
      "sentence": "We may wonder how HRAN and MEED differ in terms of the distributional representations of their respective vocabularies (words in the language model, and affect words).",
      "label": "Background",
      "prob": 0.7551403641700745
    },
    {
      "sentence": "Most commonly, researchers have included the models ability to generate grammatically correct, contextually coherent, and emotionally appropriate responses, of which the latter two properties cannot be reliably evaluated using automatic metrics.",
      "label": "Background",
      "prob": 0.7533313035964966
    },
    {
      "sentence": "Recent work [22] has shown that the automatic evaluation metrics borrowed from machine translation such as",
      "label": "Background",
      "prob": 0.7526699900627136
    },
    {
      "sentence": "We chose the multi-turn setting because a model suitable for single-turn dialogs cannot effectively track earlier context in multi-turn dialogs, both semantically and emotionally.",
      "label": "Background",
      "prob": 0.7481061220169067
    },
    {
      "sentence": "This is especially true if we want our conversational agents to speak more diversely.",
      "label": "Background",
      "prob": 0.7441194653511047
    },
    {
      "sentence": "Recently, a number of researchers begain developing automatic and data-driven evaluation methods [24, 38], with the ultimate goal of replacing human evaluation.",
      "label": "Background",
      "prob": 0.7383120059967041
    },
    {
      "sentence": "The approach is also completely data-driven, thus absent of hand-crafted rules.",
      "label": "Background",
      "prob": 0.7347573041915894
    },
    {
      "sentence": "However, for DeepMoji, the 64 categories of emojis do not have a clear and exact correspondence with standardized emotion categories, nor to the VAD vectors.",
      "label": "Background",
      "prob": 0.7319493293762207
    },
    {
      "sentence": "On the contrary, HRAN poses a question in reply, contradicting the dialog history.",
      "label": "Background",
      "prob": 0.7245187163352966
    },
    {
      "sentence": "Our model uses RNNs to encode the input sequences, and GRU cells to capture long-term dependency among different positions in the sequences.",
      "label": "Background",
      "prob": 0.7167629599571228
    },
    {
      "sentence": "For dialog 4, MEED responds in sympathy to the other speaker, which is consistent with the second utterance in the context.",
      "label": "Background",
      "prob": 0.7145865559577942
    },
    {
      "sentence": "However they are still in an early stage.",
      "label": "Background",
      "prob": 0.7029796242713928
    },
    {
      "sentence": "At text generation time, affect information is also used to increase the appropriate selection of the next word.",
      "label": "Background",
      "prob": 0.6953274011611938
    },
    {
      "sentence": "Thus the i th row of the weight matrix W i can be regarded as a vector representation of the i th word in the vocabulary.",
      "label": "Background",
      "prob": 0.6823832988739014
    },
    {
      "sentence": "For contextual coherence and emotional appropriateness, MEED achieved higher average scores than S2S and HRAN, which means MEED keeps better track of the context and can generate responses that are emotionally more appropriate and natural.",
      "label": "Background",
      "prob": 0.6789640188217163
    },
    {
      "sentence": "Perplexity is a measurement of how a probability model predicts a sample.",
      "label": "Background",
      "prob": 0.6711286306381226
    },
    {
      "sentence": "This makes Affect-LM both capable of distinguishing affect information conveyed by each word in the language modeling part and aware of the preceeding texts emotion in each generation step.",
      "label": "Background",
      "prob": 0.663076639175415
    },
    {
      "sentence": "The hierarchical attention structure involves two encoders to produce the dialog context vector c t , namely the word-level encoder and the utterance-level encoder.",
      "label": "Background",
      "prob": 0.6592287421226501
    },
    {
      "sentence": "where s t  1 is the previous hidden state of the decoder, and v b , U b and W b are utterance-level attention parameters.",
      "label": "Background",
      "prob": 0.6461920738220215
    },
    {
      "sentence": "seems to be making the responses emotionally richer, existing approaches mainly follow two directions.",
      "label": "Background",
      "prob": 0.6459462642669678
    },
    {
      "sentence": "Specifically, the Transformer [41] uses pure attention mechanisms without any recurrence structures.",
      "label": "Background",
      "prob": 0.64505535364151
    },
    {
      "sentence": "The paper also proposed the affectively diverse beam search during decoding, so that the generated candidate responses are as affectively diverse as possible.",
      "label": "Background",
      "prob": 0.6437426805496216
    },
    {
      "sentence": "as a bonus to the rater judged to be the most serious.",
      "label": "Background",
      "prob": 0.6381715536117554
    },
    {
      "sentence": "In a qualitative study [47], participants expressed an interest in chatbots capable of serving as an attentive listener and providing motivational support, thus fulfilling users emotional needs.",
      "label": "Background",
      "prob": 0.6353461742401123
    },
    {
      "sentence": "Compared with RNNs, the Transformer can capture better long-term dependency due to the self-attention mechanism, which is free of locality biases, and is more efficient to train because of better parallelization capability.",
      "label": "Background",
      "prob": 0.6346757411956787
    },
    {
      "sentence": "They include whether the intructions are clear, whether they have been tested with users before hand, and whether there is a good balance of the human judgement tasks.",
      "label": "Background",
      "prob": 0.6334341764450073
    },
    {
      "sentence": "This gives the hint that the emotion encoder in MEED is capable of tracking the emotion states in the conversation history.",
      "label": "Background",
      "prob": 0.6306504011154175
    },
    {
      "sentence": "[16] found users frustration caused by a computer system can be alleviated by computer-initiated emotional support, by providing feedback on emotional content along with sympathy and empathy.",
      "label": "Background",
      "prob": 0.6252342462539673
    },
    {
      "sentence": "In fact, the best strategy to speak to an angry customer is the de-escalation strategy (using neutral words to validate anger) rather than employing equally emotional words (minimizing affect dissonance) or words that convey happiness (maximizing affect dissonance).",
      "label": "Background",
      "prob": 0.6193575859069824
    },
    {
      "sentence": "It seems such social and emotional intelligence is captured in our conversations.",
      "label": "Background",
      "prob": 0.6193536520004272
    },
    {
      "sentence": "Evaluation of dialog models remains an open problem in the response generation field.",
      "label": "Background",
      "prob": 0.6160053610801697
    },
    {
      "sentence": "This leaves us wondering whether using a multi-turn neural model can be as effective in learning emotional exchanges as MEED.",
      "label": "Background",
      "prob": 0.6076102256774902
    },
    {
      "sentence": "since the word worried is assigned to both negative emotion and anxious .",
      "label": "Background",
      "prob": 0.5832276940345764
    },
    {
      "sentence": "[34] applied attention mechanism [2] to the same structure on Twitter-style microblogging data.",
      "label": "Background",
      "prob": 0.5776241421699524
    },
    {
      "sentence": "all the models, the vocabulary consists of 20,000 most frequent words in the Cornell and DailyDialog datasets, plus three extra tokens: <unk> for words that do not exist in the vocabulary, <go> indicating the begin of an utterance, and <eos> indicating the end of an utterance.",
      "label": "Background",
      "prob": 0.5771452784538269
    },
    {
      "sentence": "However, for our purpose, which is to speak emotionally appropriately and as human-like as possible, we believe this is a good measure.",
      "label": "Background",
      "prob": 0.5692734718322754
    },
    {
      "sentence": "because it is a well-established emotion lexical resource, covering the whole English dictionary whereas VAD only contains 13K lemmatized terms.",
      "label": "Background",
      "prob": 0.5660545229911804
    },
    {
      "sentence": "Vinyals and Le [42] were one of the first to model dialog generation using neural networks.",
      "label": "Background",
      "prob": 0.5649139881134033
    },
    {
      "sentence": "whose responses appear too many times (the threshold is set to 10 for Cornell, and 5 for DailyDialog), to prevent them from dominating the learning procedure.",
      "label": "Background",
      "prob": 0.5634574890136719
    },
    {
      "sentence": "However, recent study [22] suggests that it does not align well with human evaluation.",
      "label": "Background",
      "prob": 0.5550473928451538
    },
    {
      "sentence": "Equally we need a decoder capable of selecting the best and most human-like answers.",
      "label": "Background",
      "prob": 0.5540385246276855
    },
    {
      "sentence": "As much as these work in the above section inspired our work, our approach in generating affect dialogs is significantly different.",
      "label": "Background",
      "prob": 0.5409917831420898
    },
    {
      "sentence": "[11] made the first attempt to augment the original LSTM language model with affect treatment in what they called Affect-LM.",
      "label": "Background",
      "prob": 0.5390338897705078
    },
    {
      "sentence": "Our choice of including S2S is rather obvious.",
      "label": "Background",
      "prob": 0.5299693942070007
    },
    {
      "sentence": "In order to adapt S2S to the multi-turn setting, we concatenate all the history utterances in the context into one.For",
      "label": "Background",
      "prob": 0.5238473415374756
    },
    {
      "sentence": "Three of them are fluent English speakers and one is a native speaker.",
      "label": "Background",
      "prob": 0.5216538310050964
    },
    {
      "sentence": "Here  tjk is the word-level attention score placed on h jk , and can be calculated as",
      "label": "Background",
      "prob": 0.5211288332939148
    },
    {
      "sentence": "The evaluation of chatbots remains an open problem in the field.",
      "label": "Background",
      "prob": 0.5209084153175354
    },
    {
      "sentence": "They fixed this problem by increasing the diversity of the response.",
      "label": "Background",
      "prob": 0.5122503042221069
    },
    {
      "sentence": "With t-SNE [25], we are able to reduce the dimensionality of the weights to two, and visualize them in a straightforward way.",
      "label": "Background",
      "prob": 0.5065722465515137
    },
    {
      "sentence": "To achieve this, we need an encoder to distinguish the affect information in the context, in addition to its semantic meaning.",
      "label": "Background",
      "prob": 0.5050273537635803
    },
    {
      "sentence": "Dialog 1 and 2 are emotionally positive and dialog 3 and 4 are negative.",
      "label": "Background",
      "prob": 0.49643784761428833
    },
    {
      "sentence": "Given a target response y = { y 1 , y 2 , . . . , y T } , the perplexity is calculated as",
      "label": "Background",
      "prob": 0.49087339639663696
    },
    {
      "sentence": "Since our model learns how to respond properly in a data-driven way, we believe having a training dataset with good quality while being large enough plays an important role in developing an engaging and user-friendly chatbot.",
      "label": "Background",
      "prob": 0.49030622839927673
    },
    {
      "sentence": "[48] proposed an affect-rich dialog model using biased attention mechanism on emotional words in the input message, by taking advantage of the VAD embeddings.",
      "label": "Background",
      "prob": 0.4796879291534424
    },
    {
      "sentence": "The main idea is to use an internal memory module to capture the emotion dynamics during decoding, and an external memory module to model emotional expressions explicitly by assigning different probability values to emotional words as opposed to regular words.",
      "label": "Background",
      "prob": 0.4755904972553253
    },
    {
      "sentence": "The Google form was released on 31 January 2019, and the workers finished their tasks by 4 February 2019.",
      "label": "Background",
      "prob": 0.47063717246055603
    },
    {
      "sentence": "For our purposes we filtered out only those dialogs where more than a half of utterances have non-neutral emotional labels, resulting in 78 emotionally positive dialogs and 14 emotionally negative dialogs.",
      "label": "Background",
      "prob": 0.4688929319381714
    },
    {
      "sentence": "Here  tj is the utterance-level attention score placed on  tj , and can be calculated as",
      "label": "Background",
      "prob": 0.4666507840156555
    },
    {
      "sentence": "contrast to Affect-LM, Asghars neural affect dialog model aims at generating explicit responses given a particular utterance.",
      "label": "Objective",
      "prob": 0.4766360819339752
    },
    {
      "sentence": "Specifically, at time step t , the hidden state of the decoder s t is obtained by applying the GRU function,",
      "label": "Background",
      "prob": 0.45278307795524597
    },
    {
      "sentence": "We believe this emotion recognition step is vital for a dialog model to produce emotionally appropriate responses.",
      "label": "Background",
      "prob": 0.4505232572555542
    },
    {
      "sentence": "We are going to elaborate on how to obtain c t and e , and how they are combined in the decoding part.",
      "label": "Background",
      "prob": 0.44495689868927
    },
    {
      "sentence": "Zhou and Wang [50] extended the standard seq2seq model to a conditional variational autoencoder combined with policy gradient techniques.",
      "label": "Background",
      "prob": 0.44073179364204407
    },
    {
      "sentence": "If the emotion embedding layer is learning and distinguishing affect states correctly, we will see clear differences in the visualization.",
      "label": "Result",
      "prob": 0.46878471970558167
    },
    {
      "sentence": "Human-centered computing  Human computer interaction (HCI) ; Natural language interfaces .",
      "label": "Background",
      "prob": 0.4370056688785553
    },
    {
      "sentence": "In this section, we briefly discuss how our framework can incorporate other components, as well as several directions to extend it.",
      "label": "Background",
      "prob": 0.4360671937465668
    },
    {
      "sentence": "For all other uses, contact the owner/author(s).",
      "label": "Background",
      "prob": 0.43279722332954407
    },
    {
      "sentence": "capable of recognizing and generating emotionally appropriate responses, which is the first step toward such a goal.",
      "label": "Objective",
      "prob": 0.5290927290916443
    },
    {
      "sentence": "In addition, comparing S2S and HRAN also gives us an idea of how much the hierarchical mechansim is improving upon the basic model.",
      "label": "Background",
      "prob": 0.43160685896873474
    },
    {
      "sentence": "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "label": "Other",
      "prob": 0.4596655070781708
    },
    {
      "sentence": "The probability distribution p ( y | X ) can be written as",
      "label": "Background",
      "prob": 0.4267481565475464
    },
    {
      "sentence": "Unbalanced emotional distribution of the test dialogs may also lead to biased conclusions since the chatbots abilities are evaluated on the unrepresentative",
      "label": "Background",
      "prob": 0.42394593358039856
    },
    {
      "sentence": "Some recent work addresses this problem by adopting a hierarchical recurrent encoder-decoder (HRED) structure [32, 33, 35].",
      "label": "Background",
      "prob": 0.42352426052093506
    },
    {
      "sentence": "To compensate for the inbalance, we further curated more negative emotion dialogs so that the final set has equal emotion distributions.",
      "label": "Background",
      "prob": 0.4198758006095886
    },
    {
      "sentence": "Part of our test set comes from the DailyDialog dataset, which consists of meaningful complete dialogs.",
      "label": "Background",
      "prob": 0.4177800416946411
    },
    {
      "sentence": "(1) We describe in detail a novel emotion-tracking dialog generation model that learns the emotional interactions directly from the data.",
      "label": "Background",
      "prob": 0.41659191250801086
    },
    {
      "sentence": "Following the standard seq2seq structure, various improvements have been made on the neural conversation model.",
      "label": "Background",
      "prob": 0.4154368042945862
    },
    {
      "sentence": "The model is trained with a weighted cross-entropy loss function, which encourages the generation of emotional words.",
      "label": "Background",
      "prob": 0.41461730003356934
    },
    {
      "sentence": "[45] proposed the hierarchical recurrent attention network (HRAN), using a hierarchical attention mechanism.",
      "label": "Background",
      "prob": 0.4134479761123657
    },
    {
      "sentence": "The workers fulfilled the tasks in Google form 4 following the instructions and created five negative dialogs with four turns, as if they were interacting with another human, in each of the following topics: relationships , entertainment , service , work and study , and everyday situations .",
      "label": "Background",
      "prob": 0.4118787944316864
    },
    {
      "sentence": "Each term in Equation (13) is then given by",
      "label": "Background",
      "prob": 0.41066884994506836
    },
    {
      "sentence": "Specifically, a tone indicator is added to each step of the decoder during the training phase.",
      "label": "Method",
      "prob": 0.45721709728240967
    },
    {
      "sentence": "The model takes a post and an emoji as input, and generates the response with target emotion specified by the emoji.",
      "label": "Background",
      "prob": 0.40945965051651
    },
    {
      "sentence": "We call work in this area globally neural dialog generation.",
      "label": "Background",
      "prob": 0.40941235423088074
    },
    {
      "sentence": "This set can be expanded to include more categories if we desire a richer distinction.",
      "label": "Background",
      "prob": 0.40761178731918335
    },
    {
      "sentence": "At training time, Affect-LM can be considered as an energy based model where the added energy term captures the degree of correlation between the next word and the affect information of the preceeding text.",
      "label": "Background",
      "prob": 0.3959452509880066
    },
    {
      "sentence": "where W e and b e are trainable parameters.",
      "label": "Background",
      "prob": 0.3937932848930359
    },
    {
      "sentence": "BLEU score [27] tend to align poorly with human judgement.",
      "label": "Background",
      "prob": 0.39095938205718994
    },
    {
      "sentence": "The recruitment proceeded in the same manner as described above; the raters were offered 80 CHF (or roughly US $80) per participant gift coupons for fulfilling the task, and extra 20 CHF (or roughly US $20) coupon was promised",
      "label": "Background",
      "prob": 0.39044463634490967
    },
    {
      "sentence": "In this paper, we describe an end-to-end Multi-turn Emotionally Engaging Dialog model (MEED), capable of recognizing emotions and generating emotionally appropriate and humanlike responses with the ultimate goal of reproducing social behaviors that are habitual in human-human conversations.",
      "label": "Objective",
      "prob": 0.5088207721710205
    },
    {
      "sentence": "[14] built a tone-aware chatbot for customer care on social media, by deploying extra meta information of the conversations in the seq2seq model.",
      "label": "Background",
      "prob": 0.37837713956832886
    },
    {
      "sentence": "We recruited two human workers to augment the data to produce more emotionally negative dialogs.",
      "label": "Background",
      "prob": 0.37410223484039307
    },
    {
      "sentence": "We adopted this training pattern because the Cornell dataset is bigger but noisier, while DailyDialog is smaller but more daily-based.",
      "label": "Result",
      "prob": 0.48426198959350586
    },
    {
      "sentence": "For future directions, we would like to investigate the diversity issue of the responses generated, possibly by extending the mutual information objective function [17] to multi-turn settings.",
      "label": "Background",
      "prob": 0.3714883625507355
    },
    {
      "sentence": "When generating the word y t at time step t , the context X is encoded into a fixed-sized dialog context vector c t by following the hierarchical attention structure in HRAN [45].",
      "label": "Background",
      "prob": 0.3690704107284546
    },
    {
      "sentence": "Both of them were PhD students from our university (males, aged 24 and 25), fluent in English, and not related to the authors lab.",
      "label": "Background",
      "prob": 0.36812299489974976
    },
    {
      "sentence": "where s t  1 is the previous hidden state of the decoder,  tj + 1 is the previous hidden state of the utterance-level encoder, and v a , U a , V a and W a are word-level attention parameters.",
      "label": "Background",
      "prob": 0.36724233627319336
    },
    {
      "sentence": "The utterance-level encoder is a unidirectional RNN with GRU that goes from the last utterance in the context to the first, with its input at each step as the summary of the corresponding utterance, which is obtained by applying a Bahdanau-style attention mechanism [2] on the word-level",
      "label": "Background",
      "prob": 0.36184704303741455
    },
    {
      "sentence": "We refer to them as language model weights and emotion weights, respectively.",
      "label": "Background",
      "prob": 0.3615191876888275
    },
    {
      "sentence": "where W and b are trainable parameters.",
      "label": "Background",
      "prob": 0.3585520088672638
    },
    {
      "sentence": "However, for all of the three criteria in human evaluation, HRAN actually outperforms S2S.",
      "label": "Result",
      "prob": 0.38239309191703796
    },
    {
      "sentence": "is a sequence of m i utterances, and",
      "label": "Other",
      "prob": 0.4578601121902466
    },
    {
      "sentence": "The standard seq2seq framework is applied to single-turn response generation.",
      "label": "Method",
      "prob": 0.4767790138721466
    },
    {
      "sentence": "decoding phase, Equation (16) takes o t , the concatenation of the language context vector s t and the emotion context vector e , and generates a probability distribution over the vocabulary words by applying a softmax layer.",
      "label": "Method",
      "prob": 0.47996997833251953
    },
    {
      "sentence": "The success of sequence-to-sequence (seq2seq) learning [5, 37] in the field of neural machine translation has inspired researchers to apply the recurrent neural network (RNN) encoder-decoder structure to response generation [42].",
      "label": "Other",
      "prob": 0.44153836369514465
    },
    {
      "sentence": "We make use of the five emotion-related categories, namely positive emotion , negative emotion , anxious , angry , and sad .",
      "label": "Background",
      "prob": 0.34138360619544983
    },
    {
      "sentence": "We found them via email and messaging platforms, and offered 80 CHF (or roughly US $80) gift coupons as incentive for each participant.",
      "label": "Result",
      "prob": 0.5060639381408691
    },
    {
      "sentence": "To do so, the authors designed three affect-related loss functions, namely minimizing affect dissonance, maximizing a affective dissonance, and maximizing affective content.",
      "label": "Background",
      "prob": 0.3351641893386841
    },
    {
      "sentence": "Therefore, in this paper, we mainly adopt human evaluation, along with perplexity and BLEU score, following the existing work.",
      "label": "Background",
      "prob": 0.33462005853652954
    },
    {
      "sentence": "[34] further trained the seq2seq model with attention mechanism on a self-crawled Weibo (a popular Twitter-like social media website in China) dataset.",
      "label": "Result",
      "prob": 0.397228479385376
    },
    {
      "sentence": "For example, we can consider using more fine-grained emotion categories from GALC [31], or using DeepMoji [8], which was trained",
      "label": "Background",
      "prob": 0.3302489221096039
    },
    {
      "sentence": "The weight matrix of this softmax layer is denoted as W , whose shape is | V | 2 d , where | V | is the vocabulary size and d = 256 is the hidden state size of the RNNs.",
      "label": "Result",
      "prob": 0.33802202343940735
    },
    {
      "sentence": "5 For grammatical correctness, all three models achieved high scores, which means all models are capable of generating fluent utterances that make sense.",
      "label": "Result",
      "prob": 0.5341014862060547
    },
    {
      "sentence": "Early work [18, 30, 36] on response generation used automatic evaluation metrics borrowed from the machine translation field, such as the BLEU score, to evaluate dialog systems.",
      "label": "Result",
      "prob": 0.4318878650665283
    },
    {
      "sentence": "For better illustration, we selected 100 most frequent (emotionally) positive words and 100 most frequent negative words from the vocabulary, and used t-SNE to project the corresponding language model weights and emotion weights to two dimensions.",
      "label": "Method",
      "prob": 0.41941288113594055
    },
    {
      "sentence": "In other words, using the perplexity measures, we were able to determine when to stop training our model.",
      "label": "Result",
      "prob": 0.44881197810173035
    },
    {
      "sentence": "We do recognize that it is not the only way to measure chatbots performance.",
      "label": "Result",
      "prob": 0.5434121489524841
    },
    {
      "sentence": "The emotion flow of the context X is then modeled by an unidirectional RNN with GRU going from the first utterance in the context to the last, with its input being a j at each step.",
      "label": "Method",
      "prob": 0.4656001627445221
    },
    {
      "sentence": "For example, assuming x j = he is worried about me, then",
      "label": "Other",
      "prob": 0.45898866653442383
    },
    {
      "sentence": "Since HRAN does not have the emotion context vector, we just visualized the whole output layer weight vector, which does a similar job as the language model weights in",
      "label": "Result",
      "prob": 0.4771033227443695
    },
    {
      "sentence": "In neural dialog generation community, many researchers have adopted this method, especially in the beginning of this field [32, 42, 45, 4850].",
      "label": "Other",
      "prob": 0.34680747985839844
    },
    {
      "sentence": "In this paper, we used both perplexity measures and human judgement in our experiments to finalize our model.",
      "label": "Method",
      "prob": 0.3798319101333618
    },
    {
      "sentence": "This is why we decided to take the automatic and data-driven approach.",
      "label": "Method",
      "prob": 0.30908438563346863
    },
    {
      "sentence": "In one, an emotion label is explicitly required as input so that the machine can generate sentences of that particular emotion label or type [49].",
      "label": "Other",
      "prob": 0.6502777934074402
    },
    {
      "sentence": "We first selected the emotionally colored dialogs with exactly four turns from the DailyDialog dataset.",
      "label": "Method",
      "prob": 0.3432544469833374
    },
    {
      "sentence": "As shown in the tables, we got high agreement among the raters for grammatical correctness, and fair",
      "label": "Result",
      "prob": 0.5388670563697815
    },
    {
      "sentence": "We apply a dense layer with sigmoid activation function on top of 1 ( x j ) to embed the emotion indicator vector into a continuous space,",
      "label": "Method",
      "prob": 0.4800543487071991
    },
    {
      "sentence": "The word-level encoder is essentially a bidirectional RNN with gated recurrent units (GRU) [5].",
      "label": "Other",
      "prob": 0.6306822299957275
    },
    {
      "sentence": "As future work, we would like to adopt the Transformer architecture to replace the RNNs in our model, and initialize our encoder with pre-trained language models.",
      "label": "Method",
      "prob": 0.3572155237197876
    },
    {
      "sentence": "On the contrary, the emotion weights in MEED, in the last plot, have a clearer clustering effect, i.e., positive words are mainly grouped on the top-left, while negative words are mainly grouped at the bottom-right.",
      "label": "Result",
      "prob": 0.5735601782798767
    },
    {
      "sentence": "In the dataset each dialog turn is annotated with a corresponding emotional category, including the neutral one.",
      "label": "Result",
      "prob": 0.5134836435317993
    },
    {
      "sentence": "Since we concatenate the language context vector and the emotion context vector as the input to the softmax layer, the first half of the weight vector W i corresponds to the language context vector, and the second half corresponds to the emotion context vector.",
      "label": "Result",
      "prob": 0.36981549859046936
    },
    {
      "sentence": "Finally Bickmore and Picard [3] showed a relational agent with deliberate socialemotional skills was respected more, liked more, and trusted more, even after four weeks of interaction, compared to an equivalent task-oriented agent.",
      "label": "Result",
      "prob": 0.6201257109642029
    },
    {
      "sentence": "We consider factors such as the balance of positive and negative emotions in test dialogs, a well-chosen range of topics, and dialogs that our human evaluators can relate.",
      "label": "Result",
      "prob": 0.5089158415794373
    },
    {
      "sentence": "We would also like to adopt the Transformer architecture with pre-trained language model weights, and train our model on a much larger dataset, by extracting multi-turn dialogs from the OpenSubtitles corpus.",
      "label": "Method",
      "prob": 0.3108060359954834
    },
    {
      "sentence": "This is why our final comparision is based on three multi-turn dialog generation models: the standard seq2seq model (denoted as S2S), HRAN, and our proposed model, MEED.",
      "label": "Result",
      "prob": 0.412769615650177
    },
    {
      "sentence": "For the first two examples, we can see that MEED is able to generate more emotional content (like fun and congratulations) that is appropriate according to the context.",
      "label": "Result",
      "prob": 0.5185790061950684
    },
    {
      "sentence": "is a sequence of n ij words.",
      "label": "Other",
      "prob": 0.4766765832901001
    },
    {
      "sentence": "To avoid learning obscene and callous exchanges often found in social media data like tweets and Reddit threads [29], we opted to train our model on movie subtitles, whose dialogs were carefully created by professional writers.",
      "label": "Method",
      "prob": 0.4420223534107208
    },
    {
      "sentence": "Thus, in the future, we plan to train our model on the multi-turn conversations that we have already extracted from the much bigger OpenSubtitles corpus and the EmpatheticDialogues dataset.",
      "label": "Result",
      "prob": 0.45147210359573364
    },
    {
      "sentence": "Since being able to track several turns is really important, we made this design decision from the beginning, in contrast to most related work where models are only trained and tested on single-turn dialogs.",
      "label": "Result",
      "prob": 0.35404178500175476
    },
    {
      "sentence": "We filtered out those pairs that have at least one utterance with length greater than 30.",
      "label": "Result",
      "prob": 0.4587021470069885
    },
    {
      "sentence": "Further, if a test set for human evaluation is prepared",
      "label": "Result",
      "prob": 0.4405926764011383
    },
    {
      "sentence": "For this survey, the Google form was launched on 12 February 2019, and all the submissions from our raters were collected by 14 February 2019.",
      "label": "Result",
      "prob": 0.5387735366821289
    },
    {
      "sentence": "In our experiments, the models were first trained on the Cornell Movie Dialogs Corpus, and then fine-tuned on the DailyDialog dataset.",
      "label": "Method",
      "prob": 0.40674808621406555
    },
    {
      "sentence": "In Table 2, for all the three sets, HRAN performs worse than S2S in terms of perplexity.",
      "label": "Result",
      "prob": 0.639416515827179
    },
    {
      "sentence": "See Table 1 for the sizes of the training and validation sets.",
      "label": "Result",
      "prob": 0.45542603731155396
    },
    {
      "sentence": "In another group of work, the main idea is to develop handcrafted rules to direct the machines to generated responses of the desired emotions [1, 48].",
      "label": "Other",
      "prob": 0.374876469373703
    },
    {
      "sentence": "[44] developed a topic aware dialog system.",
      "label": "Result",
      "prob": 0.39354512095451355
    },
    {
      "sentence": "(2) We compare our model, MEED, with the generic seq2seq model and the hierarchical model of multiturn dialogs (HRAN).",
      "label": "Result",
      "prob": 0.37351512908935547
    },
    {
      "sentence": "If any word in x j belongs to one of the five categories, then the corresponding entry in 1 ( x j ) is set to 1; otherwise, x j is treated as neutral, with the last entry of 1 ( x j ) set to 1.",
      "label": "Method",
      "prob": 0.2723363935947418
    },
    {
      "sentence": "See the discussion section for more details on how to do this.",
      "label": "Other",
      "prob": 0.3887730836868286
    },
    {
      "sentence": "[17] found the original version tend to favor short and dull responses.",
      "label": "Result",
      "prob": 0.5950636267662048
    },
    {
      "sentence": "As agreement is extremely high, this can make Fleiss  very sensitive to prevalence [13].",
      "label": "Other",
      "prob": 0.6599364280700684
    },
    {
      "sentence": "agreement among the raters for contextual coherence and emotional appropriateness.",
      "label": "Result",
      "prob": 0.5346513390541077
    },
    {
      "sentence": "A key component in Affect-LM is the use of a well established text analysis program, LIWC (Linguistic Inquiry and Word Count) [28].",
      "label": "Other",
      "prob": 0.65969318151474
    },
    {
      "sentence": "The overall architecture of the model is depicted in Figure 1.",
      "label": "Result",
      "prob": 0.4191005825996399
    },
    {
      "sentence": "Note that we report Fleiss  score [10] for contextual coherence and emotional appropriateness, and Finns r score [9] for grammatical correctness.",
      "label": "Result",
      "prob": 0.5579099655151367
    },
    {
      "sentence": "Following the Transformer architecture, researchers found that pre-training language models on huge amounts of data could largely boost the performance of downstream tasks, and published many pre-trained language models such as BERT [7] and RoBERTa [23].",
      "label": "Other",
      "prob": 0.6070643067359924
    },
    {
      "sentence": "Many application areas show significant benefits of integrating affect information in natural language dialogs.",
      "label": "Result",
      "prob": 0.7077507972717285
    },
    {
      "sentence": "Offline experiments show that our model outperforms both seq2seq and HRAN by a significant amount.",
      "label": "Result",
      "prob": 0.65395587682724
    },
    {
      "sentence": "We believe reproducing conversational and emotional intelligence will make social chatbots more believable and engaging. In this paper, we proposed a multi-turn dialog system",
      "label": "Objective",
      "prob": 0.6633941531181335
    },
    {
      "sentence": "In earlier work on human computer interaction, Klein et al.",
      "label": "Other",
      "prob": 0.6681479215621948
    },
    {
      "sentence": "We present four sample dialogs in Table 6, along with the responses generated by the three models.",
      "label": "Result",
      "prob": 0.5791528224945068
    },
    {
      "sentence": "[46] built a customer service chatbot by training the seq2seq model on a dataset collected with conversations between customers and customer service accounts from 62 brands on Twitter.",
      "label": "Result",
      "prob": 0.5417143702507019
    },
    {
      "sentence": "The final hidden state h jk is then obtained by concatenating the two,",
      "label": "Method",
      "prob": 0.5077303647994995
    },
    {
      "sentence": "It is the first time such an approach is designed with consideration for human judges.",
      "label": "Method",
      "prob": 0.4126051664352417
    },
    {
      "sentence": "We pre-trained our model on the Cornell movie subtitles and then fine-tuned it with the DailyDialog dataset.",
      "label": "Result",
      "prob": 0.34796202182769775
    },
    {
      "sentence": "According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria:",
      "label": "Result",
      "prob": 0.48052310943603516
    },
    {
      "sentence": "But this condition does not gurantee the optimal results until",
      "label": "Result",
      "prob": 0.6255125999450684
    },
    {
      "sentence": "For modeling the affect information, we chose to use LIWC",
      "label": "Result",
      "prob": 0.4654114842414856
    },
    {
      "sentence": "Subsequently, to form the final test set, we randomly selected 50 emotionally positive and 50 emotionally negative dialogs from the two pools of dialogs described above.",
      "label": "Result",
      "prob": 0.3946504592895508
    },
    {
      "sentence": "We describe our model one element at a time, from the basic structure, to the hierarchical component, and finally the emotion embedding layer.",
      "label": "Method",
      "prob": 0.38074323534965515
    },
    {
      "sentence": "Based on this, we conclude that perplexity alone is not enough for evaluating a dialog system.",
      "label": "Result",
      "prob": 0.7336117625236511
    },
    {
      "sentence": "As shown in the table, MEED achieves the lowest perplexity and the highest BLEU score on all three sets.",
      "label": "Result",
      "prob": 0.6512114405632019
    },
    {
      "sentence": "The final dialog context vector c t is then obtained as another linear combination of the outputs of the utterance-level encoder  tj , for j = 1 , 2 , . . . , m ,",
      "label": "Result",
      "prob": 0.4208444654941559
    },
    {
      "sentence": "To create a training set and a validation set for each of the two datasets, we took segments of each dialog with number of turns no more than six, 2 to serve as the training/validation examples.",
      "label": "Method",
      "prob": 0.5751163363456726
    },
    {
      "sentence": "We trained our model using two different datasets and compared its performance with HRAN as well as the basic seq2seq model by performing both offline and online testings.",
      "label": "Method",
      "prob": 0.40992647409439087
    },
    {
      "sentence": "We hope to increase the performance of response generation.",
      "label": "Result",
      "prob": 0.4455646276473999
    },
    {
      "sentence": "We first consider the problem of generating response y given a context X consisting of multiple previous utterances by estimating the probability distribution p ( y | X ) from a data set D = {( X ( i ) , y ( i ) )} Ni = 1 containing N context-response pairs.",
      "label": "Method",
      "prob": 0.468843936920166
    },
    {
      "sentence": "Dialog contexts and three models responses were included into Google form.",
      "label": "Result",
      "prob": 0.5423946380615234
    },
    {
      "sentence": "For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral.",
      "label": "Result",
      "prob": 0.5552451610565186
    },
    {
      "sentence": "On the contrary, we did not use Finns r score for contextual coherence and emotional appropriateness because it is only reasonable when the observed variance is significantly less than the chance variance [40], which did not apply to these two criteria.",
      "label": "Result",
      "prob": 0.6479002833366394
    },
    {
      "sentence": "For utterance x j in X ( j = 1 , 2 , . . . , m ), the bidirectional encoder produces two hidden states at each word position k , the forward hidden state h f jk and the backward hidden state h b jk .",
      "label": "Result",
      "prob": 0.36051687598228455
    },
    {
      "sentence": "We adopted this particular training order because we would like our chatbot to talk more like human chit-chats, and the DailyDialog dataset, compared with the bigger Cornell dataset, is more daily-based.",
      "label": "Result",
      "prob": 0.6846403479576111
    },
    {
      "sentence": "where w y t  1 is the word embedding of y t  1 .",
      "label": "Other",
      "prob": 0.6067395210266113
    },
    {
      "sentence": "IUI 20 Workshops, March 17, 2020, Cagliari, Italy",
      "label": "Other",
      "prob": 0.7639458179473877
    },
    {
      "sentence": "Preparation of Natural Dialog Test Set.",
      "label": "Result",
      "prob": 0.43035125732421875
    },
    {
      "sentence": "(3) We illustrate a human-evaluation procedure for judging machine produced emotional dialogs.",
      "label": "Result",
      "prob": 0.4407123923301697
    },
    {
      "sentence": "on which we apply a softmax layer to obtain a probability distribution over the vocabulary,",
      "label": "Result",
      "prob": 0.3943062722682953
    },
    {
      "sentence": "We did not use Fleiss  score for grammatical correctness.",
      "label": "Result",
      "prob": 0.5970470905303955
    },
    {
      "sentence": "It measures how well a dialog model predicts the target response.",
      "label": "Result",
      "prob": 0.6186193227767944
    },
    {
      "sentence": "To give attention to different parts of the context while generating responses, Xing et al.",
      "label": "Other",
      "prob": 0.7364590167999268
    },
    {
      "sentence": "We model the probability distribution using an RNN language model along with the emotion context vector e .",
      "label": "Result",
      "prob": 0.4753180742263794
    },
    {
      "sentence": "Most of related work focused on integrating affect information into the transduction vector space using either VAD or LIWC, we aim at modeling and generating the affect exchanges in human dialogs using a dedicated embedding layer.",
      "label": "Objective",
      "prob": 0.6040456891059875
    },
    {
      "sentence": "We used two different dialog corpora to train our model the Cornell Movie Dialogs Corpus [6] and the DailyDialog dataset [20].",
      "label": "Result",
      "prob": 0.36842066049575806
    },
    {
      "sentence": "We also reduced the frequency of those pairs",
      "label": "Result",
      "prob": 0.5945848822593689
    },
    {
      "sentence": "learning-with-neural-networks",
      "label": "Other",
      "prob": 0.7430843114852905
    },
    {
      "sentence": "This approach can include any criterion as we judge appropriate.",
      "label": "Result",
      "prob": 0.566874086856842
    },
    {
      "sentence": "The test set consists of 100 dialogs with four turns.",
      "label": "Result",
      "prob": 0.5414515137672424
    },
    {
      "sentence": "In the final human evaluation of the model, we recruited four more PhD students from our university (1 female and 3 males, aged 2225).",
      "label": "Result",
      "prob": 0.6331958174705505
    },
    {
      "sentence": "The final emotion context vector e is obtained as the last hidden state of this emotion encoding RNN.",
      "label": "Result",
      "prob": 0.5956772565841675
    },
    {
      "sentence": "The comparison between perplexity scores and human evaluation results further confirms the fact that in the context",
      "label": "Result",
      "prob": 0.7201488614082336
    },
    {
      "sentence": "To extract the affect information contained in the utterances, we used the LIWC text analysis program.",
      "label": "Method",
      "prob": 0.4787188172340393
    },
    {
      "sentence": "the program LIWC2015, 1 we are able to map each utterance x j in the context to a six-dimensional indicator vector 1 ( x j ) , with the first five entries corresponding to the five emotion categories, and the last one corresponding to neutral .",
      "label": "Result",
      "prob": 0.5447502136230469
    },
    {
      "sentence": "More specifically, at decoding step t , the summary of utterance x j is a linear combination of h jk , for k = 1 , 2 , . . . , n j ,",
      "label": "Other",
      "prob": 0.5527582764625549
    },
    {
      "sentence": "We decided to visualize the output layer weights as word embedding representations using dimensionality reduction technique for the various models.Inthe",
      "label": "Result",
      "prob": 0.4361068606376648
    },
    {
      "sentence": "6 https://github.com/facebookresearch/EmpatheticDialogues",
      "label": "Other",
      "prob": 0.801723837852478
    },
    {
      "sentence": "Further experiments with human evaluation show our model produces emotionally more appropriate responses than both baselines, while also improving the language fluency.",
      "label": "Result",
      "prob": 0.7345799207687378
    },
    {
      "sentence": "Thus a lower perplexity score indicates that the model has better capability of predicting the target sentence, i.e., the humans response.",
      "label": "Result",
      "prob": 0.6122034788131714
    },
    {
      "sentence": "[22] showed that these metrics correlate poorly with human judgement.",
      "label": "Result",
      "prob": 0.7365599274635315
    },
    {
      "sentence": "We are able to achieve this goal, i.e., capturing the emotion information carried in the context X , in the encoder, thanks to LIWC.",
      "label": "Other",
      "prob": 0.48105156421661377
    },
    {
      "sentence": "Here we summarize the configurations and parameters of our experiments:",
      "label": "Result",
      "prob": 0.5665914416313171
    },
    {
      "sentence": "We use the cross-entropy loss as our objective function",
      "label": "Result",
      "prob": 0.37416958808898926
    },
    {
      "sentence": "We believe the quality of this dataset can be better than those curated by crowdsource platforms.",
      "label": "Result",
      "prob": 0.7896369099617004
    },
    {
      "sentence": "Similar to AffectLM [11], we then define a new feature vector o t by concatenating s t (which we refer to as the language context vector) with the emotion context vector e ,",
      "label": "Method",
      "prob": 0.3944917321205139
    },
    {
      "sentence": "Additionally, we extract the emotion information from the utterances in X by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector e , which is combined with c t to produce the distribution.",
      "label": "Method",
      "prob": 0.5880129337310791
    },
    {
      "sentence": "o t = concat ( s t , e ) ,",
      "label": "Other",
      "prob": 0.7732219099998474
    },
    {
      "sentence": "Our main goal is to increase the objectivity of the results and reduce judges mistakes due to out-of-context dialogs they have to evaluate.",
      "label": "Objective",
      "prob": 0.8281640410423279
    },
    {
      "sentence": "Table 2 gives the perplexity and BLEU scores obtained by the three models on the two validation sets and the test set.",
      "label": "Result",
      "prob": 0.6857650876045227
    },
    {
      "sentence": "Their seq2seq framework was trained on an IT Helpdesk Troubleshooting dataset and the OpenSubtitles dataset [21].",
      "label": "Other",
      "prob": 0.5072079300880432
    },
    {
      "sentence": "In parallel to these developments, Zhong et al.",
      "label": "Other",
      "prob": 0.7870673537254333
    },
    {
      "sentence": "In this subsection, we present the experimental results of the automatic evaluation metric as well as human judgement, followed by some analysis.",
      "label": "Result",
      "prob": 0.6576479077339172
    },
    {
      "sentence": "Nevertheless, we still include BLEU scores in this paper, to get a sense of comparison with perplexity and human evaluation results.",
      "label": "Result",
      "prob": 0.7568346261978149
    },
    {
      "sentence": "The main objective of the emotion embedding layer is to recognize the affect information in the given utterances so that the model can respond with emotionally appropriate replies.",
      "label": "Objective",
      "prob": 0.841560959815979
    },
    {
      "sentence": "Table 3, 4 and 5 summarize the human evaluation results on the responses grammatical correctness, contextual coherence, and emotional appropriateness, respectively.",
      "label": "Result",
      "prob": 0.777600884437561
    },
    {
      "sentence": "For the evaluation survey, we also leveraged Google form.",
      "label": "Result",
      "prob": 0.6538152694702148
    },
    {
      "sentence": "is the response with T i words.",
      "label": "Other",
      "prob": 0.6048598289489746
    },
    {
      "sentence": "Specifically, we randomly shuffled the 100 dialogs in the test set, then we used the first three utterances of each dialog as the input to the three models being compared (S2S, HRAN, and MEED), and obtain the respective responses.",
      "label": "Method",
      "prob": 0.49836575984954834
    },
    {
      "sentence": "[18] modeled the personalities of the speakers, and Xing et al.",
      "label": "Other",
      "prob": 0.799507200717926
    },
    {
      "sentence": "2020 Copyright held by the owner/author(s).",
      "label": "Other",
      "prob": 0.8559661507606506
    },
    {
      "sentence": "In the tables, we give the percentage of votes each model received for the three scores, the average score obtained, and the agreement score among the raters.",
      "label": "Result",
      "prob": 0.7014914155006409
    },
    {
      "sentence": "Similarly,",
      "label": "Result",
      "prob": 0.5048144459724426
    },
    {
      "sentence": "While a central theme",
      "label": "Other",
      "prob": 0.7219452857971191
    },
    {
      "sentence": "Visualization of Output Layer Weights.",
      "label": "Result",
      "prob": 0.46619436144828796
    },
    {
      "sentence": "5 https://en.wikipedia.org/wiki/Fleiss%27_kappa#Interpretation",
      "label": "Other",
      "prob": 0.8651810884475708
    },
    {
      "sentence": "encoder output.",
      "label": "Other",
      "prob": 0.37601596117019653
    },
    {
      "sentence": "We thus highly recommend this combination, which is also a common practice in the research community [45, 4850].",
      "label": "Other",
      "prob": 0.7902131676673889
    },
    {
      "sentence": "Recently, Hu et al.",
      "label": "Other",
      "prob": 0.8562504053115845
    },
    {
      "sentence": "Using the newest version of",
      "label": "Other",
      "prob": 0.7034531235694885
    },
    {
      "sentence": "We give more detailed description of how we created the test set in the section of human evaluation.",
      "label": "Result",
      "prob": 0.6647285223007202
    },
    {
      "sentence": "sample.Totake into account the above issues, we took several iterations to prepare the instructions and the test set before conducting the human evaluation experiment.",
      "label": "Result",
      "prob": 0.5452551245689392
    },
    {
      "sentence": "We provide the details about the test data preparation process and the evaluation experiment below.",
      "label": "Result",
      "prob": 0.6871744394302368
    },
    {
      "sentence": "We have demonstrated how to do so by (1) modeling utterances with extra affect vectors, (2) creating an emotional encoding mechanism that learns emotion exchanges in the dataset, (3) curating a multi-turn and balanced dialog dataset, and (4) evaluating the model with offline and online experiments.",
      "label": "Result",
      "prob": 0.5759879350662231
    },
    {
      "sentence": "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.",
      "label": "Other",
      "prob": 0.8762942552566528
    },
    {
      "sentence": "p t = softmax ( Wo t + b ) ,",
      "label": "Other",
      "prob": 0.8084126114845276
    },
    {
      "sentence": "It is a popular method used in language modeling.",
      "label": "Method",
      "prob": 0.6210102438926697
    },
    {
      "sentence": "We have made the source code publicly available.",
      "label": "Other",
      "prob": 0.7408648133277893
    },
    {
      "sentence": "In a similar vein, Asghar et al.",
      "label": "Other",
      "prob": 0.8608052134513855
    },
    {
      "sentence": "We conducted t -test on the perplexity obtained, and results show significant improvements of MEED over S2S and HRAN on the two validation sets (with p -value < 0 . 05).",
      "label": "Result",
      "prob": 0.8014541864395142
    },
    {
      "sentence": "Our contributions are threefold.",
      "label": "Other",
      "prob": 0.4987873136997223
    },
    {
      "sentence": "This is why we also conducted human evaluation experiment.",
      "label": "Result",
      "prob": 0.7402504682540894
    },
    {
      "sentence": "We two datasets in Table 1.",
      "label": "Result",
      "prob": 0.6256750226020813
    },
    {
      "sentence": "For a comprehensive survey, please refer to [4].",
      "label": "Other",
      "prob": 0.8639774918556213
    },
    {
      "sentence": "[1] appended the original word embeddings with a VAD affect model [43].",
      "label": "Other",
      "prob": 0.8239123225212097
    },
    {
      "sentence": "Later on, Liu et al.",
      "label": "Other",
      "prob": 0.8728461265563965
    },
    {
      "sentence": "Case Study.",
      "label": "Result",
      "prob": 0.5707208514213562
    },
    {
      "sentence": "Automatic Evaluation.",
      "label": "Result",
      "prob": 0.5107754468917847
    },
    {
      "sentence": "3 https://github.com/yuboxie/meed",
      "label": "Other",
      "prob": 0.9172834157943726
    },
    {
      "sentence": "We can observe from the first two plots that positive words (green dots) and negative words (red dots) are scattered around and mixed with each other in the language model weights for HRAN and MEED respectively, which means no emotion information is captured in these weights.",
      "label": "Result",
      "prob": 0.8880898356437683
    },
    {
      "sentence": "human judgement test can validate them.",
      "label": "Result",
      "prob": 0.7717211842536926
    },
    {
      "sentence": "For example, Shang et al.",
      "label": "Other",
      "prob": 0.8822200894355774
    },
    {
      "sentence": "Figure 2 gives the results in three subplots.",
      "label": "Result",
      "prob": 0.8363288044929504
    },
    {
      "sentence": "For every sentence, for example, I unfortunately did not pass my exam, the model generates five emotion features denoting ( sad : 1, angry : 1, anxiety : 1, negative emotion : 1, positive emotion : 0).",
      "label": "Result",
      "prob": 0.47227564454078674
    },
    {
      "sentence": "In",
      "label": "Other",
      "prob": 0.8328455090522766
    },
    {
      "sentence": "Human Evaluation Experiment Design.",
      "label": "Other",
      "prob": 0.48892924189567566
    },
    {
      "sentence": "We first conducted Friedman test [12] and then t -test on the human evaluation results (contextual coherence and emotional appropriateness), showing the improvements of MEED over S2S are significant (with p -value < 0 . 01).",
      "label": "Result",
      "prob": 0.849299430847168
    },
    {
      "sentence": "Human Evaluation.",
      "label": "Other",
      "prob": 0.4711436927318573
    },
    {
      "sentence": "Specifically, for each dialog D = ( x 1 , x 2 , . . . , x M ) , we created M  1 contextresponse pairs, namely U i = ( x s i , . . . , x i ) and y i = x i + 1 , for i = 1 , 2 , . . . , M  1, where s i = max ( 1 , i  4 ) .",
      "label": "Result",
      "prob": 0.5086786150932312
    },
    {
      "sentence": "https://doi.org/10.1145/nnnnnnn.nnnnnnn",
      "label": "Other",
      "prob": 0.9333425760269165
    },
    {
      "sentence": "Ghosh et al.",
      "label": "Other",
      "prob": 0.9243909120559692
    },
    {
      "sentence": "Meanwhile, Xu et al.",
      "label": "Other",
      "prob": 0.9163261651992798
    },
    {
      "sentence": "Here",
      "label": "Other",
      "prob": 0.8695629835128784
    },
    {
      "sentence": "MEED.",
      "label": "Other",
      "prob": 0.8958551287651062
    },
    {
      "sentence": "1 https://liwc.wpengine.com/",
      "label": "Other",
      "prob": 0.9512271881103516
    },
    {
      "sentence": "Shang et al.",
      "label": "Other",
      "prob": 0.9332093596458435
    },
    {
      "sentence": "Human Evaluation Results.",
      "label": "Result",
      "prob": 0.8757234215736389
    },
    {
      "sentence": "ACM Reference Format:",
      "label": "Other",
      "prob": 0.9455322623252869
    },
    {
      "sentence": "Automatic Evaluation Results.",
      "label": "Result",
      "prob": 0.8942875862121582
    },
    {
      "sentence": "Li et al.",
      "label": "Other",
      "prob": 0.9377241134643555
    },
    {
      "sentence": "Li et al.",
      "label": "Other",
      "prob": 0.9377241134643555
    },
    {
      "sentence": "Hu et al.",
      "label": "Other",
      "prob": 0.9374648332595825
    },
    {
      "sentence": "KEYWORDS",
      "label": "Other",
      "prob": 0.9414592385292053
    },
    {
      "sentence": "6",
      "label": "Other",
      "prob": 0.9289684891700745
    },
    {
      "sentence": "ABSTRACT",
      "label": "Other",
      "prob": 0.9528233408927917
    },
    {
      "sentence": "3",
      "label": "Other",
      "prob": 0.9225114583969116
    }
  ]
}